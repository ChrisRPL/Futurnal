---
globs: src/futurnal/extraction/**,src/futurnal/learning/**
description: Experiential learning rules - Training-Free GRPO, local-only learning
---
# Experiential Learning Rules (Training-Free GRPO)

**CRITICAL**: Ghost model must remain FROZEN. Learning happens through experiential knowledge (token priors), NOT parameter updates.

## Core Principles

### 1. Ghost Model Remains Frozen
**NEVER update model parameters:**

```python
class GhostModel:
    """Ghost model with frozen parameters."""

    def __init__(self, base_model):
        self.model = base_model
        self.model.eval()  # Frozen - no training mode

        # Freeze all parameters
        for param in self.model.parameters():
            param.requires_grad = False

    def generate(self, prompt: str) -> str:
        """Generate with frozen parameters."""
        with torch.no_grad():  # No gradients
            return self.model.generate(prompt)
```

**Validation**:
```python
def test_ghost_model_frozen():
    """Validate Ghost parameters never change."""
    params_before = {name: param.clone() for name, param in ghost.model.named_parameters()}

    # Run experiential learning
    run_learning_iteration(ghost, documents=100)

    params_after = {name: param for name, param in ghost.model.named_parameters()}

    # Parameters MUST be identical
    for name in params_before:
        assert torch.equal(params_before[name], params_after[name]), \
            f"Parameter {name} changed - Ghost model not frozen!"
```

### 2. Experiential Knowledge as Token Priors
**Store learning as natural language, not weights:**

```python
class ExperientialKnowledge:
    """Experiential knowledge stored as token priors."""

    def __init__(self):
        # NOT: Model weights
        # BUT: Natural language patterns
        self.knowledge = []

    def add_experience(self, experience: str):
        """Add experiential knowledge as text."""
        self.knowledge.append(experience)

    def get_relevant_experiences(self, context: str, top_k: int = 5) -> List[str]:
        """Retrieve relevant experiences for context."""
        # Use semantic similarity to find relevant experiences
        relevant = semantic_search(context, self.knowledge, top_k=top_k)
        return relevant

    def apply_to_prompt(self, base_prompt: str, context: str) -> str:
        """Apply experiential knowledge to prompt."""
        experiences = self.get_relevant_experiences(context)

        enriched_prompt = f"""{base_prompt}

Relevant experiential knowledge:
{format_experiences(experiences)}

Use this knowledge to improve extraction quality.
"""
        return enriched_prompt
```

### 3. Training-Free GRPO Implementation
**Rollout generation and evaluation:**

```python
class TrainingFreeGRPO:
    """Training-Free Group Relative Policy Optimization."""

    def __init__(self, ghost_model, reward_model, experiential_knowledge):
        self.ghost = ghost_model  # Frozen
        self.reward = reward_model
        self.experience = experiential_knowledge

    def generate_rollouts(
        self,
        document: Document,
        num_rollouts: int = 4
    ) -> List[ExtractionResult]:
        """
        Generate K rollouts for same input.

        Ghost + different experiences = diverse Animal behaviors
        """
        rollouts = []

        for i in range(num_rollouts):
            # Sample different experiential knowledge
            experiences = self.experience.sample_experiences(k=5)

            # Apply to prompt
            prompt = self.build_extraction_prompt(document, experiences)

            # Generate with Ghost (frozen)
            result = self.ghost.generate(prompt)

            rollouts.append(result)

        return rollouts

    def evaluate_rollouts(
        self,
        rollouts: List[ExtractionResult],
        ground_truth: Optional[ExtractionResult] = None
    ) -> List[float]:
        """Evaluate rollout quality using reward model."""
        rewards = []

        for rollout in rollouts:
            # Reward based on:
            # - Precision (if ground truth available)
            # - Confidence scores
            # - Temporal consistency
            # - Schema alignment
            reward = self.reward.compute_reward(rollout, ground_truth)
            rewards.append(reward)

        return rewards

    def extract_semantic_advantages(
        self,
        rollouts: List[ExtractionResult],
        rewards: List[float]
    ) -> List[str]:
        """
        Extract semantic advantages via LLM introspection.

        Why did best rollout succeed?
        """
        best_idx = rewards.index(max(rewards))
        best_rollout = rollouts[best_idx]
        worst_idx = rewards.index(min(rewards))
        worst_rollout = rollouts[worst_idx]

        # Use LLM to analyze differences
        analysis_prompt = f"""Compare these two extraction attempts:

Best attempt (reward: {rewards[best_idx]}):
{best_rollout}

Worst attempt (reward: {rewards[worst_idx]}):
{worst_rollout}

What made the best attempt better? Provide specific patterns to keep.
"""

        advantages = self.ghost.generate(analysis_prompt)
        return advantages

    def update_experiential_knowledge(
        self,
        semantic_advantages: List[str]
    ):
        """
        Update experiential knowledge (NOT model parameters).

        This is the "learning" in Training-Free GRPO.
        """
        # Add top patterns to experiential knowledge
        for advantage in semantic_advantages:
            self.experience.add_experience(advantage)

        # Prune low-value experiences (keep top 20)
        self.experience.prune_experiences(keep_top=20)
```

### 4. Animal Behavior Emerges
**Ghost + Experience = Animal:**

```python
class AnimalBehavior:
    """Animal emerges from Ghost + Experiential Knowledge."""

    def __init__(self, ghost_model, experiential_knowledge):
        self.ghost = ghost_model  # Frozen
        self.experience = experiential_knowledge

    def extract(self, document: Document) -> ExtractionResult:
        """
        Animal extraction = Ghost + Experience.

        NO parameter updates, just better prompts.
        """
        # Get relevant experiences
        experiences = self.experience.get_relevant_experiences(
            context=document.content,
            top_k=5
        )

        # Build enriched prompt
        prompt = self.build_prompt_with_experiences(document, experiences)

        # Generate with Ghost (frozen)
        result = self.ghost.generate(prompt)

        return result
```

## Privacy & Local-Only Learning

### Local Processing Only
**NO cloud model updates:**

```python
class LocalOnlyLearning:
    """Enforce local-only learning."""

    def __init__(self, ghost_model, experiential_knowledge):
        self.ghost = ghost_model
        self.experience = experiential_knowledge

        # Validate no cloud connections
        assert not self.has_cloud_connections(), \
            "Cloud connections detected - violates privacy"

    def has_cloud_connections(self) -> bool:
        """Check for cloud model update endpoints."""
        # Scan for API calls to cloud services
        # Ensure no parameter upload
        return False

    def learn(self, documents: List[Document]):
        """Local-only learning."""
        for doc in documents:
            # Generate rollouts (local)
            rollouts = self.generate_rollouts_local(doc)

            # Evaluate (local)
            rewards = self.evaluate_rollouts_local(rollouts)

            # Extract advantages (local)
            advantages = self.extract_advantages_local(rollouts, rewards)

            # Update experiential knowledge (local storage)
            self.experience.add_experiences_local(advantages)

        # NO cloud upload, NO parameter updates
```

## Testing Requirements

```python
class TestExperientialLearning:
    def test_ghost_frozen(self):
        """Validate Ghost model never changes."""
        params_before = get_model_parameters(ghost)
        run_learning(ghost, docs=100)
        params_after = get_model_parameters(ghost)
        assert params_before == params_after

    def test_experiential_knowledge_storage(self):
        """Validate experiences stored as token priors."""
        knowledge = ExperientialKnowledge()
        knowledge.add_experience("Pattern: Use temporal markers for events")

        assert isinstance(knowledge.knowledge[0], str)  # Natural language
        assert "temporal markers" in knowledge.knowledge[0]

    def test_quality_improvement(self):
        """Validate learning improves quality over time."""
        baseline = extract_without_experience(docs)
        with_experience = extract_with_experience(docs)

        assert with_experience.precision > baseline.precision
        assert with_experience.recall > baseline.recall

    def test_no_cloud_connections(self):
        """Validate no cloud model updates."""
        learning = LocalOnlyLearning(ghost, experience)
        assert not learning.has_cloud_connections()

        # Run learning
        learning.learn(documents=100)

        # Still no cloud connections
        assert not learning.has_cloud_connections()
```

## Quality Gates

- ✅ Ghost model frozen (parameters unchanged)
- ✅ Experiential knowledge stored as token priors
- ✅ Quality improvement demonstrable (>5% over 50 documents)
- ✅ Local-only learning (no cloud connections)
- ✅ Semantic advantages extracted and applied

## References

- **Experiential Learning Module**: [docs/phase-1/entity-relationship-extraction-production-plan/03-experiential-learning.md](mdc:docs/phase-1/entity-relationship-extraction-production-plan/03-experiential-learning.md)
- **Training-Free GRPO**: [docs/phase-1/CRITICAL_IMPLEMENTATION_TRILOGY.md](mdc:docs/phase-1/CRITICAL_IMPLEMENTATION_TRILOGY.md)
