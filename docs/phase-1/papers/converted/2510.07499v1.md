# 2510.07499v1.pdf

When Thoughts Meet Facts: Reusable Reasoning for Long-Context LMs
Soyeong Jeong1* Taehee Jung2 Sung Ju Hwang1 Joo-Kyung Kim2 Dongyeop Kang3
KAIST1 Amazon2 University of Minnesota3
{starsuzi, sungju.hwang}@kaist.ac.kr,
{jungtaeh, jookyk}@amazon.com, dongyeop@umn.edu
Abstract
Recent Long-Context Language Models
(LCLMs) can process hundreds of thousands
of tokens in a single prompt, enabling new op-
portunities for knowledge-intensive multi-hop
reasoning by integrating large sets of retrieved
documents or, in some cases, directly all nec-
essary information. However, simply feeding
more documents into the context window fails
to capture how evidence should be connected.
We address this gap withthought templates,
which recast reasoning as reusable thought
caches, derived from prior problem solving
traces, structuring how evidence is combined
and guiding multi-hop inference with factual
documents. To keep these templates effective,
we propose an update strategy that iteratively
refines templates derived from training data
through natural-language feedback. Across
diverse benchmarks and LCLM families,
our approach delivers consistent gains over
strong baselines in both retrieval-based
and retrieval-free settings. Furthermore,
we show that optimized templates can be
distilled into smaller open-source models,
demonstrating its broad applicability and
transparent reasoning reuse. We refer to our
framework as Thought Template Augmented
LCLMs (TOTAL). Code will be available at
https://github.com/starsuzi/ToTAL.
1 Introduction
Knowledge-intensive multi-hop reasoning tasks re-
quire models to gather evidence from multiple doc-
uments, and combine it through reasoning (Trivedi
et al., 2022, 2023; Tang and Yang, 2024; Huang
et al., 2025). These tasks are difficult because rel-
evant evidence must not only be identified, but
also be connected in a structured way, requir-
ing knowledge-based reasoning. The standard so-
lution, Retrieval-Augmented Generation (RAG),
tackles this by first retrieving a small set of relevant
* Work done during internship at Amazon.
(B) Long-context LM
Doc 1
Doc 2
 Doc 3
 Doc 4
(A) Standard RAG
Doc 4
Doc 2Retrieval error
Lack of reasoning
(C) Ours
Query: In the HQ city of the mermaid-logo coffee chain, which market is known for fish-throwing?
Doc 2
 Doc 3
TID 2
 TID 4‚Ä¶‚Ä¶
TID 3
Doc 4
TID 1
‚Ä¶
 Doc 1
Thought IDsDoc IDsSelected 
Figure 1: Thoughts and facts in LCLM, compared to transi-
tional RAG and simple stuffing in LCLM.
documents and then generating an answer from
them (Lewis et al., 2020; Jeong et al., 2024).
The rise of Long-Context Language Models
(LCLMs) has shifted this paradigm by enabling
prompts of hundreds of thousands of tokens (Anil
et al., 2023; Comanici et al., 2025; Anthropic, 2025;
OpenAI, 2025a). This advancement makes it possi-
ble to ‚Äújust put everything into the prompt,‚Äù such
as feeding in all retrieved documents (Lee et al.,
2025) or many in-context examples (Agarwal et al.,
2024; Baek et al., 2025). Compared to conventional
RAG, which risks cascading errors from retrieval,
LCLMs support a one-step formulation that miti-
gates such errors, and in some domains (e.g., enter-
prise settings) can even absorb an entire document
collection into the prompt. However, increasing
recall with more documents alone remains insuffi-
cient, since models may struggle to connect pieces
of evidence. Existing work on LCLMs has largely
focused on scaling input size rather than strength-
ening reasoning, leaving this gap unaddressed.
While one possible direction is adopting reason-
ing strategies such as Chain-of-Thought (Wei et al.,
2022), which elicit step-by-step reasoning, it re-
mains ad-hoc and query-specific, and they are not
designed to cope with the vast, document-heavy
contexts enabled by LCLMs. To address this, we
introducethought templates: reusable reasoning
patterns (or epistemic knowledge from prior experi-
ence) that act as structured scaffolds for integrating
and organizing evidence in these long-context set-
1
arXiv:2510.07499v1  [cs.CL]  8 Oct 2025tings. Templates act as a cache of prior reasoning
behaviors capturinghow to think, while documents
provide the factual content capturingwhat to know.
Importantly, although the entire template set is sup-
plied, LCLMs selectively leverage on the relevant
ones for each query, thereby enabling composi-
tional reasoning over complex evidence.
To instantiate this idea, we automatically con-
struct templates from multi-hop QA datasets in a
compositional manner, allowing LCLMs to flexibly
recombine multiple templates within a single gen-
eration. Unlike prior approaches, which retrieve
a single problem-specific reasoning trace (Yang
et al., 2024b, 2025a), our method enables reusabil-
ity across queries. This compositional design also
improves performance by allowing LCLMs to gen-
eralize to more complex reasoning tasks. To further
improve effectiveness, we treat thought templates
as external parameters of LCLMs and refine them
iteratively using natural language feedback. Feed-
back derived from model errors specifies how tem-
plates should be revised, functioning like a gradient
update but without altering model weights.
We present a framework, Thought Template
Augmented LCLMs (TOTAL), that equips long-
context models with reusable reasoning patterns
and iteratively refines them through natural lan-
guage feedback. We validate TOTAL on diverse
knowledge-intensive datasets that require both fac-
tual grounding and multi-hop reasoning. Further-
more, we evaluate it in two settings: an idealized
setup without retrieval and a more practical sce-
nario with retrieval. Across both settings, thought
templates consistently boost LCLM performance,
and our feedback-driven update strategy yields ad-
ditional gains. These results highlight the promise
of equipping LCLMs with structured reasoning pat-
terns rather than relying solely on larger contexts.
2 Proposed Method
Our method is motivated by three observations:
(1) simply increasing the number of accessible
documents in LCLMs does not guarantee better
reasoning; (2) current models often lack explicit,
structured strategies for combining evidence across
multiple steps; and (3) once distilled, such strate-
gies can be generalized and reused across models.
Below we introduce the necessary background and
describe the design of TOTAL.
2.1 Preliminaries
We first outline the challenges and the limitations
of existing paradigms of multi-hop reasoning.
Knowledge-intensive Multi-hop Reasoning
Multi-hop reasoning requires gathering and
integrating evidence scattered across multiple
documents and composing intermediate steps for
the final answer. Formally, given a query q and a
large corpus of documents D={d 1,d 2, . . . ,dN },
the objective is to generate the correct answer a by
selecting a relevant evidence subset Dq ‚äÜ Dand
chaining reasoning steps over it.
Retrieval-Augmented Generation (RAG)Con-
ventional approaches rely on RAG: a retriever
first identifies a subset of documents Dq =
Retriever(q,D) , and then a Language Model
(LM) generates an answer conditioned on both q
and Dq, denoted as a=LM(q,D q). Since earlier
LMs were limited by context length, retrieval qual-
ity was crucial: poor retrieval caused cascading
errors by omitting essential evidence.
Long-Context Language Models (LCLM)Re-
cent LCLMs can process even millions of tokens in
a single prompt, thereby allowing the direct inclu-
sion of large evidence sets (or entire corpora) into
the context: a=LCLM(q,D) . Alternatively, re-
trieval still can be used to select a much larger set of
documents than before, D‚àó
q =Retriever(q,D) ,
where |Dq| ‚â™ |D‚àó
q |. Thus, LCLM supports two
regimes: inserting full corpus D, or large retrieved
subset Dlarge where Dlarge ‚àà {D,D‚àó
q }. However,
simply scaling document access is insufficient: the
bottleneck now lies in how tostructureandreuse
reasoning over abundant knowledge. At the same
time, finetuning LCLM to explicitly learn long rea-
soning chains is often infeasible due to their high
cost and limited accessibility.
2.2 Thought Template Augmented LCLMs
To bridge these gaps, we introduce TOTAL, a
framework that enables better reasoning in large
document contextswithout any model finetuningby
leveragingthought templates‚Äì structured thought
processes built from training data and refined itera-
tively through textual gradient feedback (Figure 2).
These updated templates then guide the LCLM in
organizing evidence and performing multi-step rea-
soning more effectively during inference.
2Query: In the HQ city of the mermaid-logo coffee chain, which market is known for fish-throwing?
Doc 2
 Doc 3
TID 2
 TID 4‚Ä¶‚Ä¶
TID 3
Doc 4
TID 1
Doc 1
Gold Answer: Pike Place MarketSelected: TID1| TID3| Doc1| Doc4Prediction: Space Needle
Training (Template Updates)
TID 2
TID N
Hit (H)Miss (M)S (H-M)
8 4 +4
‚Ä¶
‚Ä¶
TID 1TID 3
The template correctly identifies the link between company HQ and landmarks but fails to generalize to cultural or market landmarks.  It should expand the reasoning to include markets or cultural sites.
TID 3‚Äô: Headquarters toCulturalLandmarkIdentify not onlyan iconic landmark but also cultural/market landmarks tied to local activities in the headquarters city of the company.1.Identify the company from the description.2.Find the headquarters city of that company.3.Recall famous iconic buildings/structures Consider multiple types of landmarks in that city:1.Iconic buildings/structures2.Cultural or market-based landmarks‚Ä¶
 Doc 3
TID 4‚Ä¶‚Ä¶
Doc 4
Doc 1
UpdateTemplate
Gold Answer: Millennium Park
Query1: In the HQ city of the golden-arches fast-food chain, which park has the Bean sculpture?
Inference
Thought Template DBNeeds Update
Text Gradients(ùúµùíïùüë)
TID 3‚Äô
TID 1
TID 2
Doc 2
TID 3‚Äô
TID 3ùúµùíïùüë
Selected: TID1| TID3‚Äô|Doc2| Doc3Prediction: Millennium ParkLow ùë≠ùíïùüë
( ),
Figure 2: Illustration of training and inference stages for template updates. Low-performing templates are identified via hit/miss
statistics and refined with textual gradient feedback, enabling improved performance on new queries during inference.
Thought TemplatesAthought templateis a
reusable high-level reasoning pattern, distilled from
prior problem-solving. Each template provides a
structured outline of intermediate steps that can
be instantiated for new queries. Formally, let
T={t 1,t 2, . . . ,tm} denote the set of templates.
At inference, the LCLM is conditioned on both the
queryq, large evidence setD large, and templates:
ÀÜa=LCLM(q,T,D large)
Template ConstructionTo build the initial tem-
plate set T , we prompt an LCLM to generate tem-
plates, conditioned on the training queries qtrain,
their gold answers atrain, and optionally solution
pathss train from the training set:
ti =LCLM(q train,a train,[s train])
This procedure is inspired by Yang et al. (2025a),
who also derive templates using LLMs. However,
instead of capturing full example-specific solutions,
we decompose them into sub-templates that are
reusable across queries and generalize more effec-
tively. At inference, the model selectively applies
and composes relevant templates from T with the
query and supporting documents. Below shows an
example of a thought template t3 generated from
the following template construction process.
TID 3:Headquarters to Landmark
Identify an iconic landmark in the headquarters
city of the company.
1. Identify the company from the description.
2. Find the headquarters city of that company.
3. Recall famous iconic buildings/structures ...
Template Update StrategyInitial templates may
be noisy or suboptimal. Thus, we iteratively refine
templates ti using natural-language feedback as a
surrogate gradient. We first assign each template
ti an explicit performance score F(ti) to compute
which reasoning patterns contribute positively or
negatively to model outputs. Specifically, for each
qtrain with atrain, we obtain the model‚Äôs predic-
tion ÀÜatrain =LCLM(q train,T,D large). Then, ti is
assigned an aggregated score as:
F(ti) =
X
qtrain
fi(qtrain),
where fi(qtrain) measures the performance ofti on
qtrain by comparing ÀÜatrain with atrain (e.g., using
metrics such as exact match or F1 in QA tasks). Im-
portantly, fi(qtrain) is computed only for queries
where ti is actually selected. Templates with scores
below a threshold F(ti)< œÑ1 are identified as low-
performing and selected for refinement (e.g., TID 3
in the template database in Figure 2). This enables
targeted refinement, updating only low-performing
templates, while maintaining the stability of well-
performing ones.
For each low-performing template, another LM
analyzes its failure cases by comparing the query
qtrain, its prediction ÀÜatrain, the gold answeratrain,
and the applied template ti, and produces a natural-
language ‚Äútextual gradient‚Äù feedback:
‚àáti =LM Feedback(qtrain, ÀÜatrain,a train,t i)
Below is an example feedback‚àát 3.
‚àá TID 3:The template correctly identifies the
link between company HQ and landmarks but
fails to generalize cultural or market landmarks.
It should expand the reasoning to include..
This textual gradient is accompanied by a discrete
decision indicating the appropriate update action:
di ‚àà {KEEP,FIX,ADD,DISCARD}
1œÑdenoting a threshold selected with the validation set
3