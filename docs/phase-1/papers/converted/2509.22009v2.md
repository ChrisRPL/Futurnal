# 2509.22009v2.pdf

Preprint.
GRAPHSEARCH: ANAGENTICDEEPSEARCHING
WORKFLOW FORGRAPHRETRIEVAL-AUGMENTED
GENERATION
Cehao Yang1,2,3∗, Xiaojun Wu 1,2,3∗, Xueyuan Lin 1,2,4∗, Chengjin Xu 1,3†, Xuhui Jiang 1,3,
Yuanliang Sun3 Jia Li2,Hui Xiong 2†,Jian Guo 1†
1IDEA Research, International Digital Economy Academy
2Hong Kong University of Science and Technology (Guangzhou)
3DataArc Tech Ltd.
4Hithink RoyalFlush Information Network Co., Ltd
{cyang289,xwu647,xlin058,jialee}@connect.hkust-gz.edu.cn,
xionghui@ust.hk,{xuchengjin,jiangxuhui,guojian}@idea.edu.cn
ABSTRACT
Graph Retrieval-Augmented Generation (GraphRAG) enhances factual reasoning
in LLMs by structurally modeling knowledge through graph-based representa-
tions. However, existing GraphRAG approaches face two core limitations: shal-
low retrieval that fails to surface all critical evidence, and inefficient utilization
of pre-constructed structural graph data, which hinders effective reasoning from
complex queries. To address these challenges, we propose GRAPHSEARCH, a
novel agentic deep searching workflow with dual-channel retrieval for GraphRAG.
GRAPHSEARCHorganizes the retrieval process into a modular framework com-
prising six modules, enabling multi-turn interactions and iterative reasoning. Fur-
thermore, GRAPHSEARCHadopts a dual-channel retrieval strategy that issues
semantic queries over chunk-based text data and relational queries over struc-
tural graph data, enabling comprehensive utilization of both modalities and their
complementary strengths. Experimental results across six multi-hop RAG bench-
marks demonstrate that GRAPHSEARCHconsistently improves answer accuracy
and generation quality over the traditional strategy, confirming GRAPHSEARCH
as a promising direction for advancing graph retrieval-augmented generation.
1 INTRODUCTION
When did the town WIZE is licensed in become 
capital of the state where Ward Township is located?
Ward Township, Indiana
Golden 
Evidence
Ward Township is one of eleven townships 
in Randolph County, Indiana. ……
Randolph County, Illinois
Owing to its role in the state's 
history, ……historically important village 
of Kaskaskia, Illinois's first capital.
Springfield, Illinois
Springfield‘s original name was Calhoun,…… 
Springfield became the third and current 
capital of Illinois in 1839.……
WIZE
WIZE (1340 AM) is a commercial radio 
station in Springfield, Ohio owned by 
iHeartMedia, Inc. as part of their ……
Retrieved 
Entities
town official
WIZE
Wards 3, 5, and 6
Ward Township
……
……
Retrieved 
Relationships
Retrieved 
Document Chunks
Retrieve
Once 
only
Based on the provided information, there is no direct mention of the town 
where WIZE is licensed becoming the capital of the state where Ward 
Township is located. There is no information in the given knowledge base 
that connects Springfield to becoming the capital of Indiana, nor does it 
mention any historical events regarding Indiana's capital city.
Answer
Once 
only
2010 Census Ward Township
The 2010 Census provided 
data on Ward Township……
Springfield WIZE
WIZE AM 1340 is based in 
Springfield and targets……
…… ……
……
WIZE: WIZE (1340 AM) —
branded WIZE AM 1340 —
is a commercial radio 
station in Springfield……
Tucson, Arizona: Both 
the council members and 
the mayor serve four-
year terms; ……
……
……
Figure 1: Shallow retrieval.
Large Language Models (LLMs) demonstrates remark-
able capabilities in natural language understanding and
reasoning (Zhao et al., 2023; Naveed et al., 2025). De-
spite their strong performance, LLMs inherently rely on
their parametric knowledge, which often results in hallu-
cinations and a lack of factual grounding (Zhang et al.,
2025; Wang et al., 2023). Retrieval-augmented gener-
ation (RAG) has emerged as a paradigm that combines
LLMs with external knowledge bases, enhancing factual-
ity, credibility and interpretability in knowledge-intensive
tasks (Lewis et al., 2020).
More recently, Graph Retrieval-Augmented Generation
(GraphRAG) is introduced to overcome the shortcomings
of traditional RAG, which relies solely on semantic sim-
ilarity for retrieval (Peng et al., 2024). By constructing structural graph knowledge bases (graph
1Equal contribution.
2Corresponding authors.
3Our code implementation are available at https://github.com/DataArcTech/GraphSearch.
1
arXiv:2509.22009v2  [cs.CL]  30 Sep 2025Preprint.
Graph Data
Text Data
Sometimes I can get 
a better answer even 
without any 
retrieved graph data!
Question
Figure 2: Comparison of using graph data only, text data only, or all data as commonly adopted in
GraphRAG approaches. The metric is SubEM. The contribution of retrieved graph data is marginal.
KBs) and leveraging hierarchical retrieval strategies, GraphRAG strengthens the integration of con-
textual information across massive entities and relationships (Sarthi et al., 2024; Edge et al., 2024;
Guo et al., 2024). Building upon this foundation, some advanced graph-based enhancements that
incorporate diverse structures, including heterogeneous graphs, causal graphs, and hypergraphs, to
enrich representational ability and facilitate more abundant graph construction (Fan et al., 2025;
Wang et al., 2025; Luo et al., 2025; Feng et al., 2025; Xu et al., 2025). In addition, heuristic strate-
gies such as path-based search, pruning, and memory-inspired indexing further reinforce reasoning
abilities and enable deeper multi-step exploration (Chen et al., 2025; Jimenez Gutierrez et al., 2024;
Guti´errez et al., 2025; Wang, 2025).
However, existing GraphRAG approaches still face challenges that lead to performance bottlenecks:
(i)Shallow retrieval results in missing evidence for complex queries.Most GraphRAG methods
adopt a single-round retrieval-and-generation process as the interaction strategy between the LLM
and the graph KB (Edge et al., 2024; Guo et al., 2024; Fan et al., 2025). However, as illustrated in
Figure 1, when handling a complex query that requires four pieces of golden evidence,“When did
the town WIZE is licensed in become capital of the state where Ward Township is located?”, the en-
tityRandolph Countyis not retrieved by the LightRAG retriever. Consequently, the LLM’s reasoning
suffers from broken logic and insufficient evidence. (ii)Limited ability to exploit structural data
due to constrained retrieval scope. Existing GraphRAG methods with heuristic path-construction
schemes (Fan et al., 2025; Chen et al., 2025; Jimenez Gutierrez et al., 2024) often fail to fully lever-
age the structural information in graph KBs, fundamentally because shallow retrieval restricts the
coverage of relevant nodes and relations. Without sufficient coverage of retrieved subgraphs, the
available structural signals are fragmented and sparse, making it difficult for LLMs to integrate se-
mantic and structural modalities for complex reasoning. As shown in Figure 2, models may perform
comparably with text-only evidence, highlighting that the underutilization of graph data is tightly
coupled with the limitations of current retrieval strategies.
We proposeGRAPHSEARCH, an agentic deep searching workflow for GraphRAG. As illustrated
in Figure 3, GRAPHSEARCHis a novel agent framework designed to access graph KBs through
dual-channel retrieval, acquiring both semantic and structural information, and performing multi-
turn interactions to complete complex reasoning tasks. Targeting the shallow retrieval problem
in existing GraphRAG approaches,GRAPHSEARCHmodels retrieval as a modular searching
pipeline, which consists of six modules:Query Decomposition (QD),Context Refinement (CR),
Query Grounding (QG),Logic Drafting (LD),Evidence Verification (EV), andQuery Expansion
(QE). Through the coordinated contributions of these modules, GRAPHSEARCHdecomposes com-
plex queries into tractable atomic sub-queries, retrieves fine-grained knowledge from graph KBs,
and iteratively performs logical reasoning and reflection to remedy missing evidence. Furthermore,
GRAPHSEARCHadopts a dual-channel retrieval strategy, constructing semantic queries over
chunk-based text data and relational queries over structural graph data, thereby fully synergizing
both modalities and integrating them into contexts that support LLMs in complex reasoning.
We conduct experiments on six multi-hop RAG datasets. The results demonstrate that leveraging the
graph KBs retrievers built upon the corresponding GraphRAG approaches, GRAPHSEARCHconsis-
tently outperforms the single-round interaction strategy in terms of answer accuracy and generation
quality, while also exhibiting strong plug-and-play capability, as shown in Table 1. Furthermore,
the effectiveness of the dual-channel retrieval strategy, the contributions of agentic modules, and its
robustness under a small-scale LLM and varying retrieval budgets are all empirically validated.
2Preprint.
2 RELATEDWORK
2.1 GRAPHRETRIEVAL-AUGMENTEDGENERATION
RAG augments LLMs with external evidence to improve factuality of knowledge-intensive
tasks (Lewis et al., 2020). Building on this, GraphRAG is an advance paradigm that explicitly mod-
els structural relations among entities, thereby capturing relational semantics, contextual dependen-
cies and structural knowledge integration (Peng et al., 2024; Edge et al., 2024). Early work (Sarthi
et al., 2024; Edge et al., 2024) emphasize hierarchical summarization and global information inte-
gration, but they insufficiently leveraged the fine-grained structural information. LightRAG (Guo
et al., 2024) advanced this direction by incorporating graph structures into both indexing and re-
trieval. Recent efforts in graph KB construction introduce diverse structural representations, such
as the design of heterogeneous and lightweight graph structures (Fan et al., 2025; Xu et al., 2025),
the extension to hypergraphs that capturing higher-order relational dependencies (Luo et al., 2025;
Feng et al., 2025), and the leverage of causal graphs to improve logical continuity (Wang et al.,
2025). Additionally, retrieval strategies on graph KBs increasingly rely on heuristic path explo-
ration, such as the topology-enhanced lightweight search (Fan et al., 2025), the pruning via rela-
tional path retrieval (Chen et al., 2025), the utilization of personalized memory-inspired reason-
ing (Jimenez Gutierrez et al., 2024; Guti ´errez et al., 2025), and the adoption of beam search over
proposition paths (Wang, 2025). Despite these advances, current GraphRAG approaches remain
constrained by shallow retrieval, limiting their ability to perform deep searching over graph KBs.
2.2 AGENTICRETRIEVAL-AUGMENTEDGENERATION
RAG improves factual grounding by retrieving external knowledge (Lewis et al., 2020), but single-
round interaction is insufficient for complex reasoning tasks. Early advances focus on atomic-level
improvements of RAG in query decomposition (Cao et al., 2023), query rewriting (Ma et al., 2023;
Chan et al., 2024), retrieval compression (Xu et al., 2023), and selective retrieval decisions (Tan
et al., 2024), which refine the retrieval process at a fine granularity. Beyond these, modular RAG
systems (Gao et al., 2024; Jin et al., 2025b; Wu et al., 2025) have been proposed to flexibly reconfig-
ure retrieval and reasoning modules into composable pipelines. More recently, agentic approaches
emerged, enabling LLMs to iteratively plan, retrieve, and reflect. Representative methods include
reasoning–acting synergy in ReAct (Yao et al., 2023), self-reflective retrieval in Self-RAG (Asai
et al., 2024), test-time planning in PlanRAG (Verma et al., 2024), and reinforcement-learned search
agents in Search-o1 (Li et al., 2025) and Search-r1 (Jin et al., 2025a). Subsequently, pioneering
works (Sun et al., 2023; Ma et al., 2024; Shen et al., 2024; Lee et al., 2024) integrated structural
graph knowledge for retrieval into the agentic RAG workflow to support the multi-step reasoning.
3 PRELIMINARIES
Graph Knowledge Database.Given a document collectionD, the graph indexerϕsegments
Dinto a set of text chunksK. For each chunkk∈K, an extractorR ∈ϕidentifies a set of
entitiese={e name, eprop, edesc}. For any pair of entitiese h, et ∈k, a relation is defined asr=
{eh, et, rprop, rdesc}. Aggregating all entities and relations yields the graph KBG={E, R, K},
whereEdenotes the entity set,Rthe relation set, andKthe associated chunk-level textual context.
Graph KB Retrieval.Given a queryq, a graph KB retrieverψselects a relevant context setC=
{Eq, Rq, Kq} ⊂Gthat maximizes semantic relevance toq. The retriever aims to return structural
graph data and chunk-based text data that provide sufficient evidence for answer generation.
LLM Answer Generation.The language model consumes the queryqtogether with the retrieved
contextCto generate an outputy. The generation is modeled asP(y|q) = P
C⊂G P(y|
q, C)P(C|q, G), whereP(C|q, G)represents the retrieval probability over the graph KB, and
P(y|q, C)denotes the generation probability conditioned on the integrated evidence.
3