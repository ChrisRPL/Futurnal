# 2509.25454v2.pdf

Preprint, Under Review
DEEPSEARCH: OVERCOME THEBOTTLENECK OF
REINFORCEMENTLEARNING WITHVERIFIABLERE-
WARDS VIAMONTECARLOTREESEARCH
Fang Wuâ™¡âˆ— Weihao Xuanâˆ‡,â–³âˆ— Heli Qiâ–³âˆ— Ximing Luâ™¢ Aaron Tuâ™ 
Li Erran Liâ™£ Yejin Choiâ™¡â€ 
â™¡Stanford University âˆ‡University of Tokyo â–³RIKEN AIP â™¢University of Washington
â™ UC Berkeley â™£Amazon AWS
ABSTRACT
Although Reinforcement Learning with Verifiable Rewards (RLVR) has become
an essential component for developing advanced reasoning skills in language
models, contemporary studies have documented training plateaus that emerge fol-
lowing thousands of optimization steps, demonstrating notable decreases in per-
formance gains despite increased computational investment. This limitation stems
from the sparse exploration patterns inherent in current RLVR practices, where
models rely on limited rollouts that often miss critical reasoning paths and fail
to provide systematic coverage of the solution space. We present DeepSearch, a
framework that integrates Monte Carlo Tree Search (MCTS) directly into RLVR
training. In contrast to existing methods that rely on tree search only at inference,
DeepSearch embeds structured search into the training loop, enabling systematic
exploration and fine-grained credit assignment across reasoning steps. Through
training-time exploration, DeepSearch addresses the fundamental bottleneck of
insufficient exploration, which leads to diminishing performance improvements
over prolonged training steps. Our contributions include: (1) a global frontier
selection strategy that prioritizes promising nodes across the search tree, (2) se-
lection with entropy-based guidance that identifies confident paths for supervision,
and (3) adaptive replay buffer training with solution caching for efficiency. Ex-
periments on mathematical reasoning benchmarks show that DeepSearch achieves
62.95% average accuracy and establishes a new state-of-the-art for 1.5B reasoning
models - a 1.25 percentage point improvement over the previous best while using
5.7x fewer GPU hours than extended training approaches. These results high-
light the importance of strategic exploration over brute-force scaling and demon-
strate the promise of algorithmic innovation for advancing RLVR methodologies.
DeepSearch establishes a new direction for scaling reasoning capabilities through
systematic search rather than prolonged computation.
https://huggingface.co/fangwu97/DeepSearch-1.5B
1 INTRODUCTION
Large language models (LLMs) have recently achieved notable progress on complex reasoning
tasks (DeepSeek-AI, 2025; Yang et al., 2024), driven in part by test-time computation scaling strate-
gies (Li et al., 2023; Yao et al., 2023; Bi et al., 2024; Zhang et al., 2024a; Guan et al., 2025) such
as tree search with process-level evaluation. While effective, these methods typically treat struc-
tured search as an inference-only mechanism, leaving untapped potential to integrate systematic
exploration into the training process itself.
This separation between training and inference creates fundamental limitations in how we scale rein-
forcement learning with verifiable rewards (RLVR) for reasoning. Current RLVR approaches remain
âˆ—Equal contributions.
â€ Corresponding author. Email:yejinc@stanford.edu
1
arXiv:2509.25454v2  [cs.AI]  1 Oct 2025Preprint, Under Review
constrained by sparse exploration patterns during training (Wu et al., 2025; Liu et al., 2025c), while
models are expected to demonstrate sophisticated search behaviors only at inference time. Even
recent advances in prolonged RL training (Liu et al., 2025a) have shown performance plateaus after
thousands of steps, with clear diminishing returns from allocating more computation to additional
training depth. This suggests that simply scaling the number of training stepsâ€”the primary axis
explored in prior workâ€”may not be sufficient to unlock the full potential of RLVR.
We address this gap by introducing DeepSearch, a framework that embeds Monte Carlo Tree Search
(MCTS) (Metropolis & Ulam, 1949) directly into RLVR training, representing a fundamental shift
from scaling training depth to scaling training breadth. By coupling structured search with verifiable
rewards during training, DeepSearch enables models to learn not only from correct solutions but also
from the systematic exploration process itself, providing richer supervision than outcome-based or
direct rollout methods (Lyu et al., 2025; He et al., 2025b).
The core insight driving us is tofocus on training-time explorationas the driver of improved rea-
soning. While traditional RLVR relies on limited rollouts that may miss critical reasoning paths,
DeepSearch systematically expands the reasoning frontier during training through principled tree
search. This design advances three key objectives:(i)expanding reasoning coverage beyond what
direct policy rollouts can achieve,(ii)providing fine-grained credit assignment to intermediate rea-
soning steps through tree-structured backpropagation, and(iii)maintaining computational efficiency
through intelligent node selection and solution caching strategies.
To achieve these goals, DeepSearch introduces several key innovations. First,global frontier se-
lectionstrategy prioritizes the most promising nodes across the entire search tree, moving beyond
traditional root-to-leaf UCT traversals that can be computationally wasteful and myopic. Second,se-
lection with entropy-based guidancesystematically identifies confident incorrect reasoning paths for
supervision. Finally, an adaptive training strategy with replay buffers progressively filters challeng-
ing problems and caches verified solutions to avoid redundant computation across training iterations.
We evaluate DeepSearch on mathematical reasoning benchmarks, where it significantly outperforms
state-of-the-art RLVR baselines, including Nemotron-Research-Reasoning-Qwen-1.5B v2 (Liu
et al., 2025a) and DeepScaleR (Luo et al., 2025b). Our results show that DeepSearch achieves
62.95% average accuracy on challenging mathematical tasks, representinga new state-of-the-art
for 1.5B reasoning models. Importantly, these gains are achieved while remaining computation-
ally efficient through progressive filtering and intelligent solution reuse, demonstrating that search-
augmented training can be both more effective and more practical than conventional approaches.
The implications extend beyond math reasoning: by bridging the gap between inference-time search
capabilities and training-time learning, DeepSearch establishes a new paradigm for scaling RLVR
that emphasizes systematic exploration over prolonged training. This work suggests that the future
of reasoning model development lies not just in scaling model parameters or training steps, but in
fundamentally rethinking how we structure the learning process to mirror the sophisticated reasoning
patterns we expect at inference time. We defer a detailed literature review to Appendix A due to
space constraints.
2 DEEPSEARCH WITHMCTS
Given a problemxand a policy modelÏ€ Î¸, we adopt a modified MCTS framework to build a search
tree for incremental step-by-step solution exploration. We replace traditional root-to-leaf selection
with global frontier-based node selection. The root node represents the questionx, and child nodes
correspond to intermediate stepssgenerated byÏ€ Î¸. A root-to-leaf path ending at a terminal node
send forms a trajectoryt=xâŠ•s 1 âŠ•s 2 âŠ•. . .âŠ•send, where each steps i is assigned a q-valueq(s i).
Then we extract solution trajectoriesT=

t1,t 2, . . . ,tn	
(nâ‰¥1)from the search treeT, wheret i
can be correct, incorrect or incomplete. The depth of any nodesis denoted asd(s)âˆˆZ +.N(s)and
Î¾(s)denote the number of visits tosand the number of children nodes ofs, respectively. Starting
from the root nodex, our MCTS iterations are conducted through four subsequent components.
2.1 EXPANSION WITHENTROPY-BASEDGUIDANCE
In stepi, we collect the latest reasoning trajectoryo i =xâŠ•s 1 âŠ•s 2 âŠ•. . .âŠ•siâˆ’1 as the current state,
i.e., observation. Based on this state, we prompt the policy modelÏ€Î¸(si|oi)to generatencandidates
2Preprint, Under Review
s*
Selection Expansion
Global frontier selection 
with frontier priority score
ğœ†1ğ‘„ğ‘ğ‘ğ‘Ÿğ‘’ğ‘›ğ‘¡ + ğœ†2ğ» + ğœ†3ğ·
sj sj sj
Expansion with policy
ğœ‹ğœƒ(âˆ— |ğ‘œğ‘ âˆ—)
se
sj sj
sj
se
sj
sj
Score backup
se
sj
sj
ğ‘(ğ‘š) ğ‘ğ‘’ğ‘›ğ‘‘ ğ‘ğ‘’ğ‘›ğ‘‘
sesesj
Select either correct or 
most informative negative 
for backpropagation
Repeat K iterations
se End node (incorrect or 
incomplete)
se End node (correct)
Intermediate node
s* Optimal global frontier
sj Intermediate node 
(global frontier)
Expansion with policy
Q-value update
Obtain correct
solution ğ­correct
Collect hard ones with
ğ‘…ğ‘ğ‘ğ‘›ğ‘‘ğ‘–ğ‘‘ğ‘ğ‘¡ğ‘’
(ğ‘–+1) = Pass1@K ğ‘¥, ğœ‹ğœƒ ğ‘– < ğ›¿(ğ‘–)
Replay buffer update
with trajectory from MCTS 
ğ‘…(ğ‘–+1) = ğ‘…(ğ‘–)â‹ƒğ‘…ğ‘ğ‘ğ‘›ğ‘‘ğ‘–ğ‘‘ğ‘ğ‘¡ğ‘’
(ğ‘–+1)
Replay Buffer
DeepSearch-MCTS
 Adaptive Training
Iterative obtain hard training set
{ğ‘¥ âˆˆ ğ·â„ğ‘ğ‘Ÿğ‘‘
ğ‘–+1 |Pass1@K ğ‘¥, ğœ‹ğœƒ ğ‘–
< ğ›¿ ğ‘– }
ğ·ğ‘–ğ‘Ÿğ‘’ğ‘ğ‘¡ğ‘…ğ‘œğ‘™ğ‘™ğ‘œğ‘¢ğ‘¡ğ‘  ğ‘¥, ğ›½  
if (ğ‘¥, ğ­cached) âˆˆ ğ‘…(ğ‘–)
MCTSfull ğ‘¥  otherwise
Iterative policy 
training with
Tree-GRPO
ğ‘ƒğœƒ ğ‘‚ ğ‘‚.ğ‘1
ğ‘ƒğœƒ ğ‘‚ ğ‘‚. ğ‘‚ğ‘2
ğ‘ƒğœƒ ğ‘‚ ğ‘‚. ğ‘‚. ğ‘‚ğ‘3
ğ‘ƒğœƒ ğ‘‚ ğ‘‚. . ğ‘‚ğ‘‚ â€¦ğ‘4
Node level reward
Filter training setUpdate Policy
Figure 1: DeepSearch Framework Overview.
for the next-step reasoning trail{s i,j}n
j=1. We repeat this expansion behavior until we reach the
terminal nodess end âˆˆ Send, either by arriving at the final answers or by hitting the maximum depth
of the treed T , which yields an ordered sequences 1 â†’ Â·Â·Â· â†’send.
During each expansion, letS (k)
end denote the set of newly generated terminal nodes at iterationk.
We evaluate the correctness of each terminal node using a verification functionV:S end â†’ {0,1},
whereV(s) = 1indicates a correct solution andV(s) = 0indicates an incorrect or incomplete
solution. Then we partition the terminal nodes into correct and incorrect subsets:
S(k)
correct ={sâˆˆ S(k)
end | V(s) = 1},S (k)
incorrect ={sâˆˆ S(k)
end | V(s) = 0}.(1)
IfS (k)
correct =âˆ…, we employ anentropy-based selectionto identify the most confident negative ex-
ample, where the terminal node with the lowest average entropy along its root-to-leaf trajectory is
selected:
sâˆ—
neg = arg min
sâˆˆS(k)
incorrect
Â¯H(t(s)),(2)
wheret(s) = (x, s1, s2, . . . , s)represents the unique trajectory from rootxto terminal nodes, and
the average trajectory entropy is defined as:
Â¯H(t(s)) = 1
|t(s)|
|t(s)|X
i=1
H(Ï€Î¸(si |o i)),(3)
with finalH(Ï€ Î¸(si |o i)) =âˆ’ P
ai,k
Ï€Î¸(ai,k |o i, ai,<k) logÏ€Î¸(ai,k |o i, ai,<k)being the Monte
Carlo estimation of the Shannon entropy of the policy distribution at stepi.a i,k is thek-th token
of steps i, anda i,<k denotes the tokens precedinga i,k. This selection strategy prioritizes incorrect
reasoning sequences exhibiting low decision uncertainty, targeting areas where the modelâ€™s decision-
making is most confident and would benefit from additional training supervision.
2.2 HEURISTICSCOREBACKUP
Lett âˆ— denote the selected trajectory for backpropagation, which is either a correct solution trajectory
or the most confident negative trajectoryt(s âˆ—
neg)identified through entropy-based selection. Let
q(m)(si)denote theq-valuefor nodes i âˆˆt âˆ— after them-th rollout backpropagation. We define the
iterative q-value update rule for nodes along the selected trajectory:
q(m)(si) =q (mâˆ’1)(si) +Î³(i, l)Â·q (m)(send),(4)
whereÎ³(i, l) :Z + Ã—Z + â†’[0,1]is thetemporal decayfunction that assigns higher weights to
nodes closer to the terminal node:
Î³(i, l) = max
i
l , Î³min

,(5)
3