# 2509.24107v1.pdf

Preprint
FATHOM-DEEPRESEARCH: UNLOCKINGLONG
HORIZONINFORMATIONRETRIEVAL ANDSYNTHESIS
FORSLMS
Shreyas Singh*
Fractal AI Research
shreyas.singh@fractal.ai
Kunal Singh*†
Fractal AI Research
kunal.singh@fractal.ai
Pradeep Moturi*
Fractal AI Research
pradeep.moturi@fractal.ai
ABSTRACT
Tool-integrated reasoning has emerged as a key focus for enabling agentic appli-
cations. Among these, DeepResearch Agents have gained significant attention
for their strong performance on complex, open-ended information-seeking tasks.
We introduce Fathom-DeepResearch, an agentic system composed of two spe-
cialized models. The first is Fathom-Search-4B, a DeepSearch model trained
from Qwen3-4B and optimized for evidence-based investigation through live web
search and targeted webpage querying. Its training combines three advances: (i)
DUETQA, a ~5K-sample dataset generated via multi-agent self-play that enforces
strict web-search dependence and heterogeneous source grounding; (ii) RAPO, a
zero-overhead extension of GRPO that stabilizes multi-turn Reinforcement Learn-
ing with Verifiable Rewards through curriculum pruning, reward-aware advantage
scaling, and per-prompt replay buffers; and (iii) a steerable step-level reward
that classifies each tool call by cognitive behavior and marginal utility, enabling
explicit control over search trajectory breadth, depth, and horizon. These improve-
ments enable reliable extension of tool-calling beyond 20 calls when warranted.
The second is Fathom-Synthesizer-4B, trained from Qwen3-4B, which converts
multi-turn DeepSearch traces into structured, citation-dense DeepResearch Reports
for comprehensive synthesis. Evaluated on DeepSearch benchmarks (SimpleQA,
FRAMES, WebWalker, Seal0, MuSiQue) and DeepResearch-Bench, the system
achieves state-of-the-art performance in the open-weights category while demon-
strating strong generalization to diverse reasoning tasks including HLE, AIME-25,
GPQA-Diamond, and MedQA.
/gtbhttps://github.com/FractalAIResearchLabs/Fathom-DeepResearch
1 INTRODUCTION
Recent advancements in reasoning capabilities of Large Language Models (LLMs) have enabled a
significant performance advancement across a diverse set of tasks, such as mathematical reasoning,
code generation (Jain et al., 2024; DeepSeek-AI et al., 2025; Singh et al., 2025b;a). We are not only
witnessing expert level performance on academic benchmarks, but are perceiving a paradigm shift
towards agentic intelligence. Owing to tool-integrated reasoning, these models can now autonomously
observe, reason and interact with complex and dynamic environments. Contemporary state-of-the-
art tool-augmented AI systems like DeepResearch (OpenAI, 2025b), have exhibited super-human
performance in highly sophisticated long-horizon, deep-information retrieval and synthesis tasks (Du
et al., 2025). These agents transcend the limitations of static parametric knowledge by implementing
*Equal contribution.
†Project lead.
1
arXiv:2509.24107v1  [cs.AI]  28 Sep 2025Preprint
Figure 1: Comparison ofFathom-Search-4Bon prominent DeepSearch benchmarks(WebWalker,
SimpleQA). Our model consistently outperforms strong open-source & closed source baselines.
dynamic reasoning frameworks that autonomously partition multifaceted queries, coordinate multiple
tool interactions, and integrate heterogeneous information sources into unified, evidence-supported
conclusions.
However, a substantial performance gap remains between proprietary implementations (OpenAI,
2025b; Team, 2025a;c) and open-source (Team, 2025b; AI, 2025) alternatives, making the develop-
ment of robust DeepResearch architectures a critical challenge. Current open-source frameworks
suffer from two fundamental limitations. First, they lack proficiency in sustained tool usage required
for high-uncertainty reasoning and synthesis tasks (Du et al., 2025). Efforts to scale DeepResearch
capabilities are constrained by (i) the absence of a high-quality, verifiable, and scalable dataset
creation pipeline, (ii) algorithmic instability in multi-turn reinforcement learning (RL) with tools,
and (iii) inefficient tool-calling behavior that undermines deep information exploration and retrieval.
Second, there is an overemphasis on closed-form problem solving, which comes at the expense of
the information synthesis capabilities essential for tackling open-ended investigative queries. In the
subsequent section we discuss the aforementioned issues in detail.
1.1 MOTIVATION
(1) Training instability of GRPO in multi-turn tool interaction: RLVR (Reinforcement Learning
with verifiable rewards) with GRPO (Shao et al., 2024) has demonstrated early promise in aligning
LLMs with sparse reward signals for single-turn reasoning tasks, particularly in structured domains
like Math/STEM Shao et al. (2024); Yang et al. (2024). However, GRPO struggles to scale to multi-
turn tool-augmented environments, because external tool interaction responses induce distribution
shift in the policy model from its set token generation patterns, this leads to decoding instability and
malformed generations. This cascading of errors causes group-relative advantages to saturate, leading
to extremely unstable gradient updates that breaks the entire training process. (Xue et al., 2025).
(2) Reward hacking and inefficient tool calling(a) Correctness-only sparse rewards do not scale
to long-horizon tool calling.When training with only a single end of episode correctness signal,
the agent shows early improvements achieving format adherence and basic tool-calling competence
in the beginning, however, as training progresses, tool usage increases sharply while both training
reward and validation performance deteriorate (Nguyen et al., 2025). This degradation stems from
reward hacking: the agent collapses into repetitive, identical tool calls because the vanilla RLVR
objective provides no incentive for efficiency or diversity in tool use.(b) RL amplifies SFT priors,
limiting control over the cognitive behaviors developed by the policy(Gandhi et al., 2025): Tool-use
RL typically relies on an SFT cold start to elicit basic tool competence (Li et al., 2025a); (Dong
et al., 2025) RL then amplifies pre-existing cognitive behaviors seeded by SFT. Standard RLVR
affords limited control over the exploration and verification strategies developed by the policy model,
2Preprint
Figure 2:End-to-end inference framework of our proposed Fathom-DeepResearchagentic
system combining browsing and information gathering ability ofFathom-Search-4Bwith
information synthesis and insight generation ability ofFathom-Synthesizer-4B
consequently the quality of cold-start trajectories disproportionately shape the policy model’s tool-use
behavior and provides no steerability.
(3) Limited training data characterized by high and hard-to-reduce intrinsic information
uncertainty: Training datasets such as TriviaQA (Joshi et al., 2017), and multi-hop variants like
2WIKI(Ho et al., 2020), and HotpotQA (Yang et al., 2018) represent problems where solutions
can often be found through minimal set queries or even from a model’s parametric knowledge
alone. These datasets do not expose models to the real-world retrieval challenges posed by noisy,
heterogeneous data sources on the internet. Recent synthetic efforts (Sun et al., 2025a; Li et al.,
2025a; Sun et al., 2025b) attempt to bridge this gap by simulating realistic search behavior. For
instance, WebSailor’s(Li et al., 2025a) SailorFog-QA constructs ambiguous queries using obfuscated
subgraphs of entity graphs, while SimpleDeepResearcher (Sun et al., 2025b) issues multi-stage
search-summarize-generate tool calls over raw HTML. Despite their innovation, these pipelines
remain expensive, brittle, and time-consuming. They rely on handcrafted heuristics, graph expansion,
or multi-stage LLM orchestration, limiting scalability, topical diversity, and adaptability to new
domains.
(4) Challenges in handling open-ended qureiesRecent efforts (Dao & Vu, 2025; Internet, 2025; Li
et al., 2025a) primarily focus on optimizing model performance for closed-ended queries, which are
characterized by well-defined objectives. However, many real-world applications demand handling
of open-ended, exploratory queries that fundamentally differ from their closed-ended counterparts.
These questions lack singular definitive answers and hence, not only demand extensive multi-turn
exploration to uncover diverse perspectives, but also require synthesis of comprehensive responses
that integrate multiple findings with rigorous evidence grounding. The inherently exploratory nature
of these problems necessitates sophisticated information synthesis capabilities, a critical gap that
existing approaches fail to address.
1.2 OURCONTRIBUTIONS
To this end, we present an end-to-end DeepSearch system centered onFathom-Search-4B(search en-
abled reasoning) andFathom-Synthesizer-4B(synthesis & report-generation). Our key contributions:
• RL Zero framework for DeepSearch training.We present a novel two-stage RL-Zero
framework that helps tosteer cognitive behaviorsdeveloped by the policy model like
exploration & verification during the training.
• RAPO: Reward Aware Policy Optimization.We introduce a zero-overhead modification
of GRPO withdataset pruning, advantage scaling, and replay buffers, and a steerable
step-level rewardthat stabilizes multi-turn RL and enables long-horizon tool use.
• DUETQA.We release a 5K sample dataset created through our novelmulti-agent self-play
pipleline, which has verifiable question-answer pairs, impossible to answer withoutlive web
search, for DeepSearch model training.
• DEEPRESEARCH-SFT.A synthetic SFT corpus for converting downstream search/investi-
gation traces of DeepSearch enabled models into DeepResearch reports via an explicitplan
then writeprotocol.
3