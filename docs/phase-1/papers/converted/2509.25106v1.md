# 2509.25106v1.pdf

Work in progress
TOWARDSPERSONALIZEDDEEPRESEARCH:
BENCHMARKS ANDEVALUATIONS
Yuan Liang♢♠∗, Jiaxian Li♢∗ ∗, Yuqing Wang♢, Piaohong Wang♢, Motong Tian♢,
Pai Liu♢, Shuofei Qiao♠, Runnan Fang♠, He Zhu♢, Ge Zhang♣, Minghao Liu△,
Yuchen Eleanor Jiang♢, Ningyu Zhang♠†, Wangchunshu Zhou♢†
♢OPPO ♠Zhejiang University ♣M-A-P △2077.AI
{liang_yuan,zhangningyu}@zju.edu.cn,zhouwangchunshu@oppo.com
ABSTRACT
Deep Research Agents (DRAs) can autonomously conduct complex investiga-
tions and generate comprehensive reports, demonstrating strong real-world po-
tential. However, existing evaluations mostly rely on close-ended benchmarks,
while open-ended deep research benchmarks remain scarce and typically neglect
personalized scenarios. To bridge this gap, we introducePersonalized Deep Re-
search Bench, the first benchmark for evaluating personalization in DRAs. It
pairs 50 diverse research tasks across 10 domains with 25 authentic user pro-
files that combine structured persona attributes with dynamic real-world contexts,
yielding 250 realistic user-task queries. To assess system performance, we pro-
pose the PQR Evaluation Framework, which jointly measures (P) Personalization
Alignment, (Q) Content Quality, and (R) Factual Reliability. Our experiments on a
range of systems highlight current capabilities and limitations in handling person-
alized deep research. This work establishes a rigorous foundation for developing
and evaluating the next generation of truly personalized AI research assistants.
1 INTRODUCTION
Recent advances in large language models (LLMs) have enabled the development of AI agents capa-
ble of conducting complex deep research. Early LLMs focus on isolated tasks like QA and transla-
tion, later advancing with tool integration for autonomous information retrieval and synthesis. More
recently, a new class of advanced systems has emerged, known as Deep Research Agents (DRAs),
including industry solutions (OpenAI, 2025b; Google DeepMind, 2025; xAI Team, 2025; Perplexity
Team, 2025; Moonshot AI, 2025; ByteDance, 2025b) and open-source systems (Li et al., 2025b;c;
Zhu et al., 2025a; Zhou et al., 2023; 2024; Wang et al., 2025; Hu et al., 2025; Manus AI, 2025;
MiroMind AI Team, 2025; ByteDance, 2025a; Li et al., 2025a; Tang et al., 2025; Shi et al., 2025;
Zhu et al., 2025b). DRAs extend LLMs by incorporating dynamic reasoning, adaptive planning,
and iterative tool use to acquire, aggregate, and analyze external information (Huang et al., 2025),
thereby enabling end-to-end research workflows and the production of structured, comprehensive
reports.
Despite these advances, to fully realize the potential of these intelligent systems in everyday human
contexts, they must be able to adapt their behaviors and interactions to the specific needs of dif-
ferent users (Fischer, 2001; Kirk et al., 2024; Rafieian & Yoganarasimhan, 2023), a quality known
as personalization. Important real-world decisions, from choosing a vehicle to making an invest-
ment, are strongly influenced by a user’s unique needs, preferences, budget, and prior knowledge.
In these scenarios, the agent’s value lies not only in generating a comprehensive report, but also in
acting as a personalized assistant that tailors its information filtering, reasoning, and recommenda-
tions. However, this critical dimension of personalization is a major blind spot for current evaluation
methodologies.
Existing deep research benchmarks, including close-ended suites like GAIA, BrowseComp, HLE,
and X-Bench (Mialon et al., 2023; Wei et al., 2025; Phan et al., 2025; Chen et al., 2025a) and open-
∗ Equal Contribution.
† Corresponding Author.
1
arXiv:2509.25106v1  [cs.CL]  29 Sep 2025Work in progress
ended ones like DeepResearch Bench, ResearcherBench, and DeepResearchGym (Du et al., 2025;
Xu et al., 2025; Coelho et al., 2025), focus exclusively on factual accuracy and comprehensiveness,
failing to assess user-specific adaptation. Conversely, existing personalization benchmarks such as
LaMP, PersonaGym, PersonaLens and PersonaFeedback (Salemi et al., 2024; Samuel et al., 2025;
Zhao et al., 2025; Tao et al., 2025) are confined to narrow domains like dialogue or recommendation
and do not address the complex deep research. To the best of our knowledge, our work is the first
to systematically incorporate personalization into the evaluation of DRAs, filling a critical gap in
current research.
To address this gap, we introducePersonalized Deep Research Bench, a novel benchmark specif-
ically designed to evaluate personalization in deep research agents. Our benchmark provides a
rigorous framework for assessing how well agents can integrate user profiles into their research
workflows, and whether their outputs are not only comprehensive and accurate, but also tailored and
practically useful for the end user. By formalizing and evaluating this missing dimension, our work
paves the way for the development of more effective and genuinely personal AI assistants.
Our main contributions are summarized as follows:
• We formally introduce the task ofpersonalized deep research, which extends beyond
generic information synthesis by requiring DRAs to adapt retrieval, reasoning and reporting
to user personas.
• We proposePersonalized Deep Research Bench, the first benchmark specifically targeting
personalization in DRAs. It consists of 50 diverse tasks that span 10 domains and are paired
with 25 real-world user profiles, yielding 250 unique user-task pairs, enabling systematic
evaluation of both task complexity and persona-driven adaptation.
• We develop thePQR Evaluation Framework, a novel and comprehensive methodology
that evaluates generated reports along three orthogonal dimensions: (P)Personalization
Alignment, (Q)Content Quality, and (R)Factual Reliability, providing a holistic measure
of agent utility in real-world research scenarios.
• We conduct extensive experiments across a broad spectrum of open-source DRAs, com-
mercial deep research systems, LLMs with search tools and advancing memory systems,
revealing both strengths and limitations in handling personalization.
2 RELATEDWORK
2.1 EVALUATINGDEEPRESEARCHCAPABILITIES
Evaluating DRAs requires benchmarks that go beyond traditional QA tasks to assess multi-turn
retrieval, tool use, and structured report generation. Close-ended benchmarks such as GAIA,
BrowseComp, HLE, and X-Bench (Mialon et al., 2023; Wei et al., 2025; Chen et al., 2025a) of-
fer controlled evaluations, yet rely on synthetic tasks and fall short of reflecting the challenges of
authentic research scenarios. Recently, open-ended deep research benchmarks have been proposed
to specifically evaluate deep research capabilities. DeepResearch Bench (Du et al., 2025) offers 100
PhD-level tasks across 22 fields, introducing the RACE and FACT frameworks for report quality
and retrieval assessment. Mind2Web 2 (Gou et al., 2025) features 130 real-world tasks with live
web browsing and proposes the Agent-as-a-Judge framework for automated correctness and attri-
bution. ResearcherBench (Xu et al., 2025) focuses on 65 frontier AI questions across 35 subjects
with a dual rubric–factual evaluation. Additionally, BrowseComp-Plus (Chen et al., 2025b) extends
BrowseComp (Wei et al., 2025) by pairing each query with curated documents and challenging neg-
atives to isolate retriever and LLM contributions. DeepResearchGym (Coelho et al., 2025) provides
an open-source sandbox with reproducible search APIs and standardized protocols for transparent,
low-cost benchmarking. Nevertheless, these benchmarks focus on general research capabilities and
lack metrics for personalization—the alignment of research with user-specific goals and preferences.
2.2 BENCHMARKINGPERSONALIZATIONPERFORMANCE
Meanwhile, most personalization benchmarks focus on general tasks and remain insufficient for
complex deep research scenarios. LaMP (Salemi et al., 2024) introduces seven classification and
2Work in progress
Deep Research Task Formulation
10 Domains
Education Career Health Travel Finance …
Domain Experts
… Committee
50 Deep Research Tasks
ID: 1 
Domain: Education 
Task: I want to advance my educational 
background, I plan to apply for PhD in…
…Task 2 Task 3 Task 4
25 Volunteers
   Authentic User Profile Construction
   Structured Schema 
   age, gender, education, 
occupation,, family, preference …
User 1 
age: 20, gender: male, education: 
student major in CS…
…
Explicit  
Personas
Personalized 
Context
User 1 
User interactions on phone 
• logs daily events & preferences 
• dialogue with phone assistant
…
User Proﬁle
Personalized Query Formulation                    
50 Deep  
Research TasksTask Selection
Committee 
Review
250 Personalized Deep Research Queries
Task - User  
Pairs
User  
Proﬁle
Explicit  
Persona
Task1: User1, 3, 5, 7, 8
Task2: User2, 4, 9, 16, 23
Task50: User3, 10, 12, 17, 25
…
Review
Feedback
Domain 
Speciﬁcation
Task Design
Annotators
Personalized 
Context
Figure 1: Benchmark Construction Pipeline: (1) Design 50 deep research tasks across 10 domains;
(2) Build authentic user profiles from 25 volunteers; (3) Generate 250 personalized task-user pairs.
generation tasks to evaluate the personalized output capacity of LLMs. PersonaGym (Samuel et al.,
2025) introduces PersonaScore to evaluate the adherence of LLM agents to assigned personas at
scale. PersonalLLM (Zollo et al., 2025) uses reward models to act as different user personas to
evaluate response preference. AI Persona (Wang et al., 2024) concentrates on the lifelong learn-
ing of user profiles with LLM-as-a-judge evaluation. Additionally, PersonaMem (Jiang et al., 2025)
benchmarks the adaptability of LLMs to evolving user personas. PersonaFeedback (Tao et al., 2025)
provides a large human-annotated benchmark for response tailoring to explicit personas. Person-
aLens (Zhao et al., 2025) introduces LLM-based user and judge agents to assess personalization and
task success in realistic dialogues.
Overall, current benchmarks either neglect personalization or fail to capture the complex nature of
deep research, highlighting the pressing need for a new benchmark specifically designed to measure
the personalized performance of DRAs.
3 BENCHMARK CONSTRUCTION
To rigorously evaluate the personalized capabilities of deep research agents, we introduce the Per-
sonalized Deep Research Bench, a benchmark designed to mirror the real-world personalized deep
research scenarios. Its construction is grounded in two key components: a diverse set of deep re-
search tasks and a collection of authentic, multifaceted user profiles, as shown in Figure 1.
3.1 DEEPRESEARCHTASKFORMULATION
Domain Specification and Task Generation.To begin, we defined a set of 10 distinct domains,
D={d 1, d2, ..., d10}covering major and impactful aspects of daily life (e.g., Education, Career
Development, Healthcare, Financial Planning). To ensure that the tasks within each domain are both
realistic and practically relevant, we collaborated with a diverse group of domain experts, such as
travel bloggers, financial advisors and educational consultants, to design the initial set of tasks.
Committee Review and Validation.Each task underwent multistage validation by a commit-
tee of Master’s/PhD researchers, data scientists, and product managers, following three principles:
Complexity (↑): requiring multi-step reasoning, retrieval, and analysis; Clarity (↑): unambiguous
descriptions with clear objectives; Alignment (↑): supporting the scenarios of personalized deep
research.
3