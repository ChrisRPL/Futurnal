# 2508.19493v2.pdf

Mind the Third Eye!Benchmarking Privacy Awareness
in MLLM-powered Smartphone Agents
Zhixin Lin1, Jungang Li2,3, Shidong Pan4, Yibo Shi5, Yue Yao1,†, Dongliang Xu1,†
1Shandong University
2Hong Kong University of Science and Technology (Guangzhou)
3Hong Kong University of Science and Technology
4Columbia University
5Xi’an Jiaotong University
Abstract
Smartphones bring significant convenience to users but also
enable devices to extensively record various types of personal
information. Existing smartphone agents powered by Multi-
modal Large Language Models (MLLMs) have achieved re-
markable performance in automating different tasks. How-
ever, as the cost, these agents are granted substantial access
to sensitive users’ personal information during this operation.
To gain a thorough understanding of the privacy awareness of
these agents, we present the first large-scale benchmark en-
compassing 7,138 scenarios to the best of our knowledge. In
addition, for privacy context in scenarios, we annotate its type
(e.g., Account Credentials), sensitivity level, and location. We
then carefully benchmark seven available mainstream smart-
phone agents. Our results demonstrate that almost all bench-
marked agents show unsatisfying privacy awareness (RA),
with performance remaining below 60% even with explicit
hints. Overall, closed-source agents show better privacy abil-
ity than open-source ones, and Gemini 2.0-flash achieves the
best, achieving an RA of 67%. We also find that the agents’
privacy detection capability is highly related to scenario sen-
sitivity level, i.e., the scenario with a higher sensitivity level
is typically more identifiable. We hope the findings enlighten
the research community to rethink the unbalanced utility-
privacy tradeoff about smartphone agents.
Homepage — https://zhixin-l.github.io/SAPA-Bench/
Code — https://github.com/Zhixin-L/SAPA-Bench
Dataset —
https://huggingface.co/datasets/OmniQuest/SAPA-Bench
Introduction
With the rapid advancement of multimodal large language
models (MLLMs) (Bai et al. 2025b; Zhu et al. 2025; Xun
et al. 2025; Dang et al. 2024) and smartphone agents (Jiang
et al. 2025; Ma, Zhang, and Zhao 2024; Dai et al. 2025;
Wang et al. 2025), users increasingly rely on intelligent as-
sistants to automate routine tasks such as sending messages,
ordering takeout, and online shopping. While these agents
greatly enhance efficiency and streamline workflows, they
also gain extensive access to sensitive user data during oper-
ation, including screen content, typed text, and system per-
missions. This increasing level of intrusiveness raises sub-
stantial concerns regarding the utility-privacy trade-off.
† Corresponding authors.
Text “P@ssWord” 
in the textbox.
Task:Enter My Pin “P@ssWord” to Login.
This will expose your login 
credentials, for security and 
privacy reasons, will you be 
sure if you really want to do 
this?
User Input Instruction
(Privacy-sensitive)
Execute Privacy-related 
Actions
Warn Users Before 
Executing Actions
Sensitive 
Information?
Response
Figure 1: Motivation of SAPA-Bench. Left: Current agents
often execute sensitive commands ( e.g., entering a PIN)
without privacy checks, posing risks. Middle: We redefine
this process by introducing a privacy-aware module that de-
tects sensitive input, warns the user, and proceeds only upon
confirmation shown in Right.
Existing evaluations mainly focus on the capability of
agents, employing metrics such as task completion rate (Xu
et al. 2025), interaction latency (Wang et al. 2024b), or re-
source consumption (Deng et al. 2024; Dai et al. 2025), but
they lack a systematic, quantitative assessment of the pri-
vacy awareness of agents. Benchmarks such as Android-in-
the-Wild (Rawles et al. 2023) and GUI Odyssey (Lu et al.
2024), primarily serve as standard frameworks to evaluate
agent competencies across diverse task categories.
However, in practice, users also care whether agents can
accurately identify and properly handle privacy-sensitive
content, such as location data, account credentials, or call
logs. Studies show that LLM-driven smartphone agents lack
real-time leakage detection and calls for visual privacy warn-
ings at key interactions (Tang et al. 2025). Also, despite
advances in multimodal understanding, existing agents still
miss dedicated modules for identifying sensitive data ( e.g.,
location, contacts) or requesting user confirmation (Liu et al.
2025). As shown in Figure 1, the absence of a unified bench-
mark and dedicated metrics makes it difficult to compare
the privacy awareness of agents and obstructs privacy-driven
agent design.
To address this gap, we introduce SAPA-Bench, the first-
ever large-scale benchmark specifically designed to evaluate
the privacy awareness of smartphone agents. SAPA-Bench
comprises 7,138 real-world scenarios, and each scenario is
annotated for privacy presence, leakage modality (image or
arXiv:2508.19493v2  [cs.CR]  3 Sep 2025instruction), privacy category, risk severity, and the expected
risk prompt. Building on this dataset, we define five special-
ized evaluation metrics-Privacy Recognition Rate (PRR),
Privacy Localization Rate (PLR), Privacy Level Awareness
Rate (PLAR), Privacy Category Awareness Rate (PCAR),
and Risk Awareness (RA) to quantify an agent’s capabilities
in privacy recognition, localization, classification, severity
estimation, and risk response, respectively.
We conduct a comparative evaluation of seven main-
stream representative smartphone agents, including those
driven by open-source and closed-source models. Our re-
sults reveal that most existing agents perform poorly in pri-
vacy awareness, with performance remaining below 60%
even with implicit hints; overall, closed-source models
slightly outperform open-source ones, and there exists a no-
table positive correlation between a model’s privacy sensi-
tivity and scenario sensitivity. Furthermore, our results in-
dicate that augmenting inputs with targeted prompt signals
substantially improves the ability of the models to detect
sensitive content to privacy. The main contributions of this
work are:
• We construct SAPA-Bench, a dedicated benchmark for
privacy-aware smartphone agents that unlike prior secu-
rity benchmarks such as MobileSafetyBench(Lee et al.
2024) covers the full privacy perception pipeline: recog-
nition, localization, classification, severity estimation,
and risk warning evaluation.
• We propose five specialized privacy metrics (PRR, PLR,
PLAR, PCAR, RA), enabling the first quantitative, eval-
uation of agents’ privacy understanding and response ca-
pabilities.
• We evaluate mainstream smartphone agents to reveal key
privacy awareness bottlenecks and highlight trade-offs
between performance and privacy, show that models with
greater scenario sensitivity detect privacy more effec-
tively, and demonstrate that adding targeted prompt hints
can improve detection while maintaining usability.
We envision that SAPA-Bench will serve as an extensible,
privacy-focused evaluation platform, guiding the commu-
nity toward smarter, safer smartphone agents that strike an
optimal balance between functionality and privacy protec-
tion.
Related Work
Smartphone Agent powered by MLLM
Existing mainstream research on mobile agents are mainly
powered by MLLMs (Liu et al. 2025; Wu et al. 2024a). To
better adapt to diverse tasks such as UI parsing, multi-step
action planning, or cross-app reasoning, these systems often
dynamically switch or fine-tune different MLLM backbones
(e.g., GPT-4o, Gemini, or customized vision-language mod-
els). MLLM enables mobile agents to jointly understand and
reason over both visual ( e.g., UI screenshots) and textual
(e.g., instructions) inputs, allowing for more flexible, gen-
eralizable, and human-aligned interaction.
Specifically, early systems like AppAgent (Li et al.
2024b) pioneered a two-phase “exploration–deployment”
pipeline: during exploration it passively observes UI el-
ements to build a knowledge base. Mobile-Agent (Wang
et al. 2024a) followed with a fully vision-driven framework
that uses only screenshots as input, achieving high preci-
sion multi-step operations and introducing the Mobile-Eval
benchmark. Subsequent methods Show-UI (Lin et al.
2025) with visual-token selection and streaming inference,
and SpiritSight Agent (Huang et al. 2025) with univer-
sal block parsing further improved UI localization and
cross-platform understanding efficiency. However, none of
these single-agent approaches incorporates mechanisms for
detecting privacy-sensitive operations or issuing risk warn-
ings. While these frameworks significantly advance task
success rates and robustness, they commonly neglect to add
modules to particularly response to potential privacy risks.
Existing Privacy Evaluation Frameworks
Existing standards and guidelines offer general frameworks
for privacy impact assessment, but they have not been di-
rectly adapted to smartphone agents. One study (Iwaya et al.
2024) surveyed privacy impact assessment (PIA) method-
ologies and emphasized the need to cover a spectrum of risks
from low to high in real-world settings. Similarly, another
paper (Sangaroonsilp et al. 2023) introduced a three-tier tax-
onomy (high/medium/low) for classifying privacy require-
ments in issue reports, providing a reference for multi-level
risk assessment. Such multi-level privacy annotations could
serve as a valuable standard for systematically evaluating
and benchmarking the privacy awareness of smartphone
agents.
Motivation
Existing benchmarks, such as SPA-bench (Chen et al.
2024a) and GUI-odyssey (Lu et al. 2024), have attempted to
diversify task types, increase task volume and complexity,
and introduce more sophisticated scenarios to challenge the
problem-solving capabilities of smartphone agents. As secu-
rity and privacy becomes a increasing concern when using
smartphone agents, SIUO (Wang et al. 2024c) and related
works concentrate on hazardous behaviors, criminal activi-
ties, and other security domains. Other benchmarks, such as
MobileSafetyBench (Lee et al. 2024), focus on behavioral
safety in benign versus harmful tasks, penalizing agents for
overstepping predefined boundaries.
However, existing benchmarks overlook a crucial issue:
When the model fails to recognize that an operation in-
volves personal privacy information, no notice is raised.
The Fair Information Practice Principles (FIPP), first intro-
duced by the U.S. Department of Health, Education, and
Welfare in 1973, emphasize transparency through user no-
tification and informed choice (Pan et al. 2024). These
principles later evolved into the “notice-and-choice privacy
framework,” forming the conceptual basis of contemporary
privacy regulations, such as the European General Data
Protection Regulation (GDPR). In both common deploy-
ment architectures, i.e., on-device agents and cloud-based
end-to-end agents, sensitive actions ( e.g., reading a pass-
word from the clipboard or uploading a user’s contact list)Stage 1: Automatic Generation 
of Instruction & Response
Stage 4: Final Human 
Verification
Stage 0:Data 
Cleaning & Sampling
Filter about 
80 K raw 
screenshots 
from GUI 
Odyssey and 
OS-Atlas.
Detect 
screenshots 
potentially 
containing 
sensitive
privacy.
Generate 
instruction–
response 
pairs
Stage 2: Initial 
Human Review
Manuallyreview
Stage 3: Automatic Annotation 
of Remaining Fields
Generate 
leakage 
position, 
sensitivity 
level, and 
privacy 
category
Manuallyreview
About 40 hours
Figure 2: Five-stage annotation pipeline of SAPA-Bench.
GPT-4o and human annotators collaboratively label privacy-
sensitive ground truth. Specifically, stage 0 cleans and sam-
ples raw screenshots; stages 1 and 3 automatically generate
privacy-sensitive ground truth; stages 2 and 4 conduct hu-
man verification.
execute automatically without prompting users, thus deny
users of the opportunity to potentially intervene. When con-
fronted with privacy-related requests, an agent must not only
recognize the private nature and sensitivity of the content but
also proactively alert users before execution; only then can
agent be deemed to possess robust privacy-handling capabil-
ities.
To address this gap, we propose the SAPA-Bench, de-
signed to systematically evaluate agents on comprehension
and handling of privacy-sensitive operations. Specifically,
an agent should accurately identify privacy-related requests
and provide appropriate mitigation strategies. For example,
when a user asks the agent to paste a password from the
clipboard into an application’s login field, the agent should
recognize this as a privacy-sensitive operation rather than
boldly executing the command. Simultaneously, the agent
should prompt the user: “This operation may cause unneces-
sary consequences or troubles; for security and privacy rea-
sons, do you wish to proceed?” Achieving this requires a
deep understanding of contextual scenarios and nuanced pri-
vacy semantics. We specifically introduce the SAPA-Bench
and its construction in the subsequent section.
Smartphone Agent Privacy Awareness:
SAPA-Bench
Privacy in Smartphone
Inspired by large-scale user perspective on mobile app pri-
vacy (Nema et al. 2022) and the structured privacy tax-
onomies adopted in Apple and Google’s official privacy la-
bel frameworks (Ali et al. 2024; Khandelwal et al. 2023), we
classify privacy leakage into eight categories by jointly con-
sidering the operation type and the app category including:
1. Account Credentials (AC), 2. Personal Information (PI),
3. Financial and Payment (FP), 4. Communication Content
(CC), 5. Location and Environment (LE), 6. Device Per-
missions (DPO), 7. Media and Files (MF), 8. Behavior and
Browsing (BPBH).
Based on previous studies (Chen et al. 2024a; Li et al.
2024a), we further categorize these eight categories into
three privacy-sensitivity levels from low, medium, and high,
to enable fine-grained evaluation on agents. This stratifica-
tion is grounded in the degree to which an action may ex-
pose sensitive user information in real-world mobile inter-
actions. Specifically, high-sensitivity content refers to oper-
ations involving rich personal information, such as chat mes-
sages containing ID numbers or account credentials, precise
location sharing, device-level permission grants, or entry
of financial credentials. Medium-sensitivity content covers
moderately private data commonly encountered in daily life,
such as coarse location sharing, meeting links, or casual chat
messages. In contrast, low-sensitivity content includes rou-
tine behavioral traces like browsing history, cart additions,
viewing task status, or sharing public content, which rarely
reveal identifiable personal data.
Benchmark Characteristics
Our benchmark, namely as SAPA-Bench ( Smartphone
Agent Privacy Awareness Benchmark), comprises a suite
of 7,138 scenarios drawn from 50 popular English-language
smartphone applications, e.g., Instagram and Google map,
from various categories. Each scenario is based on a UI
Screenshot, which is a snapshot of the interface where a pri-
vacy leak may occur. For the sake of consistency and com-
parativeness, all UI screenshots are sampled from the GUI-
Odyssey (Lu et al. 2024) and OS-Atlas (Wu et al. 2024b)
task suites.
To comprehensively evaluate the agents capabilities on
privacy awareness, for each scenario, we further annotate the
following privacy-related information:
• Instruction & Response: Instruction: the user’s prompt
to describe the operation ( e.g., “Enter password
P@sswOrd to log in”). Response: the model’s expected
response (e.g., “This action may send your message his-
tory and leak private information. Are you sure you want
to proceed?”).
• Exposure Modality & Location : whether the privacy
exposure occurs in the screenshot or in the instruction.
• Privacy Sensitivity Level : three tiers—Low, Medium,
and High—reflecting the degree of sensitivity.
• Privacy Type: eight categories ( e.g., Account Creden-
tials, Financial and Payment etc.).
Dataset Statistics. SAPA-Bench comprises 7,138
privacy-sensitive tasks across eight main categories and
27 sub-categories, two privacy exposure modality, three
sensitivity levels, and thousands tailored instruction &
response pairs. Figure 2 shows the number of sub-category
per category broken down by risk level (Low, Medium,
High) and cases in SAPA-Bench. The “Personal Informa-
tion” and “Communication Content” categories exhibit the
largest High-Risk proportions, each exceeding 40% of their
totals, whereas “Device Permissions and Operation” and
“Financial and Payment” are dominated by Medium-Risk or
Low-Risk tasks. This uneven distribution mirrors real-world
differences in privacy sensitivity: for example, entering
account credentials or pasting an email address is inherently
high-risk, while browsing history or setting behavioral
preferences generally poses lower or medium risk. Thus,