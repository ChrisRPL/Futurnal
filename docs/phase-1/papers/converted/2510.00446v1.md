# 2510.00446v1.pdf

LongCodeZip: Compress Long Context for Code
Language Models
Yuling Shi1, Yichun Qian 2, Hongyu Zhang 3, Beijun Shen 1, Xiaodong Gu 1∗
1Shanghai Jiao Tong University, Shanghai, China
2Stanford University, Stanford, CA, USA
3Chongqing University, Chongqing, China
{yuling.shi, bjshen, xiaodong.gu}@sjtu.edu.cn, ycqian@stanford.edu, hyzhang@cqu.edu.cn
Abstract—Code generation under long contexts is becoming
increasingly critical as Large Language Models (LLMs) are
required to reason over extensive information in the code-
base. While recent advances enable code LLMs to process
long inputs, high API costs and generation latency remain
substantial bottlenecks. Existing context pruning techniques, such
as LLMLingua, achieve promising results for general text but
overlook code-specific structures and dependencies, leading to
suboptimal performance in programming tasks. In this paper, we
propose LongCodeZip, a novel plug-and-play code compression
framework designed specifically for code LLMs. LongCodeZip
employs a dual-stage strategy: (1) coarse-grained compression,
which identifies and ranks function-level chunks using conditional
perplexity with respect to the instruction, retaining only the
most relevant functions; and (2) fine-grained compression, which
segments retained functions into blocks based on perplexity and
selects an optimal subset under an adaptive token budget to
maximize relevance. Evaluations across multiple tasks, including
code completion, summarization, and question answering, show
that LongCodeZip consistently outperforms baseline methods,
achieving up to a 5.6× compression ratio without degrading
task performance. By effectively reducing context size while
preserving essential information, LongCodeZip enables LLMs to
better scale to real-world, large-scale code scenarios, advancing
the efficiency and capability of code intelligence applications 1.
I. INTRODUCTION
LLMs specialized for code have revolutionized software
development by demonstrating remarkable capabilities in code
completion [1], [2], translation [3], [4], program synthesis [5],
[6], [7] and program repair [8], [9]. Models like DeepSeek-
Coder [10], Qwen2.5-Coder [11], Seed-Coder [12] can reason
over diverse programming languages and significantly enhance
productivity. As code LLMs are increasingly deployed for
real-world tasks like repository-level question answering [13]
and long-context code completion [1], there is a growing
demand for handling contexts that span tens of thousands
of tokens. This need has motivated efforts to extend LLM
context windows [14], [11], [15]. However, effective handling
of long code contexts remains a central bottleneck. Three
major challenges arise in such long code context scenarios.
First, as the input context grows, the quadratic complexity of
the transformer attention mechanism [16] leads to decreased
1Our code and data are available at https://github.com/YerbaPage/
LongCodeZip
* Xiaodong Gu is the corresponding author
generation efficiency. At the same time, processing longer
inputs with LLMs results in rapidly increasing API costs,
especially when pricing models are expensive [17], [18].
Second, LLMs struggle to identify and utilize relevant content
amid lengthy inputs [19], [20]. Third, even though recent
LLMs support extended context windows to 128k tokens, these
limits can still be reached when processing large files and
long conversation histories, leading to context truncation and
degraded outputs [21].
These issues are particularly pronounced in code LLMs.
Unlike natural language text, source code is highly struc-
tured with complex dependencies spanning across functions,
classes, and files. Dependencies between variable declarations,
function definitions, and their uses often extend beyond what
current context windows can accommodate. As a result, LLMs
frequently produce code that fails to compile, violates existing
patterns, or ignores critical constraints when the relevant
context exceeds their window size [22]. Consequently, context
compression has emerged as a key demand for enabling long-
context code understanding.
Existing approaches to address long context limitations
have notable shortcomings when applied to code. General
text compression methods like LLMLingua [23] and Selective
Context [24] fail to account for code-specific characteristics
and often break code structure. Retrieval-augmented genera-
tion (RAG) [25] reduce context length by selecting relevant
code snippets from the repository context, but it merely rely
on text similarities, and may overlook implicit dependencies
within the context. Traditional code compressors such as Di-
etCode [26] and SlimCode [27] improve syntax and structure
awareness but are generally limited to function-level pruning
or short code examples, leaving compression of long context
for code largely unaddressed.
To overcome these limitations, we introduce LongCodeZip,
a training-free, model-agnostic, and plug-and-play context
compression framework for code LLMs. Our approach lever-
ages the inherent structure of code through a novel two-
stage compression strategy that preserves code semantics
while significantly reducing token consumption. First, we per-
form coarse-grained compression by identifying and ranking
function-level chunks based on their relevance to the instruc-
tion. Then, within the selected functions, it applies perplexity-
based block detection followed by fine-grained block-level
1
arXiv:2510.00446v1  [cs.CL]  1 Oct 2025# In config.py...classConfig:def__init__(self, lr=1e-3, epochs=10,beta1=0.9,beta2=0.999, weight_decay=0.01):self.lr= lrself.beta1= beta1self.beta2= beta2...
# In config.py...classConfig:def__init__(self, lr=1e-3, epochs=10,beta1=0.9,beta2=0.999, weight_decay=0.01):self.lr= lrself.beta1= beta1self.beta2= beta2...
#In utils.py...defcosine_similarity(vec_a, vec_b):dot= sum(a*bfora, binzip(vec_a, vec_b))norm_a= math.sqrt(sum(a*aforainvec_a))norm_b= math.sqrt(sum(b*bforbinvec_b))returndot/ (norm_a* norm_b)...
#In utils.py...defcosine_similarity(vec_a, vec_b):dot= sum(a*bfora, binzip(vec_a, vec_b))norm_a= math.sqrt(sum(a*aforainvec_a))norm_b= math.sqrt(sum(b*bforbinvec_b))returndot/ (norm_a* norm_b)...
classAccount:def__init__(self, user_id, email):self.user_id= user_idself.email= email...defget_account_by_id(self,user_id):account= db.query_account(user_id)returnaccount...
defget_email_by_id(user_id: int) -> str:# TODO: get the email by user_id# Expected completion:# account = get_account_by_id(user_id)
classConfig:def__init__(self, lr=1e-3, epochs=10,beta1=0.9,beta2=0.999, weight_decay=0.01):self.lr= lrself.epochs=epochsself.beta1= beta1self.beta2= beta2self.weight_decay=weight_decay...
deftrain_model(model, dataloader, config: Config):# TODO: set up optimizer# Expected completion:# optimizer =torch.optim.AdamW(lr=config.lr,...)
RelatedCodefromLargeCodebase
Codetobecompleted CodetobecompletedSimilarity:1st
MutualInfo:1st Similarity:9th
SimilarityRelevance
 DependencyRelevance
RelatedCodefromLargeCodebase
MutualInfo:1st
Fig. 1: Challenge for RAG, a similariy-based context compression method.
compression to maximize relevance under an adaptive token
budget. To the best of our knowledge, LongCodeZip is the
first framework specifically designed for long-context code
compression and to introduce perplexity-based block detec-
tion, providing an efficient and general-purpose solution that
preserves task-critical content within strict token limitations.
We evaluate LongCodeZip across multiple code benchmarks
with long contexts, including Long Code Completion [1],
Long Module Summarization [21], and RepoQA [13]. Re-
sults demonstrate that our approach achieves up to a 5.6×
compression ratio without sacrificing performance, generalizes
well across tasks and models (even with only 0.5B model as
the compressor), and significantly reduces generation time and
token costs.
Our main contributions include:
1) A novel long-context, code-specific hierarchical compres-
sion approach that performs function-level chunking and
selection, followed by perplexity-based block detection
and block-level pruning.
2) An adaptive budget allocation and 0/1 knapsack selection
mechanism that prioritizes relevant blocks and maximizes
critical detail within the token budget.
3) A comprehensive evaluation demonstrating that Long-
CodeZip outperforms baselines on code completion, sum-
marization, and question answering tasks, achieving up to
a 5.6× compression ratio without sacrificing performance.
II. MOTIVATION
Code generation under long context is becoming increas-
ingly important in LLM-based software development. Such
tasks often require referencing numerous related files across
an entire project repository, resulting in input contexts that
span tens of thousands of tokens. However, these long contexts
typically contain scattered and redundant information, which
can distract the model and degrade output quality. Moreover,
the substantial computational cost of processing such large
inputs further exacerbates latency and resource constraints,
creating a significant bottleneck for practical deployment.
Retrieval-augmented generation (RAG) [28], [29] provides
an efficient way to condense overly lengthy contexts. RAG
retrieves and appends relevant code snippets to the prompt,
leveraging embedding models such as UniXcoder [30] or
CodeBERT [31], and similarity measures such as cosine
similarity. While RAG effectively reduces context length, it
primarily relies on surface-level lexical similarity between
snippets. Consequently, it often fails to capture code segments
with deeper semantic or functional dependencies—particularly
when such relationships are implicit, abstracted, or span mul-
tiple components.
Consider the examples in Figure 1. In the first scenario, the
task is to complete anget_email_by_idfunction. Retriev-
ingAccountclass and theget_account_by_idfunction
proves effective, as they share similar function and parameter
names. In this case, RAG works well due to strong lexical
and structural overlap. In the second scenario, however, the
task is to implement atrain_modelfunction that relies on
configuration values defined in a separateConfigclass. Here,
crucial context likeConfigis often missed, since RAG may
not identify these implicit or non-lexical dependencies. This
omission can lead to incomplete or incorrect code generation.
This example highlights the need for context selection
criteria that extend beyond surface-level similarity. In both
scenarios, an effective similarity measure should assign high
relevance toget_account_by_idin the first case and,
critically, toConfigin the second—even when there is
minimal lexical overlap between the configuration class and
the training function.
III. METHODOLOGY
A. Problem Formulation
Given a long code contextc={c 1, . . . , cn}withntokens
and a task instructionq={q 1, ..., qm}, the goal of context
compression is to produce a compressed contextc ′ ⊆c
such that|c ′| ≤B, whereBis the computational budget
in tokens. The objective is to maximize task performance
while satisfying the budget constraint. For instance, in the
code completion task, the instruction could be:”Complete the
following function [code to be completed]”. The long context
could consist of the unfinished code along with retrieved code
snippets.
2LLM
Approx.MutualInformation
classConfig:def__init__(self,…):self.lr= lrself.epochs=epochs...def__str__(self,…):returnf"Config(…}, \epochs=…\..."... TokenPerplexity
512tokens
# In config.py...classConfig:def__init__(self, lr=1e-3, epochs=10,beta1=0.9,beta2=0.999, weight_decay=0.01):self.lr= lrself.beta1= beta1self.beta2= beta2...
# In config.py...classConfig:def__init__(self, lr=1e-3, epochs=10,beta1=0.9,beta2=0.999, weight_decay=0.01):self.lr= lrself.beta1= beta1self.beta2= beta2...
classConfig:def__init__(self, lr=1e-3, epochs=10,beta1=0.9,beta2=0.999, weight_decay=0.01):self.lr= lrself.epochs=epochsself.beta1= beta1self.beta2= beta2self.weight_decay=weight_decay...
#Pleasecompletethefunction:deftrain_model(model, dataloader, config: Config):# TODO: set up optimizer# Expected completion:# optimizer =torch.optim.AdamW(lr=config.lr,...)
RelatedCodefromCodebase
Insturction
BlockSelection
#Relevantcodeblocks:classConfig:def__init__(self,…):self.lr= lr...#Pleasecompletethefunction:deftrain_model(model,…):
CompressedContext
AMI=0.4AMI=0.2AMI=0.3
 RelevantFunction
FunctionSelection
①Coarse-grained Compression
②Fine-grained Compression
Fig. 2: Overview of the LongCodeZip framework.
Rather than relying solely on embedding similarity between
qandc, we propose to select context snippets based on their
mutual information, specifically, how much they reduce the
perplexity (PPL) of generatingq. Specifically, for each candi-
date contextc, we define the approximated mutual information
AMI(c, q)as the reduction in perplexity whencis provided:
AMI(c, q) = PPL(q)−PPL(q|c)(1)
wherePPL(q|c)is the conditional perplexity ofqgivenc,
lower values indicate higher likelihood ofq[23]:
PPL(q|c) = exp
 
− 1
N
NX
i=1
logP(q i|q<i, c)
!
(2)
Similarly,PPL(q)denotes the perplexity ofqwithout the
context:
PPL(q) = exp
 
− 1
N
NX
i=1
logP(q i|q<i)
!
(3)
Here,Pdenotes the model’s next-token prediction probability,
andq <i is the sequence of preceding tokens beforeq i. A
higher AMI score indicates thatcenables the model to better
predictq, capturing both surface-level and dependency-based
relevance. We therefore compress long contexts by retaining
code snippets with the highest mutual information, ensuring
that the most essential information for code generation is
preserved.
B. Overview
The overview of LongCodeZip is illustrated in Figure 2.
Given input of long source code, a task instruction, and a token
budget, LongCodeZip follows acoarse-to-finecompression
pipeline. In the coarse-grained compression stage (Section
III-C), the source code is divided into function-level chunks,
which are ranked by their relevance to the instruction using
conditional perplexity. The topNfunctions are then selected
under a coarse budget, effectively filtering out irrelevant code
and avoiding unnecessary computation. In the fine-grained
compression stage (Section III-D), each retained function is
further segmented into semantic blocks via perplexity-based
chunking. An adaptive retention ratio is assigned to each
function according to its estimated importance. Within each
function, the most relevant blocks are selected by formulating
the problem as a 0/1 knapsack optimization, ensuring that the
retained content maximizes relevance while fitting within the
allocated token budget.
By combining coarse-grained filtering with fine-grained
pruning, LongCodeZip achieves a balance between aggressive
compression and semantic preservation, thereby improving
both efficiency and task performance.
C. Coarse-Grained Compression: Relevant Function Selec-
tion
The coarse-grained compression aims to select high-level
code chunks that are most relevant to the task instruction. This
process consists of three steps:
Function-Level Chunking.We first split the source code into
chunks along function or class boundaries. Functions naturally
encapsulate coherent logic and exhibit strong modularity [31].
Chunking at this level ensures that retained code segments are
both syntactically valid and semantically self-contained, which
is essential for preserving program integrity.
Instruction-aware Relevance Ranking.To measure the rel-
evance of each chunk to the task instruction, we employ an
instruction-aware ranking mechanism based on approximated
mutual information (1). Chunks are scored and ranked in
descending order, allowing us to prioritize those most infor-
mative for the given task.
Budget-Constrained Function Selection.Finally, we greedily
select the top-ranked chunks under a coarse-grained token
budgetB coarse, which is the division of the final token budget
Bby the configurable fine-grained compression ratioR fine.
This greedy selection balances efficiency and coverage: a
larger budget allows more functions to pass into the fine-
grained stage, potentially improving downstream quality but at
3