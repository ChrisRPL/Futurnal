# 2501.02157v2.pdf

arXiv:2501.02157v2  [cs.CL]  31 May 2025
Personalized Graph-Based Retrieval for Large Language Models
Steven Au1, Cameron J. Dimacali1, Ojasmitha Pedirappagari1,
Namyong Park 2, Franck Dernoncourt3, Yu Wang4, Nikos Kanakaris5,
Hanieh Deilamsalehy3 Ryan A. Rossi3, Nesreen K. Ahmed6
1University of California Santa Cruz, 2Meta AI 3Adobe Research,
4University of Oregon, 5University of Southern California, 6Cisco AI Research
Abstract
As large language models (LLMs) continue
to evolve, their ability to deliver personal-
ized, context-aware responses holds signifi-
cant promise for enhancing user experiences.
However, most existing personalization ap-
proaches rely solely on user history, limit-
ing their effectiveness in cold-start and sparse-
data scenarios. We introduce Personalized
Graph-based Retrieval-Augmented Generation
(PGraphRAG), a framework that enhances per-
sonalization by leveraging user-centric knowl-
edge graphs. By integrating structured user
information into the retrieval process and aug-
menting prompts with graph-based context,
PGraphRAG improves both relevance and gen-
eration quality. We also present the Per-
sonalized Graph-based Benchmark for Text
Generation, designed to evaluate personal-
ized generation in real-world settings where
user history is minimal. Experimental results
show that PGraphRAG consistently outper-
forms state-of-the-art methods across diverse
tasks, achieving average ROUGE-1 gains of
14.8% on long-text and 4.6% on short-text gen-
eration‚Äîhighlighting the unique advantages of
graph-based retrieval for personalization.
1 Introduction
The rapid advancement of large language models
(LLMs) has enabled a wide range of NLP appli-
cations, including conversational agents, content
generation, and code synthesis. Models like GPT-
4 (OpenAI, 2024) now power virtual assistants ca-
pable of answering complex queries and engag-
ing in multi-turn dialogue (Brown et al., 2020).
As these models continue to evolve, their ability
to generate personalized, context-aware responses
offers new opportunities to enhance user experi-
ences (Salemi et al., 2024b; Huang et al., 2022).
Personalization enables LLMs to adapt outputs to
individual preferences and goals, resulting in richer,
more relevant interactions (Zhang et al., 2024).
Retrieval Model
...
user documentsuser attributes
Language Modelùúô! ùúô" ‚Ñ≥input
user i
‚Ä¶
ùë¶"generated textfor user i‚Ñõ
User Data
user interactions
ùîº(ùë¶$,ùë¶)evaluation
ùíô
User Profile
Figure 1: Overview of the proposed PGraphRAG frame-
work. We construct user-centric graphs from user pro-
file and interaction data, then retrieve structured, user-
relevant information from the graph. This context is
used to condition the language model‚Äôs generation, pro-
ducing personalized outputs for user i.
While personalization has been studied in areas
such as information retrieval and recommender sys-
tems (Xue et al., 2009; Naumov et al., 2019), its
integration into LLMs for generation tasks remains
relatively underexplored.
One of the key challenges in advancing per-
sonalized LLMs is the lack of benchmarks that
adequately capture the complexities of personal-
ization tasks. Popular natural language process-
ing (NLP) benchmarks (e.g., (Wang et al., 2019b),
(Wang et al., 2019a), (Gehrmann et al., 2021))
primarily focus on general language understand-
ing and generation, with limited emphasis on per-
sonalization. As a result, researchers and practi-
tioners lack standardized datasets and evaluation
metrics for developing and assessing models de-
signed for personalized text generation. Recently,
efforts such as LaMP (Salemi et al., 2024b) and
LongLaMP (Kumar et al., 2024) have begun ad-
dressing this gap. LaMP evaluates personalization
for tasks like email subject and news headline gen-
eration, while LongLaMP extends this to long-text
tasks such as email and abstract generation. How-
ever, both benchmarks rely exclusively on user his-
tory to model personalization. Here, user history
typically refers to a set of previously written textsby the same user‚Äîsuch as past reviews, messages,
or profile-specific documents‚Äîwhich are used as
context to condition the generation.
Challenges with Cold-Start Users. While leverag-
ing user history is valuable for capturing individual
style and preferences, it presents a cold-start chal-
lenge: many users have little or no prior data. In
fact, as shown in Figure 2, over 99.99% of users
in the Amazon Reviews dataset have fewer than
three interactions. Benchmarks like LaMP and
LongLaMP filter out these users by imposing a min-
imum user profile size threshold to ensure sufficient
data for personalization. As a result, they exclude
the vast majority of users, making their evaluations
less representative of real-world deployment. This
design choice leads to model failures when prompts
lack sufficient context, often resulting in generic
outputs.
Figure 2: Distribution of user profile sizes in the Ama-
zon user-product dataset. The vast majority of users
have only a few reviews, highlighting the prevalence of
sparse profiles. The red vertical line indicates the min-
imum profile size threshold used in prior benchmarks
such as LaMP and LongLaMP.
Proposed Approach. To address these challenges,
we propose Personalized Graph-based Retrieval-
Augmented Generation (PGraphRAG), a novel
framework that enhances personalized text genera-
tion by leveraging user-centric knowledge graphs.
These structured graphs represent user information
‚Äî such as interests, preferences, and prior interac-
tions ‚Äî in an interconnected graph structure. Dur-
ing inference, PGraphRAG retrieves semantically
relevant context from both the user‚Äôs own profile
and neighboring profiles extracted from the graph,
and augments the prompt with this information
to guide generation. This graph-based approach
enables the model to produce contextually appro-
priate and personalized outputs, even when user
history is sparse or unavailable (see Figure 1).
Formally, the target task of PGraphRAG is
personalized text generation conditioned on user-
specific context retrieved from a structured knowl-
edge graph. Given a user query (e.g., a product title
or review prompt), the system retrieves relevant
entries from the graph-based profile and generates
an output tailored to the user‚Äôs preferences. This
setup generalizes personalization beyond pure user
text history, enabling context-rich generation even
in sparse or cold-start settings.
Proposed Benchmark. To evaluate our approach,
we introduce the Personalized Graph-based Bench-
mark for Text Generation, a novel evaluation bench-
mark designed to fine-tune and assess LLMs on
twelve personalized text generation tasks, includ-
ing long- and short-form generation as well as clas-
sification. This benchmark addresses the limita-
tions of existing personalized LLM benchmarks by
providing datasets that specifically target person-
alization capabilities in real-world settings where
user history is sparse. In addition, it enables a more
comprehensive assessment of a model‚Äôs ability to
personalize outputs based on structured user infor-
mation.
Our benchmark supports evaluation in sparse-
profile settings, and PGraphRAG is designed to re-
trieve semantically relevant context not only from
the user‚Äôs own profile but also from neighboring
profiles extracted from the graph ‚Äî enabling effec-
tive personalization even when the user has only a
single input (e.g., one review in their profile). Em-
pirically, PGraphRAG significantly outperforms
LaMP in these low-profile scenarios, demonstrat-
ing the advantages of graph-based reasoning over
strict reliance on user history.
Our contributions are summarized as follows:
1. Benchmark. We introduce the Personalized
Graph-based Benchmark for Text Generation,
consisting of 12 tasks spanning long-form gen-
eration, summarization, and classification. To
support further research, we release the bench-
mark publicly. 1
2. Method. We propose PGraphRAG, a
retrieval-augmented generation framework
that addresses the cold-start problem by aug-
menting generation with structured, user-
specific information from a knowledge graph.
3. Effectiveness. We show that PGraphRAG
achieves state-of-the-art performance across
all tasks in our benchmark, demonstrating the
value of graph-based reasoning for personal-
ized text generation.
1https://github.com/
PGraphRAG-benchmark/PGR-LLMFigure 3: Example of a bipartite user-centric graph
G = (U, V, E) showing users, items, and interaction
edges (e.g., reviews).
2 Personalized Graph-based Benchmark
for LLMs
We introduce thePersonalized Graph-Based Bench-
mark to evaluate LLMs on their ability to gener-
ate personalized outputs across twelve tasks, span-
ning long-form generation, short-form generation,
and ordinal classification. The benchmark is con-
structed from real-world datasets across multiple
domains.
2.1 Personalized Text Generation: Problem
Definition
Each benchmark instance includes: (1) an input
sequence x to the LLM, (2) a target output y the
model is expected to generate, and (3) a user profile
Pi derived from a structured user-centric graph.
Given an input-output pair (x, y) associated with
user i, the goal is to generate a personalized output
ÀÜy that aligns with the semantics and style of y,
conditioned on the user profile Pi.
We assume user context is represented using
a bipartite user-centric graph that captures user-
item interactions (see Figure 3 for an illustration).
The profile Pi is constructed from this graph and
includes both interactions authored by the user and
related signals from similar items or neighboring
users. The full construction of Pi is detailed in
Section 3.
Formally, the personalized generation task is de-
fined as:
ÀÜy = arg max
y‚Ä≤
Pr(y‚Ä≤ | x, Pi) (1)
where x is the input query, y is the target output,
and Pi denotes the profile of user i derived from a
user-item interaction graph. The model generates
an output ÀÜy that maximizes the likelihood of per-
sonalized text conditioned on the input and user
profile. This formulation enables generalization be-
yond user history by leveraging structured, graph-
derived context.
In practice, our framework retrieves a personal-
ized context R(Pi) ‚äÜ Pi from the graph to condi-
tion generation, yielding the operational objective:
ÀÜy = arg max
y‚Ä≤
Pr(y‚Ä≤ | x, R(Pi)) (2)
where R(Pi) represents the retrieved subset of user-
and item-level interactions used as context during
generation.
Finally, statistics for all benchmark tasks and
their associated graphs are summarized in Table 1
and Table 2. Additional dataset split details are
provided in the appendix.
2.2 Task Definitions
Task 1: User Product Review Generation. Per-
sonalized review text generation has progressed
from incorporating user-specific context to utiliz-
ing LLMs for producing fluent and contextually
relevant reviews and titles (Ni and McAuley, 2018).
This task aims to generate a product review itext for
a target user, conditioned on their own review title
ititle and a set of additional reviews Pi from their
user profile. We construct this dataset from the
Amazon Reviews 2023 corpus (Hou et al., 2024),
spanning multiple product categories and used to
define a bipartite user-item graph.
Task 2: Hotel Experience Generation. Hotel
reviews often contain rich narratives reflecting per-
sonal experiences, making personalization essential
to capturing individual preferences and expecta-
tions (Kanouchi et al., 2020). This task focuses on
generating a personalized hotel experience story
itext, using the target user‚Äôs review summary ititle
and contextual reviews Pi. We use the Hotel Re-
views dataset, a subset of Datafiniti‚Äôs Business
Database (Datafiniti, 2017), to construct a user-
hotel bipartite graph.
Task 3: Stylized Feedback Generation. Writing
style ‚Äî influenced by grammar, punctuation, and
expression ‚Äî is deeply personal and often shaped
by geographic and cultural factors (Alhafni et al.,
2024). This task involves generating personalized
product feedback itext, based on the user‚Äôs feedback