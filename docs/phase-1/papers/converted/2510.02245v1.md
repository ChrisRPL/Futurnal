# 2510.02245v1.pdf

Preprint.
EXGRPO: LEARNING TOREASON FROMEXPERIENCE
Runzhe Zhan12∗ Yafu Li2B Zhi Wang3 Xiaoye Qu2 Dongrui Liu2 Jing Shao2
Derek F. Wong1B Yu Cheng4
1University of Macau 2Shanghai AI Laboratory 3Nanjing University
4The Chinese University of Hong Kong
ABSTRACT
Reinforcement learning from verifiable rewards (RLVR) is an emerging paradigm
for improving the reasoning ability of large language models. However, standard
on-policy training discards rollout experiences after a single update, leading to
computational inefficiency and instability. While prior work on RL has highlighted
the benefits of reusing past experience, the role of experience characteristics in
shaping learning dynamics of large reasoning models remains underexplored.
In this paper, we are the first to investigate what makes a reasoning experience
valuable and identify rollout correctness and entropy as effective indicators of
experience value. Based on these insights, we proposeExGRPO(Experiential
GroupRelativePolicyOptimization), a framework that organizes and prioritizes
valuable experiences, and employs a mixed-policy objective to balance exploration
with experience exploitation. Experiments on five backbone models (1.5B-8B
parameters) show that ExGRPO consistently improves reasoning performance on
mathematical/general benchmarks, with an average gain of +3.5/7.6 points over
on-policy RLVR. Moreover, ExGRPO stabilizes training on both stronger and
weaker models where on-policy methods fail. These results highlight principled
experience management as a key ingredient for efficient and scalable RLVR.
Code:
 ExGRPO Models:
 ExGRPO Models
1 INTRODUCTION
Reinforcement learning (RL) has become a pivotal technique for advancing the reasoning capabilities
of language models on complex tasks (Guo et al., 2025; Yang et al., 2025). RL augments language
models’ reasoning by modeling chain-of-thought (CoT) as action sequences optimized under verifiable
rewards (RLVR), laying the groundwork for sophisticated downstream applications (Gridach et al.,
2025). However, a significant yet overlooked challenge persists: due to the on-policy nature of most
RLVR algorithms, the valuable experience generated during the rollout phase is often discarded
after a single gradient update (Shao et al., 2024). This practice not only squanders substantial
computational resources but also forfeits critical opportunities for the model to learn from prior
successful explorations, thereby imposing a bottleneck on scaling RL for reasoning.
Experience replay (Lin, 1992) is a widely adopted technique in RL to address this issue and improve
sample efficiency. As stated in Silver & Sutton (2025), AI is at the cusp of a new period in which
experience will become the dominant medium of improvement. This idea, often referred to as
experience-based RL, leverages the intuition that previously collected interactions (i.e., state-action-
reward tuples) contain valuable information for learning, helping models stabilize training and
accelerate convergence (Mnih et al., 2013). A non-trivial challenge lies in how to exploit past
experiences based on their differing “values” (Schaul et al., 2016) and manage the replay process
according to customized learning schedules (Sujit et al., 2023). However, efficient experience replay
mechanisms remain largely underexplored in RLVR for building large reasoning models (LRMs;
Plaat et al. 2024; Zhang et al. 2025b; Qu et al. 2025). Given the vast quantity of experiences collected
during rollouts, a fundamental question remains:How can the reasoning model effectively exploit its
own stream of experience to maximize learning toward scaling RL compute for LRMs?
∗ Work was done during Runzhe Zhan’s internship at Shanghai AI Laboratory.
B Corresponding authors.
1
arXiv:2510.02245v1  [cs.LG]  2 Oct 2025Preprint.
To address this gap, we begin by examining what constitutes a valuable reasoning experience for
RLVR optimization1. We hypothesize that experience utility varies with measurable properties.
RLVR experience generally consists of a question and its corresponding trajectories. Accordingly, we
study experience properties from these two components. Through systematic analysis, we identify
rollout correctness (for questions) and trajectory entropy (for trajectories) as effective online proxy
metrics for characterizing experience quality. Specifically, tasks of intermediate difficulty and their
associated low-entropy trajectories tend to be beneficial for RLVR optimization.
Building on these insights, we introduceExperiential Group Relative Policy Optimization (ExGRPO),
a novel framework designed to strategically identify, manage, and replay valuable experiences.
ExGRPO maintains a replay buffer of reasoning trajectories derived from partially correct rollouts
and organizes them into buckets according to their correctness levels. To manage the buffer effectively,
it uses a sampling strategy that prioritizes experiences from the most beneficial buckets, along with
the corresponding trajectory with the lowest entropy. This approach allows the model to learn more
efficiently from past experiences that align best with its current capabilities, guided by the principles of
valuable experience concluded in our preliminary analysis. During mini-batch optimization, ExGRPO
uses a mixed-policy optimization objective that balances between leveraging fresh exploration and
reusing strategically selected past experiences, improving both sample efficiency and training stability.
Our experimental results demonstrate that ExGRPO delivers improvements over on-policy RLVR
baselines. We evaluate across five backbone models, spanning the Qwen (Yang et al., 2024; Qwen
et al., 2024) and Llama (Grattafiori et al., 2024) families from 1.5B to 8B parameters, on both
mathematical reasoning benchmarks (AIME24/25, AMC, OlympiadBench, Minerva, MATH500) and
out-of-distribution reasoning benchmarks (ARC-c, GPQA, MMLU-Pro). Averaged overall back-
bone models, ExGRPO achieves +3.5 and +7.6 point gains2 on in-distribution and out-of-distribution
benchmark performance, respectively, compared to the on-policy RLVR. Notably, ExGRPO stabilizes
RLVR training on the weaker Llama-3.1 8B model (Grattafiori et al., 2024) and continual learning on
the stronger LUFFY model (Yan et al., 2025), where on-policy optimization collapses. Detailed anal-
ysis and ablations further confirm that improvements stem from ExGRPO’s experience management
and optimization mechanisms, which amplify the utility of past explorations.
2 RELATEDWORK
Reinforcement Learning with Verifiable Rewards.Reinforcement Learning with Verifiable
Rewards (RLVR; Lambert et al. 2024) frames the language model as a policy that generates reasoning
trajectories for verifiable tasks, with rewards provided by rule-based (Guo et al., 2025; Yu et al.,
2025) or model-based verifiers (Su et al., 2025; Ma et al., 2025b; Chen et al., 2025). Most RLVR
methods adopt on-policy optimization (Schulman et al., 2017; Shao et al., 2024), which ensures
stability but incurs high computational cost. Recent work has explored off-policy techniques (Kallus
& Uehara, 2020; Meng et al., 2023) that incorporate historical or external data. Some approaches
mix expert demonstrations with on-policy updates, e.g., off-policy policy gradients (Yan et al., 2025),
SFT loss (Zhang et al., 2025c; Ma et al., 2025a), or knowledge distillation (Xu et al., 2025); others
develop direct off-policy update rules for improved sample efficiency (Cohen et al., 2025; Roux et al.,
2025; Arnal et al., 2025). However, they overlook the impact of data quality within the experience
replay buffer, a factor that remains underexplored in RLVR and is the central focus of our work.
Experience-based Reinforcement Learning.Experience replay (Lin, 1992) is a classical RL
technique to improve sample efficiency and stabilize training, later extended with prioritized re-
play (Schaul et al., 2016; Sujit et al., 2023) to emphasize informative transitions, enabling break-
throughs in control tasks (Mnih et al., 2013; 2015; Lillicrap et al., 2016; Zha et al., 2019). For LRMs,
recent studies show that replaying successful trajectories accelerates convergence and improves
reasoning capabilities. ReMix (Liang et al., 2025) leverages off-policy experience replay to improve
training efficiency; RePO adopts online replay (Li et al., 2025), collecting early on-policy rollouts and
revisiting them asynchronously; RLEP (Zhang et al., 2025a) reuses trajectories from a well-trained
policy; and RRL (Dou et al., 2025) dynamically revisits promising early states; ARPO (Lu et al.,
2025) extends this idea to GUI agents, combining GRPO with a replay buffer to enhance stability and
sample efficiency. Among them, the last three methods overlook importance weighting to correct
1In the context of RLVR, the experience refers to a state-action-reward trajectory during rollout of the
reasoning chain. We will use the terms experience/trajectory/rollout interchangeably throughout the paper.
2Due to space limitations,numerical resultsof some backbone models are reported in Section E.3.
2Preprint.
distribution mismatch in off-policy updates. In this work, we argue that not all stored experiences
are equally valuable in zero RLVR for LRMs. We systematically analyze properties that determine
the value of past experiences and design methods to better leverage high-value experiences, thereby
improving efficiency and performance.
3 PRELIMINARIES
In this section, we briefly review the RL foundations underlying our method: RLVR and Group
Relative Policy Optimization (GRPO), upon which ExGRPO is built. We then present a prelimi-
nary analysis of experience data, i.e., the model’s past successful rollouts, and examine how their
characteristics influence performance.
3.1 REINFORCEMENTLEARNING WITHVERIFIABLEREWARD
Verifiable Reward Function.The verifiable reward compares the extracted answer from the
model’s output with a predefined golden answer. For example, the model is instructed to output the
final answer in a format such as \boxed{}, from which a verifier extracts the value. Formally, given
a model outputofor questionq, the reward is:
r(q, o) =
1ifocontains the correct final answer toq
0otherwise. (1)
Group Relative Policy Optimization (GRPO).GRPO (Shao et al., 2024) is a strong baseline
within the RLVR paradigm (Guo et al., 2025; Zeng et al., 2025; Liu et al., 2025), achieving effective
scaling without requiring an additional value model. It estimates the advantage of each trajectory
by normalizing rewards within a group of N sampled solutions. Given a group of K trajectories
Gq ={o i}K
i=1 using the current reference policy πθold (i.e, rollout policy), GRPO estimates the
advantage of each trajectory oi by normalizing its reward against the empirical statistics of the group:
bAi = r(q, oi)−µ Gq
σGq
(2)
where µGq = 1
K
PK
j=1 r(q, oj) and σGq are the mean and standard deviation of rewards in the group
Gq. The on-policy objective maximizes a clipped surrogate function:
JGRPO(θ) =E q∼D,{oi}∼πθold (·|q)
"
1
K
KX
i=1
CLIP(wi(θ), bAi)
#
(3)
where D is the training query set and the loss term is CLIP(w, A) =
1
|o|
P|o|
t=1 min (wtA,clip(w t,1−ε,1 +ε)A) . Since the trajectory o is generated by the rollout policy
model before update, i.e., πθold , the per-token importance weight wi,t(θ) is the probability ratio be-
tween the current policy πθ and reference policy πθold : wi,t(θ) =π θ(oi,t|q, oi,<t)/πθold (oi,t|q, oi,<t).
Following Dr.GRPO (Liu et al., 2025), we remove the length normalization and standard deviation
normalization of GRPO loss (Eqs. 2 and 3).
3.2 PRELIMINARYSTUDY ONEXPERIENCEDATA
To motivate our experience management design, identifying which questions and trajectories are
most valuable for experience-based learning, we begin by analyzing rollouts from on-policy RLVR.
Our study is guided by two questions: (1) Are all questions equally useful for training, or do
certain difficulty levels provide stronger learning signals? and (2) Does low entropy correlate with
higher-quality trajectories that the model should prioritize?
Setup.We conduct experiments using Qwen2.5-Math 7B (Yang et al., 2024) backbone model,
trained with vanilla on-policy RLVR following the Dr.GRPO setup on the OpenR1-Math dataset (Face,
2025). Implementation details and hyperparameters are provided in Section 5.1. The experimental
setting is as follows: we train three models, each using training batches restricted to questions of a
3