# 2508.04700v2.pdf

SEAgent: Self-Evolving Computer Use Agent with
Autonomous Learning from Experience
Zeyi Sun1,2 Ziyu Liu1,2 Yuhang Zang2 Yuhang Cao2
Xiaoyi Dong2,3 Tong Wu3 Dahua Lin2,3 Jiaqi Wang2
1Shanghai Jiao Tong University 2Shanghai Artificial Intelligence Laboratory
3The Chinese University of Hong Kong
szy2023@sjtu.edu.cn, wangjiaqi@pjlab.org.cn
https://github.com/SunzeY/SEAgent
Abstract
Repurposing large vision-language models (LVLMs) as computer use agents
(CUAs) has led to substantial breakthroughs, primarily driven by human-labeled
data. However, these models often struggle with novel and specialized software,
particularly in scenarios lacking human annotations. To address this challenge,
we propose SEAgent, an agentic self-evolving framework enabling CUAs to au-
tonomously evolve through interactions with unfamiliar software. Specifically,
SEAgent empowers computer-use agents to autonomously master novel software
environments via experiential learning, where agents explore new software, learn
through iterative trial-and-error, and progressively tackle auto-generated tasks or-
ganized from simple to complex. To achieve this goal, we design a World State
Model for step-wise trajectory assessment, along with a Curriculum Generator
that generates increasingly diverse and challenging tasks. The agent’s policy is
updated through experiential learning, comprised of adversarial imitation of failure
actions and Group Relative Policy Optimization (GRPO) on successful ones. Fur-
thermore, we introduce a specialist-to-generalist training strategy that integrates
individual experiential insights from specialist agents, facilitating the development
of a stronger generalist CUA capable of continuous autonomous evolution. This
unified agent ultimately achieves performance surpassing ensembles of individual
specialist agents on their specialized software. We validate the effectiveness of
SEAgent across five novel software environments within OS-World. Our approach
achieves a significant improvement of 23.2% in success rate, from 11.3% to 34.5%,
over a competitive open-source CUA, i.e., UI-TARS.
1 Introduction
“A new generation of agents will acquire superhuman capabilities by learning predominantly from
experience. ” [55]
— David Silver, Richard S. Sutton
With the rapid development of large vision-language models (LVLMs) [ 61, 16, 7, 64, 42, 5, 60],
computer use agents (CUAs) [ 3, 43, 48, 29, 67] have not only emerged but also demonstrated
increasing practical utility. By leveraging the powerful perception and reasoning capabilities of
LVLMs, these agents can interpret screenshots as visual inputs and operate computers via keyboard
and mouse actions. Despite their promising capabilities, current CUAs [ 47, 46, 12, 19, 6, 34]
primarily depend on costly human-curated datasets [12, 9, 67, 24, 28], which are typically derived
preprint.
arXiv:2508.04700v2  [cs.AI]  12 Aug 2025Software Specialized RLDirect multi-software RL Software Specialists
Software Generalist
Create light effect on top left 
corner and make focus blur 
centered on bird’s head.
I’m not familiar with this 
software for image editing. 
Task
Gen
Action 
Eval
Initial
Action Model
 Specialist
Action Model
Step1. Click the filter panel 
to create light effect...
Create light effect on top left 
corner and make focus blur 
centered on bird’s head.
Trying to learn all 
the software at once 
is too hard for me...
State 
Caption
Figure 1: SEAgent enables computer use agents self-evolving in novel environments by au-
tonomously exploring and learning from their own experiences without human intervention. The
specialist-to-generalist training strategy further enhances the development of a strong generalist agent.
from demonstrations [34, 78, 18, 51, 75] or video tutorials in the wild [70]. However, new software
continuously emerges and existing software may regularly be updated, often in the absence of
annotated human data. It is both necessary and timely to enter an era that emphasizes learning from
experience [55] in CUA domain. In this paper, we aim to enable CUAs to autonomously explore
unfamiliar software environments and evolve into experts without relying on human supervision.
To address this challenge, we propose SEAgent, an agentic self-evolving framework in which
Computer Use Agents (CUAs) are exposed to previously unfamiliar software environments and
engage in autonomous exploration and experiential learning, as illustrated in Fig. 1. Enabling such self-
evolution requires addressing two key challenges: (1) generating executable tasks within unfamiliar
software environments, and (2) accurately assessing task success and precisely identifying the step
at which failure occurs. To this end, we introduce a World State Model for environmental state
captioning and step-wise trajectory assessment, together with a Curriculum Generator powered by
a continuously updated software guidebook memory to generate increasingly diverse and challenging
tasks, thereby establishing a curriculum learning paradigm. The agent’s policy is optimized through
experiential learning from both failures and successes, combining adversarial imitation of failure
actions and Group Relative Policy Optimization (GRPO) on successful ones.
Given the critical role of reward accuracy, we conduct extensive evaluations and observe that existing
reward models of computer use tasks fall short in terms of judgment precision and reward density.
Leveraging the enhanced long-context processing capabilities of advanced LVLMs, we input the
agent’s full trajectory of states into the reward model and fine-tune a reward model, World State
Model, using Qwen2.5-VL [7], substantially narrowing the gap with commercial models such as
GPT-4o [42] with +7.5% improvement in precision compared to baseline model in evaluating CUAs’
trajectories on AgentRewardBench [35], enable World State Model to provide high quality step level
reward signals in self-evolving agentic system.
Moreover, SEAgent enables agents to evolve into either single-software specialists or multi-software
generalists. To overcome the limitation that directly training a generalist underperforms compared to
specialists, inspired by [77], we introduce a novel specialist-to-generalist training strategy, which
even surpasses the performance of individual specialists on their respective software applications.
We perform extensive experiments of SEAgent built on UI-TARS [48] and evaluated on five profes-
sional software applications from OSWorld [68]. SEAgent with the specialist-to-generalist strategy
significantly improves the UI-TARS [48] from 11.3% to 34.5%. Furthermore, SEAgent with the
2specialist-to-generalist strategy (34.5%) outperforms both specialist RL (32.2%) and generalist
RL (30.6%) by a substantial margin, demonstrating the effectiveness of the specialist-to-generalist
paradigm. In general, SEAgent offers a promising approach for developing more powerful and
versatile computer-use agents without human involvement.
2 Related Work
Agent for Computer Use. With the recent advances in LLMs and LVLMs [61, 16, 30, 7, 64], which
enable human-level perception and reasoning capabilities, the development of agents for computer use
has garnered significant attention [22, 20, 11, 41, 29]. These agents either rely solely on structured
text inputs [47, 40, 46, 26, 36] or, in a more human-like manner, use multi-modal inputs such as
screenshots combined with textual conditions [21, 29, 67, 43]. Although they have been extensively
studied and show strong performance on in-domain benchmarks [34, 79, 31, 27, 11], computer use
agents still lag significantly behind human-level performance in simulated environments [ 68, 50,
25, 80]. This gap highlights the challenges posed by the multi-dimensional demands on LVLMs,
including grounding, decision-making, and reasoning. Some approaches address this by decomposing
tasks into specialized expert models [15, 62] and employing agent collaboration [1, 2, 32, 74] through
prompt engineering [71, 19, 76, 63, 66]. However, improvements from these training-free methods
remain limited without fine-tuning. In this work, we explore the next phase of computer use agents,
where a pretrained agent is fine-tuned to learn from its own experience, enabling self-evolution on
novel, specialized software without human annotations.
Reinforcement Learning for LLMs/LVLMs. Previous scalable training efforts for LLMs and
LVLMs [61, 16, 30, 7, 64, 69, 59, 58, 13] have primarily relied on supervised fine-tuning (SFT) [30,
65]. Analogous to imitation learning or behavior cloning in reinforcement learning (RL), SFT
trains models to produce desired outputs based on labeled data, making it heavily dependent on
high-quality human-curated procedures. Recently, DeepSeek-R1 [17] demonstrated strong reasoning
capabilities via Group Relative Policy Optimization (GRPO) [53] using verifiable rewards. Earlier
works [44, 82, 49] have also employed RL for single-turn optimization from human feedback.
However, in agentic scenarios such as computer usage—where feedback is sparse with reward signals
often results from multi-step interactions—it becomes crucial to introduce stable, step-level reward
signals. Prior RL approaches for agents [ 6, 47, 81, 73, 8] have fine-tuned their own critic models
for advantage estimation [52], either using output reward models (ORMs) trained on labeled data
or adopting Direct Preference Optimization (DPO) [49] based on interaction data [46, 48]. In this
work, we investigate various strategies for constructing high-performing reward models for CUAs
and find that full-process-based analysis yields more accurate evaluations with fine-grained reward
signals compared to training dedicated critic models for advantage estimation as done in [6, 47] or
with filtered behavior cloning [45, 10].
3 Methods
Problem Formulation. The objective of SEAgent is to establish a training pipeline enabling the
Computer Use Agent (CUA) to autonomously explore its environment (Sec. 3.1) and progressively
self-evolve on novel software applications via reinforcement learning from experience (Sec. 3.2).
Specifically, the SEAgent pipeline comprises three primary components: an Actor Model π perform-
ing exploratory actions to accomplish these tasks, and a World State ModelMstate describing the
current environment state and evaluating the success or failure of executed actions, and a Curriculum
Generator Mtask that continuously proposes more diverse and challenging exploration tasks:
(1) Actor Model π: The policy π(a|st, I) defines the probability of taking action a at time step t,
conditioned on the current state st and the overall task instruction I.
(2) World State Model Mstate: This component is a fine-tuned Large Vision-Language Model
(LVLM) responsible for providing detailed descriptions of environment states. It also evaluates
each step of the trajectory executed by the Actor Model π, producing trajectory judgement J
which indicates whether the task has been successfully completed. Joint training with state change
captioning C of the software GUI has been shown to enhance judgment accuracy, as shown in Table 1.
3