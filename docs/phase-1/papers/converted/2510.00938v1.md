# 2510.00938v1.pdf

Large Reasoning Models Learn Better Alignment
from Flawed Thinking
ShengYun Peng1,2,∗,Eric Smith1,†,Ivan Evtimov1,†,Song Jiang1,†,Pin-Yu Chen3,Hongyuan Zhan1,
Haozhu Wang1,Duen Horng Chau2,Mahesh Pasupuleti1,Jianfeng Chi1
1Meta Superintelligence Labs,2Georgia Tech,3IBM Research
∗Work done at Meta,†Equal contribution
Large reasoning models (LRMs) “think” by generating structured chain-of-thought (CoT) before
producing a final answer, yet they still lack the ability to reason critically about safety alignment and
are easily biased when a flawed premise is injected into their thought process. We proposeRECAP
(Robust Safety Alignment via Counter-Aligned Prefilling), a principled reinforcement learning (RL)
method for post-training that explicitly teaches models to override flawed reasoning trajectories and
reroute to safe and helpful responses. RECAP trains on a mixture of synthetically generated counter-
aligned CoT prefills and standard prompts, requires no additional training cost or modifications
beyond vanilla reinforcement learning from human feedback (RLHF), and substantially improves
safety and jailbreak robustness, reduces overrefusal, and preserves core reasoning capability — all
while maintaining inference token budget. Extensive analysis shows that RECAP-trained models
engage in self-reflection more frequently and remain robust under adaptive attacks, preserving safety
even after repeated attempts to override their reasoning.
/exclamati⌢n-triangleThis paper includes potentially offensive red-teaming data and model-generated content.
Date:October 2, 2025
Correspondence:ShengYun Pengspeng65@gatech.edu, Jianfeng Chijianfengchi@meta.com
Code:We will release the code shortly.
1 Introduction
FrontierLRMs, such as DeepSeek-R1 (Guo et al., 2025), OpenAI-o3 (OpenAI), and Qwen3 (Team, 2025), have
achieved remarkable performance in math (Shao et al., 2024) and coding (Jiang et al., 2024a) tasks, where
they “think” by first generating structuredCoT reasoning before producing a final answer (Zhang et al., 2025a;
Xu et al., 2025). Trained via online RL algorithms, such as group relative policy optimization (GRPO) (Shao
et al., 2024),LRM exhibits emergent behaviors such as “aha moments,” where the model revisits earlier steps
or backtracks to refine its reasoning (Guo et al., 2025; Zhou et al., 2025a; Xie et al., 2025; Yang et al., 2025).
Yet recent work shows that prefilling the CoT with a simple phrase like “I know that” can bypass alignment
constraints and elicit unsafe completions (Rager et al., 2025), raising concerns about whetherLRMs truly
understand how to reason safely (Huang et al., 2025a; Chen et al., 2025). SinceCoT prefilling is widely
supported in both open-source models (Jeung et al., 2025) and commercial APIs (Anthropic, 2025), this
brittleness highlights a deeper issue:frontierLRMs still lack the ability to reason critically about safety alignment,
as they are easily biased when a flawed premise is injected into their thought process.
We investigate its root cause of the issue (Sec. 2) and discover that once anLRM begins from a flawedCoT,
it tends to forget its safety alignment and follows unsafe reasoning into harmful completions. In contrast,
prefilling the sameLRM with reasoning traces from a safer model consistently improves its performance.
These findings reveal a generalization gap: during onlineRL training, models are commonly rewarded only
for correct final responses, while at inference they have to navigate through noisy reasoning trajectories that
may begin with flawed or misleading steps. This raises a central question:How can we trainLRMs to achieve
robust safety alignment by recovering from misleading reasoning, rather than hoping self-correction will emerge
implicitly?To address this gap, we introduce anRL training recipe that improves safety, reduces overrefusal,
and preserves core reasoning capability, making the following three main contributions (Fig. 1):
1
arXiv:2510.00938v1  [cs.LG]  1 Oct 2025Figure 1RECAP trains LRMs on a mixture of counter-aligned prefilled and standard prompts. Harmful prompts
are prefilled with unsafe reasoning, and benign prompts with refusal reasoning, forcing the model to override flawed
trajectories to achieve high rewards. This simple recipe teaches models to internalize safety values and remain robust
under both clean and adversarial reasoning traces, with no extra cost beyond standard RLHF.
1. We propose RECAP (Robust Safety Alignment via Counter-Aligned Prefilling), a principledRL method for
post-training that addresses the core brittleness ofLRM safety alignment by explicitly training models to recover
from flawed reasoning traces(Sec. 3). We construct counter-aligned flawed reasoning by prefilling theCoT
of LRM, inducing it to “think unsafe” for harmful queries and “think overly conservative” for benign ones.
Naively following these prefills would cause the model to provide unsafe instructions or overrefuse benign
queries. To achieve high rewards, the model must instead override these flawed trajectories and recover
appropriate reasoning. RECAP trains on a mixture of counter-aligned reasoning prefills and standard
prompts, ensuring that models internalize core safety values and can robustly initiate reasoning from
both correct and flawed traces. RECAP is easy to adopt, requiring no additional training cost and no
modification to the RLHF objective.
2. RECAP simultaneously strengthens safety, helpfulness, and math reasoning capability, with theoretical analysis
supporting its robustness(Sec. 4). In a realistic post-trainingRL setting with multiple reward signals from
different capabilities, RECAP delivers substantial gains over vanilla decouple clip and dynamic sampling
policy optimization (DAPO) (Yu et al., 2025) on DeepSeek distilled Llama-8B and Qwen-14B. Specifically,
it achieves on average +12.3% on direct harmful benchmarks, +21.0% on jailbreaking benchmarks, and
+7.8% on the helpfulness score for overrefusal. Additionally, it improves math reasoning by +0.9%, an
emerging benefit that arises purely from prefilling on safety alignment data. These empirical gains are
consistent with our theoretical analysis, which shows that RECAP achieves higher expected reward than
vanilla DAPO under both inference with and without prefilling. Finally, compared to vanillaRLHF,
RECAP maintains a similar inference-time token budget while generating more structured and logically
coherent reasoning traces.
3. We demonstrate that RECAP yields persistent robustness even under adaptive attacks and fundamentally
improves LRM reasoning dynamics by increasing the frequency of self-reflection(Sec. 5). To stress-test the
reasoning safety behavior, we introduce two adaptive attacks: fullCoT hijacking and iterative prefill
reset (IPR), explicitly designed to bypass RECAP’s self-reflection mechanism. We find that RECAP
remains robust against both attacks, preserving safety even after repeated attempts to override its reasoning.
A deeper behavioral analysis shows that RECAP-trainedLRMs engage in self-reflection far more often
than vanillaRLHF, frequently revising unsafe or mistaken reasoning mid-trajectory. Finally, our ablations
reveal that counter-aligned prefills are essential to induce this reflective behavior, and that the ratio and
length of prefills control the trade-off between safety and overrefusal.
2 Following Without Thinking: The Brittleness of Reasoning in Current LRMs
RLHF-tuned LRMs exhibit emergent behaviors such as “aha moments,” where the model allocates more
reasoning budget to a problem by revisiting its initial thought, reflecting on flawed reasoning, and sometimes
even backtracking to reconsider earlier steps (Guo et al., 2025). Yet we find thatLRMs remain highly sensitive
2to their initial reasoning direction: when seeded with a misleadingCoT, they frequently continue along the
flawed trajectory and produce incorrect outputs; conversely, when provided with a higher-quality reasoning
trace than they could generate on their own, they tend to follow it to a better answer. To understand
this phenomenon, Sec. 2.1 constructs a suite of controlled case studies, and Sec. 2.2 demonstrates that how
prefilling with different CoT traces can dramatically alter the model’s final response.
2.1 Preliminaries
Notation.Let πθ denote anLRM parameterized by weightsθ. Given an input promptx, we represent the
model’s output asy = (ycot, yresp), whereycot is the intermediateCoT reasoning andyresp is the final response.
To study reasoning brittleness, we construct prefilling samples by injecting a partial, pre-generated reasoning
trace ypre
cot into theCoT, where ypre
cot is syntactically fluent but semantically flawed or counter-aligned. At
inference time, the model is given(x, ypre
cot)as input and asked to generate the continuationygen
cot followed by
yresp. The full output is thus:y = (ypre
cot ∥y gen
cot , yresp),, where∥ denotes sequence concatenation. This setup
allows us to measure how different types of reasoning prefills affect the final model behavior.
Evaluation and metrics.We evaluate the safety alignment following the protocol introduced by Peng et al.
(2025). Specifically, models are tested on theStrongREJECTbenchmark, which contains 313 harmful
prompts (Souly et al., 2024). The model’s final responseyresp is judged by GPT-4o to determine whether
it is safe. Our metric is thesafety score, defined as the percentage of completions judged safe. We report
results on DeepSeek-distilledLRMs (abbreviated as DS) (Guo et al., 2025) and on Qwen3-4B-Thinking-2507
reasoning models (Team, 2025).
2.2 Prefilled Reasoning Traces Steer LRM Behavior Dramatically
LRMs are highly sensitive to their initial reasoning direction.We hypothesize that the safety of the final response
yresp depends strongly on the quality of the intermediate reasoningycot. To test this, we ask:What happens if
a model is forced to continue from another model’s reasoning trace?We first evaluate the original DS models
and observe, as shown in Table 1 (Original), that larger models generally achieve higher safety scores. Next,
we extract the firstℓpre = 200words of ycot from the least safe model, DSQwen-1.5B, and use it asypre
cot.
When the other four DS models are required to continue from this unsafe prefix, their average safety score
drops by 36.4% (Table 1,ypre
cot from DSQwen-1.5B). Conversely, when we prefill them with the firstℓpre = 200
words ofycot from the safest model, DSQwen-32B, their safety score increases by 91.7% (Table 1,ypre
cot from
DSQwen-32B).
The brittleness of reasoning extends across model families.This sensitivity to initial reasoning direction is not
limited to models within the same family. We test Qwen3-4B-Thinking-2507, a reasoning model from the
Qwen 3.0 family trained with a different dataset and recipe than the DSQwen models (which are finetuned
from Qwen 2.5). Although Qwen3-4B achieves relatively high safety scores under standard evaluation (no
prefill), its performance drops by 19.5% when initialized with theypre
cot from DSQwen-1.5B and by 11.4% when
initialized with theypre
cot from DSQwen-32B.
Brittleness generalizes beyond safety.In Appendix B, we extend this analysis to overrefusal and mathematical
reasoning tasks. We observe the same pattern: when prefilled with flawedCoT, models are more likely to
produce overcautious refusals or incorrect math solutions. This indicates that the brittleness revealed in safety
alignment reflects a broader vulnerability in current LRM reasoning.
3 RECAP: Robust Safety Alignment via Counter-Aligned Prefilling
Building on our finding in Sec. 2 thatLRMs are easily biased by flawed premises injected into theirCoT,
we now explore the counter-aligned setting: what if such flawed reasoning were deliberately introduced
during rollouts in onlineRL, and the model were trained to recover from it? In other words, can systematic
exposure to counter-aligned reasoning trajectories during training strengthen safety alignment? In Sec. 3.1, we
demonstrate how to construct counter-aligned prefills and integrate them into theRLHF training objective,
using DAPO as an example. In Sec. 3.2, we provide a theoretical analysis showing why training with RECAP
yields more robust safety alignment at inference time.
3