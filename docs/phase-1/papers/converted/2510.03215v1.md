# 2510.03215v1.pdf

Preprint version
CACHE-TO-CACHE: DIRECTSEMANTICCOMMUNICA-
TIONBETWEENLARGELANGUAGEMODELS
Tianyu Fu∗1,2 , Zihan Min∗1, Hanling Zhang∗3, Jichao Yan1,
Guohao Dai5,2,Wanli Ouyang 3,4,Yu Wang†1
1Tsinghua University 2Infinigence AI 3The Chinese University of Hong Kong
4Shanghai AI Laboratory 5Shanghai Jiao Tong University
ABSTRACT
Multi-LLM systems harness the complementary strengths of diverse Large Lan-
guage Models, achieving performance and efficiency gains unattainable by a sin-
gle model. In existing designs, LLMs communicate through text, forcing internal
representations to be transformed into output token sequences. This process both
loses rich semantic information and incurs token-by-token generation latency.
Motivated by these limitations, we ask:Can LLMs communicate beyond text?
Oracle experiments show that enriching the KV-Cache semantics can improve re-
sponse quality without increasing cache size, supporting KV-Cache as an effec-
tive medium for inter-model communication. Thus, we propose Cache-to-Cache
(C2C), a new paradigm for direct semantic communication between LLMs. C2C
uses a neural network to project and fuse the source model’s KV-cache with that of
the target model to enable direct semantic transfer. A learnable gating mechanism
selects the target layers that benefit from cache communication. Compared with
text communication, C2C utilizes the deep, specialized semantics from both mod-
els, while avoiding explicit intermediate text generation. Experiments show that
C2C achieves 8.5-10.5% higher average accuracy than individual models. It fur-
ther outperforms the text communication paradigm by approximately 3.0-5.0%,
while delivering an average 2.0× speedup in latency. Our code is available at
https://github.com/thu-nics/C2C.
1 INTRODUCTION
communication text:response text:
context Receiver LLM: R-Cachet0
Sharer LLM: S-Cachet0
St1
tn-1
Stn
… context
r0
contextS-CacheR-Cache
contextS&R-CacheR-Cacher0
Cache FuserRr0
r1
… r0
r1
…Rresponse text: 
s-prompt queryt0 … tn query(a) (b)
!
Figure 1: (a) Previous Text-to-Text (T2T) communication passes information through explicit text
generation. (b) Our Cache-to-Cache (C2C) communication directly projects and merges KV-Cache
with rich semantics from different LLMs.
With the rapid progress of Large Language Models (LLMs) (Guo et al., 2025; Yang et al., 2025a;
OpenAI, 2025), they are now applied across increasingly diverse domains and tasks. To meet ver-
satile demands, LLMs are trained with distinct focuses, such as coding (Hui et al., 2024), mathe-
matics (Yang et al., 2024a), visual understanding (Bai et al., 2025), edge computing (Zhang et al.,
2024b), and so on. Meanwhile, general-purpose LLMs can also simulate specialized capabilities
through prompt engineering, enabling flexible role adaptation across downstream applications.
Leveraging the diversity of LLMs, many multi-LLM systems are proposed to further enhance overall
performance and efficiency (Guo et al., 2024; Tran et al., 2025). Incollaborative multi-LLM
∗Equal contribution.
†Corresponding author: Yu Wang (yu-wang@tsinghua.edu.cn).
1
arXiv:2510.03215v1  [cs.CL]  3 Oct 2025Preprint version
Coder LLM
“<title>Introduction</title><p>I’m Tom...</p></section>”
Coder: “Write contentinside the <section> wrapper.”
Writer LLM
<p>: don’t know this word…wrapper: some structureWriter LLMthe start of content…after <p>
(b) Cache-to-Cache (C2C)
(a) Text-to-Text (T2T)
 L do not know what <p> means
J know <p> from Coder cache
“Sorry, I don’t know the specific location to insert. Writing plain text:I’m Tom…”
Neural KV-Cache projection<p>→ place→ …
User: “Help Chatbot code.”
&
User“<!doctype html><section><title></title><p></p></section>…Insert my self-introduction in the right place”
Text
 KV-Cache: embedding
<!doctype html>: start of doc<section>: start of section<title>: start of title</title>: end of title<p>: start of contentplace: after <p>…
Figure 2: Conceptual comparison of T2T and C2C communication in a Coder-Writer collaboration
example. In T2T, the Coder’s ambiguous text instruction fails to convey the structural semantics
of<p>as a paragraph separator, causing the Writer to misplace the content. C2C directly projects
the Coder’s KV-Cache into the Writer, transferring both the semantic understanding and precise
insertion location without intermediate text generation.
systems(Li et al., 2023; Wu et al., 2023), LLMs are assigned distinct roles and proactively exchange
text messages. Mirroring human collaboration, these systems accumulate partial understandings
or sub-solutions from different agents via verbal communication. They harnessing the collective
capabilities of multiple LLMs to solve complex problems that a single model cannot. By contrast,
routing-basedmulti-LLM inference systems rely on passive context inheritance rather than active
message exchange. These systems coordinate models of varying parameter sizes or reasoning depths
for more dynamic and efficient responses (Li et al., 2024; Fu et al., 2025; Ong et al., 2024; OpenAI,
2025). Downstream models inherit the context from preceding models in multi-round conversations,
then generate follow-up responses to the new questions based on their own understanding of the
conversation history.
However, current text-to-text (T2T) interfaces restrict information exchange among LLMs, particu-
larly when conveying rich or diverse semantic interpretations of a shared context. As illustrated in
Figure 2, these limitations arise from several inherent constraints of T2T communication. First, as
a low-bandwidth medium, text introduces an information bottleneck. The high-dimensional inter-
nal representations must be repeatedly compressed into linear strings and then decompressed by the
receiver LLM. When models differ in knowledge or assigned roles, some signals may be irrecover-
able (e.g., interpreting<p>as a section marker). Second, natural language is inherently ambiguous,
with idioms, underspecified references, and vague expressions. Although recent agent protocols
aim to standardize text messages (Anthropic, 2024; Surapaneni et al., 2025), rigid templates remain
insufficient for flexible, open-domain collaboration. Third, T2T communication incurs noticeable
latency. Every exchange requires exhaustive, token-by-token decoding of contextual explanations in
sequence. These limitations motivate a key question:
Can LLMs communicate beyond text?
In this work, we explore using KV-Cache as the medium for LLM communication. KV-Cache is a
naturally richer representation than text. It also enables fully parallel communication through direct
projection, avoiding the slow sequential decoding in text exchanges. Our oracle experiments show
that (1) Enriching KV-Cache under the same context length can lead to an increase in accuracy. (2)
KV-Cache is convertible between LLMs. (3) Different LLMs encode distinct semantic understand-
ings and contextual knowledge of the same input, reflecting their complementary strengths.
Encouraged by these oracles, we propose Cache-to-Cache (C2C), a new paradigm for richer and
faster multi-LLM communication. As shown in Figure 1(b), C2C projects the KV-Cache from a
source model into the space of a target model and merges them through a neural Cache Fuser.
Experiments show that C2C achieves 8.5-10.5% higher average accuracy than individual models.
It further outperforms the T2T paradigm by approximately 3.0-5.0%, while delivering an average
2.0×speedup in latency.
2 RELATEDWORK
2.1 KV-CACHESHARING ANDREUSE
Based on the similarity of KV-Cache between layers, intra-model cache sharing methods (Yang
et al., 2024b; Wu & Tu, 2024; Sun et al., 2024; Brandon et al., 2024; Wu et al., 2025) are proposed
2Preprint version
to reuse shallow layers’ KV-Cache for deeper layers to accelerate single LLM inference. Another
research focus is to reuse a portion of KV-Cache (e.g., common prefix, reference documents) for the
same model in multiple user queries (Bang, 2023; Ye et al., 2024; Yao et al., 2024; Qin et al., 2024;
Yang et al., 2025b). DroidSeek Liu et al. (2024a) extends cache reuse to models fine-tuned from
the same base model. Unlike existing works that focus on computational efficiency through cache
reuse, our approach leverages the KV-Cache as a medium for semantic transfer between LLMs.
Furthermore, unlike existing cache sharing methods that are restricted to only a single model or
models with identical structure and size, our method supports sharing across different model families
and varying model sizes.
2.2 MULTI-LLM SYSTEMS
Collaborative multi-LLM systems. Collaborative systems treat multiple LLMs as peers that ex-
change information to improve collective performance. Chain-of-Agents (Zhang et al., 2024c) and
MetaGPT (Hong et al., 2023) create sequential message flows where agents directly communicate
using natural language interfaces. Mixture-of-Agents Wang et al. (2024) and DyLAN (Liu et al.,
2024b) introduce layered communication architectures. Target LLMs aggregate messages from mul-
tiple models using voting or summarization mechanisms. Multi-agent debate methods (Estornell &
Liu, 2024; Liang et al., 2024; Du et al., 2023) involve iterative communication rounds, letting LLM
agents discuss and refine responses. Recent works such as MCP Anthropic (2024) and A2A Sura-
paneni et al. (2025) establish formal text protocols beyond natural language, standardizing agent
interaction and tool usage in collaborative multi-LLM systems. These approaches rely on text-level
interfaces, where communication requires one model to generate text token-by-token and another to
ingest it as input. Our work explores a deeper and more efficient collaboration by directly sharing
internal KV-Cache representations.
Routing-based multi-LLM inference systems. To accelerate LLM inference, several systems
leverage multiple models with different capabilities and costs. Dynamic model selection meth-
ods (OpenAI, 2025; Ong et al., 2024; Feng et al., 2024) route queries to different models with
varying sizes and configurations to balance efficiency and performance. Token-level routing meth-
ods (Zhang et al., 2024a; Shen et al., 2024; Zheng et al., 2025; Fu et al., 2025) enable finer-grained
selection, utilizing smaller models for simple token generation within the reasoning process of com-
plex tasks. While these systems achieve efficiency through strategic model switching, they either
completely drop context from other models, or simply rely on their own understandings of the con-
text. Without understanding sharing, smaller models cannot benefit from the richer representations
already computed by larger models.
3 METHOD
3.1 PRELIMINARIES
LLM inference. Autoregressive LLM inference involves two stages:prefillanddecode. Prefill
encodes the full input to produce the first output token; decode then generates subsequent tokens
iteratively using the last token and the cached key–value (KV) states. Formally, letX [0:n] =
[x0, . . . , xn−1]be the input token sequence. After prefill, LLM produces a per-token KV-Cache
C(X[0:n]) = [c0, . . . , cn−1]∈R n×d. For notation brevity,ddenotes the KV dimensionality that is
flattened from all layers into a single vector per token. The range subscripts are omitted when clear.
During decoding, with current tokeny i and caches from the input and the generated prefix, the next
token is predicted as
yi+1 =P
 
yi | C(X)⊕ C(Y[0:i])

,(1)
where⊕denotes sequence-wise concatenation. The cache updates asC(Y [0:i+1]) =C(Y [0:i])⊕C(yi).
LLM communication. In LLM communication scenarios, we define the LLM that provides con-
textual understanding or knowledge asSharer, and the one that utilizes it asReceiver.
3.2 ORACLES FORCACHE-TO-CACHECOMMUNICATION
We aim to explore whether LLMs can have direct semantic communication through KV-Cache.
Specifically, we design two oracle experiments to answer the following questions: (1)Benefit: can
3