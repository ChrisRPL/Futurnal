# 2509.25137v1.pdf

The Era of Real-World Human Interaction:
RL from User Conversations
Chuanyang Jin1,2,Jing Xu1,‚Ä†,Bo Liu1,‚Ä†,Leitian Tao1,Olga Golovneva1,Tianmin Shu2,Wenting Zhao1,
Xian Li1,Jason Weston1
1FAIR at Meta,2Johns Hopkins University
‚Ä†Joint second author
We posit that to achieve continual model improvement and multifaceted alignment, future models must
learn from natural human interaction. Current conversational models are aligned using pre-annotated,
expert-generated human feedback. In this work, we introduce Reinforcement Learning from Human
Interaction (RLHI), a paradigm that learns directly from in-the-wild user conversations. We develop
two complementary methods: (1)RLHI with User-Guided Rewrites, which revises unsatisfactory model
outputs based on users‚Äô natural-language follow-up responses, (2)RLHI with User-Based Rewards,
which learns via a reward model conditioned on knowledge of the user‚Äôs long-term interaction history
(termed persona). Together, these methods link long-term user personas to turn-level preferences
via persona-conditioned preference optimization. Trained on conversations derived from WildChat,
both RLHI variants outperform strong baselines in personalization and instruction-following, and
similar feedback enhances performance on reasoning benchmarks. These results suggest organic human
interaction offers scalable, effective supervision for personalized alignment.
Date:September 30, 2025
Correspondence:Chuanyang Jin atcjin33@jhu.edu, Jason Weston atjase@meta.com
Prompt: How can I develop a habit of drawing daily?--------------------------------------------------------Response 1 (chosen): Developing a daily habit of drawing can be challenging but‚Ä¶Response 2 (rejected): As an AI language model, I cannot develop habits‚Ä¶ Meta data: scores, ranking, reasons, etc.
I‚Äôm sketching a city at night. what details should I highlight?
Can you give more details?
Show glowing windows, neon signs, rivers of car headlights‚Ä¶
‚ãÆ
Interaction with       User 1‚Ä¶
Preference labels from annotators
Learning from Annotated Human FeedbackLearning from Organic Human InteractionInteraction with         User 2Real users‚Äô diverse hidden preferences from history‚Ä¶‚Ä¶ ‚Ä¶
Authentic, dynamic signalsRequests: arising from real-life needsFeedback: expressed naturally within multi-turn conversations
Static, isolated queries
User 1: prefers detailed, descriptive, casual, and artistic answers‚Ä¶ User 2: prefers concise, professional‚Ä¶
Figure 1 From annotated feedback to the era of real-world human interaction. Left:Traditional alignment relies on
expert-curated annotations of ranked responses or labels, providing static, out-of-distribution supervision.Right:
In-the-wild conversations reveal users‚Äô long-term histories, dynamic demands, and diverse signals, enabling personalized,
contextual, and continual learning.
1 Introduction
Today, language model post-training primarily depends on static corpora of expert-annotated data: verifiable
questions, fixed demonstrations, and rankings or ratings collected outside of natural conversational contexts.
While these datasets are effective for instilling general capabilities, they reflect the opinions and heuristics of
annotators in unnatural scenarios rather than theauthentic, diverse long-term goals and preferences of real
users; they capture static, context-free judgments instead ofevolving, situational demands; and they scale
with labeling budgets rather than with actual usage and diversity of organic users, as is illustrated on the left
side of Figure 1.
1
arXiv:2509.25137v1  [cs.AI]  29 Sep 2025In contrast, humans learn and improve through continual experience by interacting with their environment
and other actors, receiving feedback, and adjusting behavior over time (Tomasello et al., 2005). Likewise, a
rich and organic source of supervision for language models already exists in the wild:human interaction‚Äîthe
ongoing, natural exchanges between models and real users. As is shown on the right side of Figure 1, such
organic interactions reveal hidden user preferences from long-term histories and dynamic, context-dependent
demands, as people reveal their priorities and concerns not through annotation formats, but by discussing
what matters to them, revising or re-attempting questions, explicitly or implicitly approving or critiquing
model outputs, following up, or switching goals mid-dialogue. Because they arise directly from model outputs
in authentic usage contexts, such interactions provide a rich signal for learning personalized and adaptable
behavior, paving the way toward personal superintelligence. While this source of supervision has historically
been hard to extract, resulting in resorting to collecting static training data instead, the power of modern
language models now gives us a greater ability to extract these signals.
To achieve this vision, we introduce RLHI, a paradigm that learns directly from in-the-wild conversations
through two complementary methods: (1)RLHI with User-Guided Rewrites(¬ß2.3), which revises unsatisfactory
model outputs based on users‚Äô natural-language follow-ups, and pairs the rewrites with the originals for
preference learning; and (2)RLHI with User-Based Rewards(¬ß2.4), which ranks candidate responses using
a reward model conditioned on user personas derived from long-term histories to generate preference pairs.
Together, these methods link long-term user personas to turn-level preferences via persona-conditioned
preference optimization.
We evaluate RLHI in three settings. (i)User-based evaluationwith ourWildChat UserEval: both
RLHI variants outperform strong baselines in personalization and instruction-following, and a human study
corroborates these trends. (ii)Standard instruction-following benchmarks:User-Based Rewardsattains a
77.9% length-controlled win rate on AlpacaEval 2.0, surpassing all baselines. (iii)Reasoning:User-Guided
Rewritesraises average accuracy from 26.5 to 31.8 across four benchmarks. Our ablation studies further show
that RLHI benefits from user guidance and interaction diversity, that reinforcement learning outperforms
supervised finetuning, and that quality filtering is essential for effectively leveraging noisy human interaction
data.
2 RLHI: Reinforcement Learning from Human Interaction
2.1 The Era of Real-World Human Interaction
Artificial intelligence (AI) has progressed rapidly in recent years through large-scale pretraining and fine-tuning
with human examples and preferences. Yet this trajectory is slowing: high-quality data is running out, and
imitation alone cannot push systems beyond existing human knowledge. Recent proposals call for anera of
experience(Silver and Sutton, 2025), in which AI systems advance by continually learning from their own
interactions with the world. Since these systems ultimately exist to assist humans, interaction with users
becomes a natural and essential dimension of this shift. Theera of real-world human interactionthus forms a
core pillar of the era of experience, providing both the raw data and personalization signals necessary for
adaptive, human-centered intelligence.
We define learning from human interaction as the process of improving AI models through natural, continual
exchanges with real users. Such interactions may involve messages, actions, requests, or demonstrations
provided in direct response to the model‚Äôs outputs. These exchanges not only reveal user goals and preferences
but also create an evolving feedback loop that enables systems to refine their behavior over time. To truly
benefit from human interaction, AI needs to go beyond coarse binary labels to absorb knowledge, preferences,
reasoning skills, perceptual cues, cooperative strategies, and social norms, learning deeper forms of intelligence
through interaction.
Compared with other training data sources, human interaction is distinguished by three key properties:
1. Contextual grounding‚Äî arises within the flow of ongoing tasks or conversations, directly tied to the
user‚Äôs situational needs and the model‚Äôs prior outputs, while being shaped by personalized knowledge of
the user‚Äôs profile, history, and preferences;
2√•
Why do humans need to sleep?
include numbers and statistics
Humans need sleep because it helps restore physical energy and supports processes like memory consolidation‚Ä¶
‚ãÆUser-Guided Rewrite:Adults need 7‚Äì9 hoursof sleep. Less than 6hours increases accident risk by 33%and cuts memory retention by 20%...User Persona:1. Favors answers with numbers, statistics, and concrete evidence.2. Prefers action-oriented and practical responses‚Ä¶Preference Data:persona & multi-turn context(preferred rewrite, dispreferred original)
Is coffee actually helpful?Coffee can be helpful, but‚Ä¶
(no feedback or follow-ups)
RLHI with User-Guided RewritesRLHI with User-Based Rewards
Response Candidate 1: One cup of coffee improves alertness in 15 minutesand reaction speed by ~10%...User-Based Reward = 1.8 (highest)
Response Candidate N: Coffee is generally helpful in moderation but harmful in excess intake‚Ä¶User-Based Reward = -0.6 (lowest)
chosen
‚ãÆ rejected
Preference Data:persona & multi-turn context(chosen, rejected)
Hi, my AI friend!Hi, my human friend! How‚Äòs your night going?
‚ãÆ
summarize long-term history
Conversation History 1-100 with       User ùë¢
 User Conversation 101User Conversation 102
Figure 2 Reinforcement Learning from Human Interaction (RLHI).We derive a natural-language persona summary from each
user‚Äôs long-term conversational history. For real-world requests, RLHI operates in two modes: (1) User-Guided Rewrites,
where unsatisfactory model outputs are revised based on users‚Äô natural-language follow-ups, creating preference pairs
between the original and rewritten responses; and (2) User-Based Rewards, where multiple candidate responses are
generated and ranked by a reward model conditioned on the user‚Äôs persona, yielding chosen-rejected pairs. Both
methods leverage personas and multi-turn context to enable personalized alignment.
2. Evolving distribution‚Äî reflects goals that shift, environments that change, and preferences that adapt
over time, thereby providing supervision that is temporally relevant and aligned with the real distribution
of human needs and priorities; and
3. Diverse supervision signals‚Äî appears in both explicit high-bandwidth signals beyond scalar rewards
(e.g., corrections or clarifications) and implicit cues (e.g., disengagement or frustration), and may include
style and role assignments, emotional tone, or even adversarial inputs such as jailbreak attempts, which
require careful handling, but also offer valuable information.
In this paper, we focus on large language models that engage daily with millions of users. Here, human
interaction takes the minimal form of textual messages, yet still conveys contextual, dynamic, and diverse
requests, holding unique potential as a driver of continual model improvement.
2.2 Analysis of Real-World Human Interaction
To determine the feasibility of our approach, we first considercurrently available human interaction data,
analyzing its properties. We note that these properties are necessarily tied to the capabilities of current
models, and we expect these statistics to change considerably in the coming years.
Users often provide feedback to improve model responses.We analyze user messages in the WildChat-1M
dataset, which contains over one million conversations with ChatGPT (Zhao et al., 2024b). In each multi-turn
conversation, the first message is theinitial request, and we prompt an GPT-4o model to classify user follow-up
messages into four types: (1)new requests, where the user shifts to a new topic, substantially reformulates the
original, or provides unrelated input; (2)re-attempts with feedback, where the user refines the initial prompt,
adds clarification, or provides explicit or implicit feedback; (3)re-attempts without feedback, where the same
prompt is repeated with no new input; and (4)positive feedback, where the user expresses praise or satisfaction.
We find the distributions are: 27.07% of user messages areinitial requests, 40.40% arenew requests, 26.51%
arere-attempts with feedback, 4.77% arere-attempts without feedback, and 1.25% arepositive feedback, with
more details and examples in Appendix A. Conversations of later stages are dominated byre-attempts with
feedback, accounting for 83.15% of user utterances after the fifth turn.re-attempts with feedbackare relatively
3