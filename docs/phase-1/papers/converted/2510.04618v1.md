# 2510.04618v1.pdf

Agentic Context Engineering: Evolving Contexts for Self-Improving
Language Models
Qizheng Zhang 1∗ Changran Hu 2∗ Shubhangi Upasani 2 Boyuan Ma 2 Fenglu Hong 2
Vamsidhar Kamanuru 2 Jay Rainton 2 Chen Wu 2 Mengmeng Ji 2 Hanchen Li 3
Urmish Thakker 2 James Zou 1 Kunle Olukotun 1
1 Stanford University 2 SambaNova Systems, Inc. 3 UC Berkeley ∗ equal contribution
/envel⌢peqizhengz@stanford.edu, changran.hu@sambanovasystems.com
Abstract
Large language model (LLM) applications such as agents and domain-specific reasoning increasingly rely on
context adaptation—modifying inputs with instructions, strategies, or evidence, rather than weight updates.
Prior approaches improve usability but often suffer from brevity bias, which drops domain insights for
concise summaries, and from context collapse, where iterative rewriting erodes details over time. Building on
the adaptive memory introduced by Dynamic Cheatsheet, we introduce ACE (AgenticContextEngineering),
a framework that treats contexts as evolving playbooks that accumulate, refine, and organize strategies
through a modular process of generation, reflection, and curation. ACE prevents collapse with structured,
incremental updates that preserve detailed knowledge and scale with long-context models. Across agent
and domain-specific benchmarks, ACE optimizes contexts both offline (e.g.,system prompts) and online (e.g.,
agent memory), consistently outperforming strong baselines: +10.6% on agents and +8.6% on finance, while
significantly reducing adaptation latency and rollout cost. Notably, ACE could adapt effectively without
labeled supervision and instead by leveraging natural execution feedback. On the AppWorld leaderboard,
ACE matches the top-ranked production-level agent on the overall average and surpasses it on the harder
test-challenge split, despite using a smaller open-source model. These results show that comprehensive,
evolving contexts enable scalable, efficient, and self-improving LLM systems with low overhead.
1 Introduction
Base LLM
ICL GEPA DC ACE
40.0
42.5
45.0
47.5
50.0
52.5
55.0
57.5
60.0Accuracy (%)
42.4%
46.0% 46.4%
51.9%
59.5%
Agent: AppWorld
Base LLM
ICL GEPA DC ACE
68
70
72
74
76
78
80
82
70.7%
72.3%
73.5%
74.2%
78.3%
Domain Knowledge: FiNER
Base LLM
ICL GEPA DC ACE
66
68
70
72
74
76
78
80
67.5% 67.0%
71.5%
69.5%
76.5%
Numerical Reasoning: Formula
Figure 1:Overall Performance Results.Our proposed framework, ACE, consistently outperforms strong
baselines across agent and domain-specific reasoning tasks.
Modern AI applications based on large language models (LLMs), such as LLM agents [49, 52] and compound
AI systems [55], increasingly depend oncontext adaptation. Instead of modifying model weights, context
arXiv:2510.04618v1  [cs.LG]  6 Oct 2025adaptation improves performance after model training by incorporating clarified instructions, structured
reasoning steps, or domain-specific input formats directly into the model’s inputs. Contexts underpin
many AI system components, including system prompts that guide downstream tasks [4, 36], memory that
carries past facts and experiences [41, 48], and factual evidence that reduces hallucination and supplements
knowledge [6].
Adapting throughcontextsrather thanweightsoffers several key advantages. Contexts are interpretable and
explainable for users and developers [45, 47], allow rapid integration of new knowledge at runtime [7, 27],
and can be shared across models or modules in a compound system [ 23]. Meanwhile, advances in long-
context LLMs [39] and context-efficient inference such as KV cache reuse [17, 51] are making context-based
approaches increasingly practical for deployment. As a result, context adaptation is emerging as a central
paradigm for building capable, scalable, and self-improving AI systems.
Despite this progress, existing approaches to context adaptation face two key limitations. First, abrevity
bias: many prompt optimizers prioritize concise, broadly applicable instructions over comprehensive
accumulation. For example, GEPA [ 4] highlights brevity as a strength, but such abstraction can omit
domain-specific heuristics, tool-use guidelines, or common failure modes that matter in practice [16]. This
objective aligns with validation metrics in some settings, but often fails to capture the detailed strategies
required by agents and knowledge-intensive applications. Second,context collapse: methods that rely on
monolithic rewriting by an LLM often degrade into shorter, less informative summaries over time, causing
sharp performance declines (Figure 2). In domains such as interactive agents [38, 43, 57], domain-specific
programming [53, 56], and financial or legal analysis [18, 33, 44], strong performance depends on retaining
detailed, task-specific knowledge rather than compressing it away.
As applications such as agents and knowledge-intensive reasoning demand greater reliability, recent work
has shifted toward saturating contexts with abundant, potentially useful information [11, 12, 22], enabled by
advances in long-context LLMs [34, 39].We argue that contexts should function not as concise summaries,
but as comprehensive, evolving playbooks—detailed, inclusive, and rich with domain insights.Unlike
humans, who often benefit from concise generalization, LLMs are more effective when provided with long,
detailed contexts and can distill relevance autonomously [22, 31, 41]. Thus, instead of compressing away
domain-specific heuristics and tactics, contexts should preserve them, allowing the model to decide what
matters at inference time.
To address these limitations, we introduce ACE (AgenticContextEngineering), a framework for compre-
hensive context adaptation in both offline settings (e.g.,system prompt optimization) and online settings
(e.g.,test-time memory adaptation). Rather than compressing contexts into distilled summaries, ACE treats
them as evolving playbooks that accumulate and organize strategies over time. Building on the agentic
architecture of Dynamic Cheatsheet [41], ACE incorporates a modular workflow of generation, reflection,
and curation, while adding structured, incremental updates guided by a grow-and-refine principle. This
design preserves detailed, domain-specific knowledge, prevents context collapse, and yields contexts that
remain comprehensive and scalable throughout adaptation.
We evaluate ACE on two categories of LLM applications that most benefit from comprehensive, evolving
contexts: (1)agents[ 43], which require multi-turn reasoning, tool use, and environment interaction, where
accumulated strategies can be reused across episodes; and (2)domain-specific benchmarks, which demand
specialized tactics and knowledge, where we focus on financial analysis [33, 44]. Our key findings are:
• ACE consistently outperforms strong baselines, yielding average gains of 10.6% onagentsand 8.6% on
domain-specific benchmarks, across both offline and online adaptation settings.
• ACE is able to construct effective contextswithoutlabeled supervision, instead leveraging execution
feedback and environment signals—key ingredients for self-improving LLMs and agents.
• On the AppWorld benchmark leaderboard [5], ACE matches the top-ranked production-level agent IBM-
CUGA [35] (powered by GPT-4.1) on average and surpasses it on the harder test-challenge split, while
using a smaller open-source model (DeepSeek-V3.1).
2• ACE requires significantly fewer rollouts and lower dollar costs, and achieves 86.9% lower adaptation
latency (on average) than existing adaptive methods, demonstrating that scalable self-improvement can be
achieved with both higher accuracy and lower overhead.
2 Background and Motivation
2.1 Context Adaptation
Context adaptation (or context engineering) refers to methods that improve model behavior by constructing
or modifying inputs to an LLM, rather than altering its weights. The current state of the art leveragesnatural
language feedback[ 4, 40, 54]. In this paradigm, a language model inspects the current context along with
signals such as execution traces, reasoning steps, or validation results, and generates natural language
feedback on how the context should be revised. This feedback is then incorporated into the context, enabling
iterative adaptation. Representative methods include Reflexion [40], which reflects on failures to improve
agent planning; TextGrad [54], which optimizes prompts via gradient-like textual feedback; GEPA [4], which
refines prompts iteratively based on execution traces and achieves strong performance, even surpassing
reinforcement learning approaches in some settings; and Dynamic Cheatsheet [ 41], which constructs an
external memory that accumulates strategies and lessons from past successes and failures during inference.
These natural language feedback methods represent a major advance, offering flexible and interpretable
signals for improving LLM systems beyond weight updates.
2.2 Limitations of Existing Context Adaptation Methods
The Brevity Bias.A recurring limitation of context adaptation methods isbrevity bias: the tendency of
optimization to collapse toward short, generic prompts. Gao et al. [ 16] document this effect in prompt
optimization for test generation, where iterative methods repeatedly produced near-identical instructions
(e.g.,"Create unit tests to ensure methods behave as expected"), sacrificing diversity and omitting domain-
specific detail. This convergence not only narrows the search space but also propagates recurring errors
across iterations, since optimized prompts often inherit the same faults as their seeds. More broadly, such
bias undermines performance in domains that demand detailed, context-rich guidance—such as multi-step
agents, program synthesis, or knowledge-intensive reasoning—where success hinges on accumulating rather
than compressing task-specific insights.
# Tokens: 18,282Accuracy: 66.7 
# Tokens: 122Accuracy: 57.1 Accuracy w/o context: 63.7
Figure 2:Context Collapse.Monolithic rewriting of context by an LLM can collapse it into shorter, less
informative summaries, leading to sharp performance drops.
Context Collapse.In a case study on the AppWorld benchmark [ 43], we observe a phenomenon we
callcontext collapse, which arises when an LLM is tasked with fully rewriting the accumulated context at
each adaptation step. As the context grows large, the model tends to compress it into much shorter, less
informative summaries, causing a dramatic loss of information. For instance, at step 60 the context contained
3