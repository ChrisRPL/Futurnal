# 2505.17631v1.pdf

arXiv:2505.17631v1  [cs.IR]  23 May 2025
BehaveGPT: A Foundation Model for Large-scale User Behavior Modeling
JIAHUI GONG, Department of Electronic Engineering, BNRist, Tsinghua University, Beijing, China
JINGTAO DING,Department of Electronic Engineering, BNRist, Tsinghua University, Beijing, China
FANJIN MENG, Department of Electronic Engineering, BNRist, Tsinghua University, Beijing, China
CHEN YANG, Honor Device Co., Ltd , Beijing, China
HONG CHEN, Honor Device Co., Ltd , Beijing, China
ZUOJIAN WANG,Honor Device Co., Ltd , Beijing, China
HAISHENG LU, Honor Device Co., Ltd , Beijing, China
YONG LI, Department of Electronic Engineering, BNRist, Tsinghua University, Beijing, China
In recent years, foundational models have revolutionized the fields of language and vision, demonstrating remarkable abilities in
understanding and generating complex data; however, similar advances in user behavior modeling have been limited, largely due
to the complexity of behavioral data and the challenges involved in capturing intricate temporal and contextual relationships in
user activities. To address this, we propose BehaveGPT, a foundational model designed specifically for large-scale user behavior
prediction. Leveraging transformer-based architecture and a novel pretraining paradigm, BehaveGPT is trained on vast user behavior
datasets, allowing it to learn complex behavior patterns and support a range of downstream tasks, including next behavior prediction,
long-term generation, and cross-domain adaptation. Our approach introduces the DRO-based pretraining paradigm tailored for user
behavior data, which improves model generalization and transferability by equitably modeling both head and tail behaviors. Extensive
experiments on real-world datasets demonstrate that BehaveGPT outperforms state-of-the-art baselines, achieving more than than a
10% improvement in macro and weighted recall, showcasing its ability to effectively capture and predict user behavior. Furthermore,
we measure the scaling law in the user behavior domain for the first time on the Honor dataset, providing insights into how model
performance scales with increased data and parameter sizes.
CCS Concepts: • Information systems →Recommender systems; Personalization; • Computing methodologies →Machine
learning.
Additional Key Words and Phrases: Behavior modeling, Foundation model, long-tail data learning
1 Introduction
Recently, many foundational models have emerged in the language and vision domains [9, 13, 40, 45]. These models are
trained on massive datasets and exhibit strong generalization capabilities, allowing them to address various downstream
tasks in their respective domains. However, these models primarily fall under the category of perceptual intelligence,
which is considered an early stage of artificial intelligence [ 1, 32]. While perceptual intelligence enables models to
perceive and process data, it has limitations when it comes to understanding, predicting user behaviors, and making
decisions based on behavioral data, which are the key components of behavioral intelligence [ 15, 33]. Behavioral
intelligence is crucial for intelligent agents, such as robots [44, 51], automated systems [18, 43], and virtual characters
Authors’ Contact Information: Jiahui Gong, Department of Electronic Engineering, BNRist, Tsinghua University, Beijing, China, gjh22@mails.tsinghua.
edu.cn; Jingtao Ding, Department of Electronic Engineering, BNRist, Tsinghua University, Beijing, China, dingjt15@tsinghua.org.cn; Fanjin Meng,
Department of Electronic Engineering, BNRist, Tsinghua University, Beijing, China, mengfj23@mails.tsinghua.edu.cn; Chen Yang, Honor Device Co., Ltd
, Beijing, China, yangchen6@honor.com; Hong Chen, Honor Device Co., Ltd , Beijing, China, chenhong3@honor.com; Zuojian Wang, Honor Device Co.,
Ltd , Beijing, China, wangzuojian@honor.com; Haisheng Lu, Honor Device Co., Ltd , Beijing, China, luhaisheng@honor.com; Yong Li, Department of
Electronic Engineering, BNRist, Tsinghua University, Beijing, China, liyong07@tsinghua.edu.cn.
Manuscript submitted to ACM 12 Jiahui Gong et al.
Language 
Domain
GPT
Llama
Qw en
GLM
LLa V A
Int ent VL
Clip
GPT -4v
ViT
User Beha vior 
Domain
Visual 
Domain
Summar y
T r anslation
R eading Compr ehension
Q A syst em
Dialogue
Classification
Segment
Det ection
Caption
Beha viorGPT
Ne xt beha vior 
pr ediction
Ne w beha vior 
pr ediction
Long-t erm 
gener ation
Cr oss-domain 
pr ediction
App usage pr ediction User int ent pr ediction Ne xt -POI r ecommendation
Fig. 1. The paradigm of foundation model.
[29, 36], as it allows them to optimize their actions to achieve specific goals. These agents must be capable of perceiving
their environment, learning from behaviors, and predicting future behaviors to adapt and make intelligent decisions.
User behavior prediction is a critical first step toward achieving behavioral intelligence, as it enables models to
understand and predict future actions based on an individual’s historical behavior. This capability is essential for a wide
range of applications, such as personalized recommendations [21], next point of interest (POI) predictions [28], and user
intent predictions cite pituning. However, recent works [ 10, 25] have primarily used large language models (LLMs)
to model and predict user behavior. While LLMs excel at processing sequential data, they are inherently optimized
for the language domain, where data distributions tend to be relatively balanced. In contrast, the behavior domain
presents a starkly different challenge, characterized by highly imbalanced distributions: a small number of frequent
Manuscript submitted to ACMBehaveGPT: A Foundation Model for Large-scale User Behavior Modeling 3
behaviors dominate, while the majority consist of infrequent, long-tail behaviors. This imbalance complicates user
behavior prediction, as LLMs often fail to capture the nuances, variations, and intricate dependencies inherent in
diverse behavioral patterns. As a result, they struggle to model the full spectrum of user activities and provide equitable
predictions for both common and rare behaviors.
In this paper, we propose BehaveGPT, a foundational model designed for user behavior modeling. Trained on vast
amounts of user behavior data, more than 600 million behavior logs in total, BehaveGPT is capable of understanding
and capturing diverse behavior patterns, leveraging its pretrained knowledge and flexible architecture to support a
range of downstream tasks, such as next behavior prediction, new behavior prediction, long-term behavior generation,
and cross-domain prediction. Specifically, we develop a transformer-based framework and introduce a new pretraining
paradigm tailored to user behavior data, enabling more effective and fair modeling of user behaviors. This design
enhances the model’s transferability and adaptation capabilities, making it more robust across different tasks and
domains. Besides, we also explore the scaling phenomenon in the user behavior domain, offering insights into how
model performance improves with increasing data and parameter sizes. To summarize, our main contributions are as
follows,
•We are the first to train a foundational model specifically for the user behavior domain. After training on extensive
user behavior data, we have developed a model that surpasses traditional LLMs in this area and supports multiple
downstream tasks. Additionally, our initial exploration of the scaling phenomenon in the user behavior domain
demonstrates that increasing both data size and model capacity significantly enhances performance, with models
trained from scratch ultimately surpassing those initialized with pretrained LLM parameters as data scales.
•We propose a transformer-based framework and introduce a novel DRO-based pretraining paradigm specifically
tailored to user behavior data, allowing for more effective and equitable modeling of user behaviors to enhance
the model’s transferability and generalization capabilities.
•Extensive experiments on three real-world datasets demonstrate the superiority of BehaveGPT over state-of-the-
art baselines, achieving more than a 10% improvement in macro and weighted recall and supporting various
downstream tasks.
2 Preliminary
2.1 Data Ananlysis
We begin with a comprehensive data analysis, revealing a highly imbalanced distribution in user behavior data. A small
number of common behaviors occur frequently, while the majority of behaviors are rarely observed.
Figure 2 illustrates the behavior distribution across two datasets, categorizing behaviors into four primary categories.
The Honor dataset collects users’ mobile phone logs, while the Tencent dataset collects users’ trajectory data. As shown
in the figure, the long-tail problem is apparent, and different behaviors dominate in each dataset. These variations
highlight distinct user behavior patterns across different domains. Such an imbalance requires innovative solutions to
ensure fair representation, making it essential to address both common and rare behaviors equitably to improve model
robustness, generalization, and predictive accuracy across the entire spectrum of user activities.
2.2 Problem Definition
Now we give a formal definition of our research problem:
Manuscript submitted to ACM