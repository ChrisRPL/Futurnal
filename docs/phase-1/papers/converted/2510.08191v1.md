# 2510.08191v1.pdf

Training-Free GRPO
Training-Free Group Relative Policy Optimization
Youtu-Agent Team∗
Recent advances in Large Language Model (LLM) agents have demonstrated their promising general
capabilities. However, their performance in specialized real-world domains often degrades due
to challenges in effectively integrating external tools and specific prompting strategies. While
methods like agentic reinforcement learning have been proposed to address this, they typically
rely on costly parameter updates, for example, through a process that uses Supervised Fine-Tuning
(SFT) followed by a Reinforcement Learning (RL) phase with Group Relative Policy Optimization
(GRPO) to alter the output distribution. However, we argue that LLMs can achieve a similar effect
on the output distribution by learning experiential knowledge as a token prior, which is a far more
lightweight approach that not only addresses practical data scarcity but also avoids the common issue
of overfitting. To this end, we propose Training-Free Group Relative Policy Optimization (Training-
Free GRPO), a cost-effective solution that enhances LLM agent performance without any parameter
updates. Our method leverages the group relative semantic advantage instead of numerical ones
within each group of rollouts, iteratively distilling high-quality experiential knowledge during
multi-epoch learning on a minimal ground-truth data. Such knowledge serves as the learned token
prior, which is seamlessly integrated during LLM API calls to guide model behavior. Experiments
on mathematical reasoning and web searching tasks demonstrate that Training-Free GRPO, when
applied to DeepSeek-V3.1-Terminus, significantly improves out-of-domain performance. With just a
few dozen training samples, Training-Free GRPO outperforms fine-tuned small LLMs with marginal
training data and cost.
Date:October 9, 2025
Correspondence:tristanli@tencent.com
Code:https://github.com/TencentCloudADP/youtu-agent/tree/training_free_GRPO
Figure 1.Applying Training-Free GRPO on both prompting (without tools) and ReAct [ 1] (with tools) achieve improved
Mean@32 on AIME benchmarks [2] with DeepSeek-V3.1-Terminus [3]. It consumes significantly fewer training data and
lower costs on the 671B LLM than fine-tuning a 32B model [4], serving as a cost-effective alternative to RL methods.
*Full author list in contributions.
1
arXiv:2510.08191v1  [cs.CL]  9 Oct 2025Training-Free GRPO
1 Introduction
Large Language Models (LLMs) are emerging as powerful general-purpose agents capable of interacting
with complex, real-world environments. They have shown remarkable capabilities across a wide range
of tasks, including complex problem-solving [4, 5, 6], advanced web research [7, 8, 9, 10], code generation
and debugging [ 11, 12], and proficient computer use [ 13, 14, 15]. Despite their impressive capabilities,
LLM agents often underperform in specialized, real-world domains. These scenarios typically demand the
integration of external tools (e.g., calculators, APIs, databases), along with domain-specific task definitions
and prompting strategies. Deploying a general-purpose agent out-of-the-box in such settings often results
in suboptimal performance due to limited familiarity with domain-specific requirements or insufficient
exposure to necessary tools.
To bridge this gap, agentic training has emerged as a promising strategy to facilitate the adaptation of LLM
agents to specific domains and their associated tools [4, 7, 8, 16]. Recent advancements in agentic reinforce-
ment learning (Agentic RL) approaches have employed Group Relative Policy Optimization (GRPO) [17]
and its variants [ 18, 19, 20] to align model behaviors in the parameter space. Although these methods
effectively enhance task-specific capabilities, their reliance on tuning LLM parameters poses several practical
challenges:
• Computational Cost:Even for smaller models, fine-tuning demands substantial computational resources,
making it both costly and environmentally unsustainable. For larger models, the costs become prohibitive.
Furthermore, fine-tuned models require dedicated deployment and are often limited to specific applica-
tions, rendering them inefficient for low-frequency use cases compared to more versatile general-purpose
models.
• Poor Generalization:Models optimized via parameter tuning often suffer from unsatisfactory cross-
domain generalization, limiting their applicability to narrow tasks. Consequently, multiple specialized
models must be deployed to handle a comprehensive set of tasks, significantly increasing system com-
plexity and maintenance overhead.
• Data Scarcity:Fine-tuning LLMs typically necessitates large volumes of high-quality, carefully annotated
data, which are often scarce and prohibitively expensive to obtain in specialized domains. Additionally,
with limited samples, models are highly susceptible to overfitting, leading to poor generalization.
• Diminishing Returns:The prohibitive training costs usually compel existing approaches to fine-tune
smaller LLMs with fewer than 32 billion parameters, due to resource constraints rather than optimal
design choices. While larger models would be preferred, the computational expense of fine-tuning
necessitates this compromise. Paradoxically, API-based or open-source larger LLMs often deliver better
cost-performance ratios through scalability and continuous model updates. However, these general-
purpose models underperform in specialized domains where fine-tuning is necessary, creating a cost-
performance dilemma.
Such limitations inherent in parameter tuning motivate a fundamental research question:Is applying RL in
parametric space the only viable approach? Can we enhance LLM agent performance in a non-parametric way with
lower data and computational costs?
We answer this question affirmatively by proposingTraining-Free Group Relative Policy Optimization
(Training-Free GRPO), a novel and efficient method that improves LLM agent behavior in a manner similar
to vanilla GRPO, while preserving the original model parameters unchanged. Our approach is motivated by
the insight that LLMs already possess the fundamental capability to adapt to new scenarios, requiring only
minimal practice through limited samples to achieve strong performance. Thus, instead of adapting their
output distribution through parameter tuning, in-context learning [21] that leverages a lightweighttoken
priorcan also encapsulate experiential knowledge learned from a minimal training dataset.
2Training-Free GRPO
Training-Free GRPO retains the multi-epoch learning mechanism of vanilla GRPO. In each epoch, multiple
outputs are generated to deliver a group of rollouts for every query, which helps to explore the policy
space and evaluate potential strategies. While vanilla GRPO relies on gradient-based parameter updates
to iteratively improve policy performance, Training-Free GRPO eliminates this requirement by employing
inference-only operations using LLMs. At each optimization step, rather than calculating a numerical
advantage for gradient ascent within each group of rollouts, our method leverages LLMs to introspect on
each group and distill a semantic advantage. Such advantage refines external experiential knowledge and
guide policy outputs based on evolving contextual priors, thereby achieving policy optimization effects
without modifying any model parameters.
By evaluating challenging mathematical reasoning and interactive web searching tasks, we demonstrate
that our method significantly enhances the performance of frozen policy models such as DeepSeek-V3.1-
Terminus [3] with only dozens of training samples. It surpasses fine-tuned 32B models in performance
while requiring only a fraction of the computational resources, offering a simple and much more efficient
alternative to traditional fine-tuning techniques.
Our principal contributions are threefold:
• A New Training-Free RL Paradigm:We introduce Training-Free GRPO, which shifts policy optimization
from the parameter space to the context space by leveraging evolving experiential knowledge as token
priors without gradient updates.
• Semantic Group Advantage:We replace numerical group advantage in vanilla GRPO withsemantic
group advantage, enabling LLMs to introspect their own rollouts and continuously updating experiential
knowledge at multiple optimization steps.
• Data and Computational Efficiency:Experiments confirm that Training-Free GRPO effectively enhances
the performance of a frozen policy with minimal training samples, offering a practical and cost-effective
alternative across different domains.
• Superior Generalization:By leaving model parameters frozen and plugging in different token priors,
our approach fully preserves the generalization power, eliminating the cost and complexity of deploying
multiple fine-tuned specialists.
2 Training-Free GRPO
In this section, we introduce our Training-Free GRPO, a method designed to replicate the alignment benefits
of the GRPO algorithm without performing any gradient-based updates to the policy model’s parameters.
Vanilla GRPO.As shown in Figure 2, the vanilla GRPO procedure operates by first generating a group of
G outputs {o1, o2, . . ., oG} for a given query q using the current policy LLM πθ, i.e., πθ(oi |q) . Each output
oi is then independently scored with a reward model R. Subsequently, with rewards r={r 1, . . ., rG}, it
calculates a group-relative advantage ˆAi = ri−mean(r)
std(r) for each output oi. By combining a KL-divergence
penalty against a reference model, it constructs a PPO-clipped objective function JGRPO(θ), which is then
maximized to update the LLM parametersθ.
Training-Free GRPO repurposes the core logic of this group-based relative evaluation but translates it into
a non-parametric, inference-time process. Instead of updating the parameters θ, we leave θ permanently
frozen and maintain an externalexperiential knowledgeE, which is initialized to∅.
3