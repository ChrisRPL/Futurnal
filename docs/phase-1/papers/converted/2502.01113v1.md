# 2502.01113v1.pdf

GFM-RAG: Graph Foundation Model for Retrieval Augmented Generation
Linhao Luo * 1 Zicheng Zhao * 2 Gholamreza Haffari 1 Dinh Phung 1 Chen Gong 2 Shirui Pan 3
Abstract
Retrieval-augmented generation (RAG) has
proven effective in integrating knowledge into
large language models (LLMs). However, conven-
tional RAGs struggle to capture complex relation-
ships between pieces of knowledge, limiting their
performance in intricate reasoning that requires
integrating knowledge from multiple sources. Re-
cently, graph-enhanced retrieval augmented gen-
eration (GraphRAG) builds graph structure to ex-
plicitly model these relationships, enabling more
effective and efficient retrievers. Nevertheless,
its performance is still hindered by the noise and
incompleteness within the graph structure. To ad-
dress this, we introduce GFM-RAG, a novel graph
foundation model (GFM) for retrieval augmented
generation. GFM-RAG is powered by an innova-
tive graph neural network that reasons over graph
structure to capture complex query-knowledge re-
lationships. The GFM with 8M parameters under-
goes a two-stage training process on large-scale
datasets, comprising 60 knowledge graphs with
over 14M triples and 700k documents. This re-
sults in impressive performance and generalizabil-
ity for GFM-RAG, making it the first graph foun-
dation model applicable to unseen datasets for
retrieval without any fine-tuning required. Exten-
sive experiments on three multi-hop QA datasets
and seven domain-specific RAG datasets demon-
strate that GFM-RAG achieves state-of-the-art per-
formance while maintaining efficiency and align-
ment with neural scaling laws, highlighting its
potential for further improvement1.
1. Introduction
Recent advancements in large language models (LLMs)
(OpenAI, 2024a; Meta, 2024; Yang et al., 2024) have greatly
*Equal contribution 1Monash University 2Nanjing Univer-
sity of Science and Technology 3Griffith University. Correspon-
dence to: Shirui Pan <s.pan@griffith.edu.au>, Linhao Luo <lin-
hao.luo@monash.edu>.
1Code: https://github.com/RManLuo/gfm-rag
Documents KG-index
Q GFM
Retriever
Retrieved Docs.
Doc.
Ranker
Q LLM A
Query Answer
Figure 1.The overview framework of GFM-RAG. We feed the
query and a constructed KG-index into the graph foundation model
retriever to obtain relevant documents for LLM generation.
propelled the evolution of natural language processing, po-
sitioning them as foundational models for artificial general
intelligence (AGI). Despite the remarkable reasoning abil-
ity (OpenAI, 2024b), LLMs are still limited in accessing
real-time information and lack of domain-specific knowl-
edge, which is outside the pre-training corpus. To address
these limitations, retrieval-augmented generation (RAG)
(Gao et al., 2023) has become a popular paradigm in adding
new knowledge to the static LLMs by retrieving relevant
documents into the context of LLM generation.
Existing RAG methods typically retrieve documents inde-
pendently, making it difficult to capture complex relation-
ships between pieces of knowledge (Karpukhin et al., 2020;
Chen et al., 2023; Moreira et al., 2024). This limitation
hampers the performance of LLMs in integrating knowl-
edge across document boundaries, particularly in multi-hop
reasoning tasks (Yang et al., 2018; Trivedi et al., 2022) and
real-world applications like legal judgment (Kang et al.,
2024) and medical diagnoses (Jin et al., 2019), which re-
quire reasoning over multiple sources. Although recent
methods have expanded the retrieval process into multiple
steps and incorporate LLM reasoning, they still encounter
high computational costs due to iterative retrieval and rea-
soning with LLMs (Trivedi et al., 2023; Sun et al., 2024;
Joshi et al., 2024).
Recently, graph-enhanced retrieval augmented generation
(GraphRAG) (Peng et al., 2024; Han et al., 2024) has
emerged as a novel solution that builds a graph structure to
explicitly model the intricate relationships between knowl-
edge. This enables the development of a graph-enhanced
retriever to identify relevant information using graphs. The
structural nature of graphs allows GraphRAG to capture
1
arXiv:2502.01113v1  [cs.IR]  3 Feb 2025GFM-RAG: Graph Foundation Model for Retrieval Augmented Generation
global context and dependencies among documents, signifi-
cantly improving reasoning across multiple sources (Edge
et al., 2024). Methods like HippoRAG (Guti ´errez et al.,
2024) enhance retrieval by employing a personalized PageR-
ank algorithm to locate relevant knowledge with graphs.
However, these algorithms rely solely on the graph structure,
which is often noisy or incomplete, limiting their overall
performance. Alternative methods (Mavromatis & Karypis,
2024; He et al., 2024) incorporate graph neural networks
(GNNs) into the retrieval process. These methods have
shown impressive performance due to GNNs’ powerful rea-
soning capabilities in handling incomplete graphs (Galkin
et al., 2024). Nevertheless, they still face limitations in gen-
eralizability since they require training from scratch on new
datasets.
Nowadays, the search for a foundation GNN model that
can transfer and generalize across different datasets has
been an active research topic. Ideally, a foundation GNN
or graph foundation model (GFM) can benefit from large-
scale training and generalize across diverse graphs (Mao
et al., 2024; Liu et al., 2023). Efforts have been made to
identify transferable graph tokens (e.g., motifs, sub-trees,
and relation graphs) (Galkin et al., 2024; Wang et al., 2024;
Xia et al., 2024) that can be shared among different graphs
for GFM design. However, these methods primarily focus
on graph-related tasks (e.g., node classification and link
prediction), leaving the design of a GFM to enhance LLMs’
reasoning ability unexplored.
To bridge the gap, in this paper, we propose an effective, effi-
cient, and general graph foundation model for retrieval aug-
mented generation (GFM-RAG), thereby enhancing LLMs’
reasoning ability. As shown in Figure 1, we create a knowl-
edge graph index (KG-index) from documents in each
dataset. The KG-index consists of interconnected factual
triples pointing to the original documents, which serves as
a structural knowledge index across multiple sources, en-
hancing the integration of diverse knowledge for complex
reasoning tasks (Guti´errez et al., 2024). Then, we present the
graph foundation model retriever (GFM retriever), driven
by a query-dependent GNN that captures complex query-
knowledge relationships in a unified, transferable space of
semantics and graph structure. Through multi-layer mes-
sage passing, the GFM retriever enables efficient multi-hop
retrieval in a single step, surpassing previous multi-step
methods. The GFM retriever, with 8M parameters, under-
goes a two-stage training: unsupervised KG completion
pre-training and supervised document retrieval fine-tuning
on large-scale datasets, including 60 knowledge graphs with
over 14M triples and 700k documents. This large-scale
training ensures the generalizability of GFM retriever to be
applied directly to unseen datasets without further training.
In experiments, GFM-RAG achieves state-of-the-art perfor-
mance across three multi-hop QA datasets, demonstrating
its effectiveness and efficiency in multi-hop reasoning. It
also generalizes well across seven RAG datasets from di-
verse domains, such as biomedical, customer service, and
general knowledge, without requiring additional training.
Furthermore, GFM-RAG follows the neural scaling law (Hes-
tness et al., 2017), whose performance benefits from training
data and model size scaling, emphasizing its potential as a
foundational model for future improvements.
The main contributions of this paper are as follows:
• We introduce a graph foundation model for retrieval
augmented generation ( GFM-RAG), powered by a
novel query-dependent GNN to enable efficient multi-
hop retrieval within a single step.
• We train a large-scale model with 8M parameters,
marking the first graph foundation model (GFM) that
can be applied directly to various unseen datasets for
retrieval augmented generation.
• We evaluateGFM-RAG on three multi-hop QA datasets
and seven domain-specific RAG datasets, achieving
state-of-the-art performance across all, demonstrating
its effectiveness, efficiency, generalizability, and poten-
tial as a foundational model for further enhancement.
2. Related Work
Retrieval-augmented generation (RAG)(Gao et al., 2023)
provides an effective way to integrate external knowledge
into large language models (LLMs) by retrieving relevant
documents to facilitate LLM generation. Early works adopt
the pre-trained dense embedding model to encode docu-
ments as separate vectors (Karpukhin et al., 2020; Chen
et al., 2023; Li et al., 2023b; Moreira et al., 2024), which
are then retrieved by calculating the similarity to the query.
Despite efficiency and generalizability, these methods strug-
gle to capture complex document relationships. Subsequent
studies have explored multi-step retrieval, where LLMs
guide an iterative process to retrieve and reason over multi-
ple documents (Trivedi et al., 2023; Jiang et al., 2023; Su
et al., 2024). However, this approach is computationally
expensive.
Graph-enhanced retrieval augmented generation
(GraphRAG) (Peng et al., 2024; Han et al., 2024) is a
novel approach that builds graphs to explicitly model the
complex relationships between knowledge, facilitating
comprehensive retrieval and reasoning. Early research
focuses on retrieving information from existing knowledge
graphs (KGs), such as WikiData (Vrande ˇci´c & Kr ¨otzsch,
2014) and Freebase (Bollacker et al., 2008), by identifying
relevant facts or reasoning paths (Li et al., 2023a; LUO
2GFM-RAG: Graph Foundation Model for Retrieval Augmented Generation
Graph Foundation Model for Retrieval Augmented Generation
Barack Obama (born in
August 4, 1961, Honolulu)
is an American
politician… He married
to Michelle Obama.
Honolulu is the capital
and most populous city
of the U.S. state of
Hawaii, which is in the
Pacific Ocean.
Michelle Obama is served as
the first lady of the United
States from 2009 to 2017,
being married to Barack
Obama.
USA is a country primarily
located in North America. It is
a federal union of 50
states, the federal capital
district of Washington, D.C.
born_in
married_to
politician_of
city_of
live_in
Barack
Obama
Michelle
Obama
Washington,
D.C.
Honolulu
USA
capital_of
Q: Barack Obama is
the politician of
which country?
0.1
0.9
0.2
0.6
Document
Corpus
Query Embedding
①  KG-Index
Construction
② Query
Initialization 
③  Query-dependent
Message Passing
Knowledge Graph
Ent. Relevance
Scores
Doc. 1 Doc. 2
Doc. 3 Doc. 4
Graph Foundation
GNN Model
Doc.
Ranker
LLMQ A: USA
Ent. to Doc.
Inverted
Index
Retrieved
Docs.
⑤  LLM Generation
Q: (Barack Obamna,
born_in, ?) A: Honolulu
Triple Target Ent.
Stage 1: Unsupervised KG Completion Pre-training
Graph Foundation
GNN Model
Q: Where was Barack
Obamna born in?
A: Honolulu,
USA
Question Target 
Ents.
Stage 2: Supervised Document Retrieval Finetuning
Graph Foundation
GNN Model
Supporting
Doc.
④  Doc. Ranking
Training Knowledge Graphs Training Question-Doc. Pairs and KG-Indexes
Figure 2.The detailed framework of GFM-RAG and training processes of graph foundation model. The GFM-RAG consists of three main
components: A. KG-index construction, which constructs a knowledge graph index from document corpus ( 1 ); B. graph foundation
model retriever (GFM retriever), which is pre-trained on large-scale datasets and could retrieve documents based on any user query and
KG-index ( 2 3 ); and C. documents ranking and answer generation, which ranks retrieved documents and generates final answer ( 4 5 ).
et al., 2024). Recent studies have integrated documents with
KGs to improve knowledge coverage and retrieval (Edge
et al., 2024; Liang et al., 2024). A graph structure is built
from these documents to aid in identifying relevant content
for LLM generation (Dong et al., 2024). Based on graphs,
LightRAG (Guo et al., 2024) incorporates graph structures
into text indexing and retrieval, enabling efficient retrieval
of entities and their relationships. HippoRAG (Guti ´errez
et al., 2024) enhances multi-hop retrieval by using a person-
alized PageRank algorithm to locate relevant knowledge
with graphs. However, the graph structure can be noisy and
incomplete, leading to suboptimal performance. Efforts to
incorporate GNNs into graph-enhanced RAG (Mavromatis
& Karypis, 2024; He et al., 2024) have shown impressive
results due to the strong graph reasoning capabilities of
GNNs in handling incomplete graphs (Galkin et al., 2024).
Nonetheless, these methods still limit in generalizability
due to the lack of a graph foundational model.
Graph Foundation models (GFM) aims to be a large-scale
model that can generalize to various datasets (Mao et al.,
2024; Liu et al., 2023). The main challenge in designing
GFMs is identifying graph tokens that capture invariance
across diverse graph data. For instance, ULTRA (Galkin
et al., 2024) employs four fundamental relational interac-
tions in knowledge graphs (KGs) to create a GFM for link
prediction. OpenGraph (Xia et al., 2024) develops a graph
tokenizer that converts graphs into a unified node token
representation, enabling transformer-like GFMs for tasks
such as link prediction and node classification. GFT (Wang
et al., 2024) introduces a transferable tree vocabulary to con-
struct a GFM that demonstrates effectiveness across various
tasks and domains in graph learning. Despite these success-
ful efforts, most methods primarily focus on conventional
graph-related tasks. How to design a GFM to enhance the
reasoning of LLM remains an open question.
3