# 2510.05592v1.pdf

IN-THE-FLOWAGENTICSYSTEMOPTIMIZATION FOR
EFFECTIVEPLANNING ANDTOOLUSE
Zhuofeng Li‚àó1,2, Haoxiang Zhang‚àó1,3, Seungju Han1, Sheng Liu1, Jianwen Xie4,
Yu Zhang2,Yejin Choi 1,James Zou ‚Ä†1,Pan Lu ‚Ä†1
1Stanford University, 2Texas A&M University,3UC San Diego, 4Lambda
Website: https://agentflow.stanford.edu
Code
 Model
 Demo
 Visualize
ABSTRACT
Outcome-driven reinforcement learning has advanced reasoning in large language
models (LLMs), but prevailing tool-augmented approaches train a single, mono-
lithic policy that interleaves thoughts and tool calls under full context; this scales
poorly with long horizons and diverse tools and generalizes weakly to new scenar-
ios. Agentic systems offer a promising alternative by decomposing work across
specialized modules, yet most remain training-free or rely on offline training de-
coupled from the live dynamics of multi-turn interaction. We introduce AGENT-
FLOW, a trainable,in-the-flowagentic framework that coordinates four modules
(planner, executor, verifier, generator) through an evolving memory and directly
optimizes its planner inside the multi-turn loop. To train on-policy in live environ-
ments, we proposeFlow-based Group Refined Policy Optimization(Flow-GRPO),
which tackles long-horizon, sparse-reward credit assignment by converting multi-
turn optimization into a sequence of tractable single-turn policy updates. It broad-
casts a single, verifiable trajectory-level outcome to every turn to align local plan-
ner decisions with global success and stabilizes learning with group-normalized
advantages. Across ten benchmarks, AGENTFLOWwith a 7B-scale backbone
outperforms top-performing baselines with average accuracy gains of 14.9% on
search, 14.0% on agentic, 14.5% on mathematical, and 4.1% on scientific tasks,
even surpassing larger proprietary models like GPT-4o. Further analyses confirm
the benefits of in-the-flow optimization, showing improved planning, enhanced
tool-calling reliability, and positive scaling with model size and reasoning turns.
Bamboogle2Wiki
HotpotQA
Musique
GAIAAIME24AMC23
GameOf24
GPQA
MedQA
19.225.317.216.747.431.0
37.0
76.058.460.0
51.357.0
71.269.680.0
47.0
53.0
61.540.033.1+19.8%Math +15.9%Agentic
+7.0%Science +10.1%Search
2Wiki(Search)HotpotQA(Search)GAIA(Agentic)
AIME24(Math)GameOf24(Math)GPQA(Science)
AgentFlow(w/oFlow-GRPO)AgentFlow
Accuracy(%)Accuracy(%)
Qwen-2.5-7BGPT-4o (~200B)Search-R1 (7B) AgentFlow (7B)TIR(7B) ToRL(7B) AutoGen (7B) ReSearch (7B)
Figure 1:Left:Performance of AGENTFLOWwith a 7B-scale backbone before and after Flow-
GRPO tuning across ten diverse reasoning benchmarks. Flow-GRPO substantially improves per-
formance by enhancing planning quality and tool-calling reliability.Right:AGENTFLOWachieves
consistent gains over top baselines, including base LLMs, tool-integrated RL models, and training-
free agentic systems. All 7B results use Qwen2.5-7B-Base/Instruct as the backbone and tools.
*Equal contribution. ‚Ä†Co-senior authors. Work was partially done while ZL and HZ were visiting Stanford.
1
arXiv:2510.05592v1  [cs.AI]  7 Oct 20251 INTRODUCTION
Recent advances in large language models (LLMs) have unlocked remarkable reasoning capabilities,
largely driven by reinforcement learning (RL) from outcome-based feedback. By fine-tuning models
to maximize verifiable rewards, LLMs like DeepSeek-R1 (Guo et al., 2025) and SimpleRL (Zeng
et al., 2025b) have demonstrated sophisticated behaviors in self-correction and multi-step deduction.
A complementary line of work augments LLMs with external tools (e.g., web search, code exe-
cution) for knowledge retrieval and precise computation. Tool-integrated reasoning (TIR) extends
reinforcement learning with verifiable rewards to learnwhenandhowto call tools by interleav-
ing reasoning (e.g.,<think>) with tool invocations (e.g.,<tool call>) under full context (Jin
et al., 2025; Song et al., 2025; Chen et al., 2025; Feng et al., 2025). Early systems supported only
a single tool type, whereas recent work enables multi-tool settings by encoding tool metadata into
prompts (Dong et al., 2025; Qian et al., 2025a; Zhang et al., 2025). However, these methods still
train asingle, monolithic policy under multi-turn full-context reasoning, which introduces scaling
challenges: (i)trainingbecomes increasingly unstable as horizons lengthen, tool diversity grows,
and environments shift with tool feedback (Wang et al., 2025c; Mai et al., 2025; Moonshot AI, 2025;
Xue et al., 2025); and (ii)inference-time generalization remains brittle to unseen tasks or tools (Dong
et al., 2025; Hu et al., 2025b).
Agentic systems (Wu et al., 2024; Hong et al., 2024; Hu et al., 2025b) offer a promising alter-
native to monolithic tool-integrated reasoning models. They consist of multiple modules‚Äîoften
distinct LLMs with prescribed roles (e.g., planner, critic) or specialized components with dedicated
tools and capabilities (e.g., executor, coder)‚Äîthat coordinate via shared memory and inter-module
communication. By decomposing problems into sub-goals and iterating over multiple turns, these
systems can tackle tasks that demand diverse tools, long horizons, or multi-stage reasoning. How-
ever, achieving robust coordination in such systems ultimately requirestraining, since handcrafted
logic or static prompting cannot reliably capture when and how modules should collaborate, adapt to
evolving tool outputs, or recover from early mistakes. At the same time, they introduce newtraining
challenges: modules coordinate sequentially, outcome feedback propagates through long reasoning
chains, and state distributions shift with evolving tool outputs. As a result, most systems remain
training-free, relying on handcrafted logic or prompting heuristics. While some employ supervised
fine-tuning or preference optimization for key modules (Motwani et al., 2024; Park et al., 2025),
these off-policy approaches are decoupled from live dynamics and learn poorly from downstream
successes or failures. Thus, agentic systems struggle with sparse rewards, brittle adaptation, and
inefficient orchestration in dynamic environments.
To address the central challenge of learning long-horizon reasoning with sparse rewards in tool-
integrated agentic systems, we introduce AGENTFLOW, atrainableframework for effective plan-
ning and tool use (Figure 2). AGENTFLOWcomprises four specialized modules‚Äîplanner, executor,
verifier, and generator‚Äîthat interact iteratively over multiple turns via a shared evolving memory
and a toolset. The system operatesin the flow, with each turn cycling through planning, execu-
tion, and verification. Unlike prior agentic systems, AGENTFLOWdirectly optimizes its planner
on-policy,insidethe live multi-turn loop, allowing it to dynamically adapt to trajectories shaped by
tool calls, verifier signals, and memory updates. This evolving memory serves as a deterministic,
structured record of the reasoning process, enabling transparent state tracking, controllable behavior,
and bounded context growth.
To train the planner on-policy within this agentic system, we need to overcome the long-horizon
credit assignment problem inherent to sparse, trajectory-level rewards. We introduceFlow-based
Group Refined Policy Optimization(Flow-GRPO, Figure 4), an on-policy algorithm designed for
this setting. Flow-GRPO operates onin-the-flowrollouts, which capture the full trajectory of states,
actions, and tool events induced by the live system. Instead of attempting to assign credit with brit-
tle, intermediate heuristics, we assign a single, verifiable final-outcome reward to the entire trajec-
tory andbroadcastit to every turn. This design effectively transforms the multi-turn reinforcement
learning challenge into a series of single-turn updates: at each turn, the planner has access to the full
memory context and receives a consistent reward signal aligned with global success. This approach,
coupled with group-normalized advantages to stabilize training, enables robust credit assignment
and allows the planner to learn effective long-horizon strategies from sparse feedback.
We evaluate AGENTFLOWon ten benchmarks across diverse reasoning domains, as results high-
lighted in Figure 1. In our main setting, all four modules use Qwen2.5-7B-Instruct (Yang et al.,
2(a) AgentFlow: In-the-Flow Agentic System
Planner ùëé1 Executor Verifier
Planner ùëé2 Executor Verifier
Planner ùëéùëá Executor Verifier Generator
o
Query
...
Turn T
Turn 1
Turn 2
Answer
Toolkit Set ...
ùëû ùêæ ùëÄùë°
ùëéùë°
Input:
[Query Analysis]
[Global Goal]
[Required Skills]
Output:
[Current Sub-Goal]
[Selected Tool]
[Context for Tool Use]
Input:
[Current Sub-Goal]
[Selected Tool &
Context]
[Tool Metadata]
Output:
[Generated Command]
[Execution Result]
ùëíùë°
ùëéùë° ùêæ
ùë£ùë°
ùëû ùëíùë° ùëÄùë°
ùëÄùë°+1
Input:
[Generated Command]
[Execution Result]
Output:
[Execution Analysis]
[Memory Analysis]
[Verification Status]
Planner VerifierExecutor
(b) In-the-Flow Rollout at Turn t
Trained
Frozen
ùúãùúÉ Memory
Memory
Figure 2:(a)Overview of AGENTFLOW, a trainable agentic system for in-the-flow planning and tool
use. Four modules (planner, executor, verifier, generator) coordinate via a shared evolving memory
Mand toolsetK, given a queryq. The planner policy is optimized on-policyinsidethe system‚Äôs
multi-turn loop to enable adaptive, long-horizon reasoning.(b)A single state transition, showing
the actiona t, execution resulte t, and verifier signalv t that update the memory fromM t toM t+1.
2024a) as a backbone, with only the planner trained via Flow-GRPO. AGENTFLOWsubstan-
tially outperforms top-performing specialized tool-integrated reasoning models and agentic systems,
achieving average accuracy by 14.9% on knowledge-intensive search, 14.0% on broader agentic
tasks, 14.5% on mathematical reasoning, and 4.1% on scientific reasoning (¬ß4.2). Notably, our
7B-backbone system even surpasses the‚àº200B-parameter GPT-4o (Hurst et al., 2024) across all
domains. The trained planner learns to optimize planning, enhance tool-calling reliability, and dis-
cover effective solution pathways (¬ß4.3). Further analyses confirm that our in-the-flow optimization
with Flow-GRPO is crucial, far surpassing offline supervised tuning (¬ß4.4). Moreover, our training
approach proves highly efficient, leading to increased rewards and condensed responses compared to
traditional tool-integrated RL methods (¬ß4.5). Finally, we demonstrate that these benefits generalize,
with consistent gains from scaling backbone size and turn budget (¬ß4.6).
Our work makes three key contributions: (1) We present AGENTFLOW, a trainablein-the-flowagen-
tic system that directly optimizes its plannerinsidethe multi-turn loop. By coordinating specialized
modules through an evolving memory, it enables adaptive long-horizon planning and robust tool
orchestration. (2) We introduceFlow-GRPO, an on-policy, outcome-driven algorithm that hatcon-
vertsmulti-turn RL into a sequence of tractablesingle-turnpolicy updates bybroadcastinga sin-
gle, verifiable final-outcome reward to every turn. (3) Through comprehensive experiments on ten
benchmarks, we show that AGENTFLOWwith a 7B backbone outperforms specialized baselines and
even larger proprietary models. Further analyses reveal improved planning, enhanced tool-calling
reliability, and positive scaling with model size and turn budgets.
2 PRELIMINARY
Reinforcement learning for reasoning LLMs.Recent progress in reasoning LLMs has been sig-
nificantly driven by reinforcement learning from outcome feedback, using a verifiable reward sig-
nal (Shao et al., 2024; Yu et al., 2025). This paradigm fine-tunes a language model to maximize
an outcome-based reward while remaining close to a reference policy. Formally, the objective is to
optimize a policy LLMœÄ Œ∏ to generate a responseofor a given queryqfrom datasetD:
max
œÄŒ∏
Ex‚àºD, o‚àºœÄŒ∏(¬∑|q)

R(q, o)

‚àíŒ≤D KL(œÄŒ∏(o|q)‚à•œÄ ref(o|q)),(1)
whereR(q, o)is the outcome-based reward,œÄ ref is a reference model to prevent policy collapse, and
Œ≤controls KL regularization. Algorithms like Group Relative Policy Optimization (GRPO) (Shao
et al., 2024) implement this by sampling groups of responses, normalizing advantages by their re-
wards, and updating the policy with a clipped objective to encourage high-reward outputs.
Tool-integrated reasoning models (LLM agents).LLMs can be augmented with external tools
to access knowledge and perform precise computation under reinforcement learning with outcome-
based reward. As shown in Figure 3(a), the LLMinterleavesreasoning and tool calls, produc-
ing a chain of thought within<think></think>tokens followed by tool invocations (e.g.,
<tool call></tool call>). The resulting trajectoryœÑis a sequence of model generations
and tool observations:œÑ={s 1, a1, e1, . . . , sT , aT }, wheres t denotes the context,a t the generated
action (thought + tool call), andet the tool‚Äôs execution result. The policy modelœÄŒ∏ is then trained to
3