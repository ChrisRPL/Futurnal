# 2509.18133v3.pdf

Self-Evolving LLMs via Continual Instruction Tuning
Jiazheng Kang
Beijing University of Posts and
Telecommunications
Beijing, China
kjz@bupt.edu.cn
Le Huang
Beijing University of Posts and
Telecommunications
Beijing, China
lehuang@bupt.edu.cn
Cheng Hou
Tencent AI Lab
Beijing, China
chenghou@tencent.com
Zhe Zhao
Tencent AI Lab
Beijing, China
nlpzhezhao@tencent.com
ZhenXiang Yan
Tencent AI Lab
Beijing, China
kimmoyan@tencent.com
Chuan Shi
Beijing University of Posts and
Telecommunications
Beijing, China
shichuan@bupt.edu.cn
Ting Bai*
Beijing University of Posts and
Telecommunications
Beijing, China
baiting@bupt.edu.cn
Abstract
In real-world industrial scenarios, large language models (LLMs)
require Continuous Learning (CL) to adapt to diverse tasks as opera-
tional requirements diversify, demanding self-evolution capabilities
to autonomously refine their knowledge and adapt to dynamic envi-
ronments. However, existing CL approaches, such as replay-based
and parameter isolation techniques, struggle with the catastrophic
forgetting problem: new task training degrades performance on
prior tasks due to the model’s adaptation to new data distributions,
which weakens its generalization to old tasks. To address this issue,
we propose a novel parameter-efficient adversarial MoE framework,
MoE-CL, for industrial-scale self-evolving continual instruction
tuning of LLMs. Specifically, MoE-CL employs a dual-expert archi-
tecture to enable self-evolution: a dedicated LoRA expert for each
task to preserve task-specific knowledge, ensuring parameter inde-
pendence and mitigating forgetting, and a shared LoRA expert to
facilitate cross-task knowledge transfer. Specifically, a task-aware
discriminator within a Generative Adversarial Network (GAN) is
integrated into the shared expert to suppress task-irrelevant noise,
ensuring only task-aligned knowledge is transferred during se-
quential task training. Through adversarial training, the shared ex-
pert learns generalized representations that mimic the task-aware
discriminator, while dedicated experts retain task-specific details,
balancing knowledge retention and cross-task generalization—key
*Corresponding author.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
Conference’25, Washington, DC, USA
©2025 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 978-1-4503-XXXX-X/25/11
https://doi.org/XXXXXXX.XXXXXXX
to the model’s self-evolution by autonomously optimizing knowl-
edge integration across tasks. Extensive experiments on a public
MTL5 benchmark and an industrial Tencent3 benchmark validate
MoE-CL’s effectiveness in self-evolving continual learning. In real-
world A/B testing on content compliance review in the Tencent
Video Platform, MoE-CL reduced manual review costs by 15.3%,
demonstrating its applicability for large-scale industrial deploy-
ment where self-evolution is critical for adapting to evolving op-
erational demands. Implementation code is publicly available at
https://github.com/BAI-LAB/MoE-CL.
CCS Concepts
•Do Not Use This Code →Generate the Correct Terms for
Your Paper;Generate the Correct Terms for Your Paper; Generate
the Correct Terms for Your Paper; Generate the Correct Terms for
Your Paper.
Keywords
Self-Evolution, Continual Learning, Mixture of Experts, Large Lan-
guage Models
ACM Reference Format:
Jiazheng Kang, Le Huang, Cheng Hou, Zhe Zhao, ZhenXiang Yan, Chuan
Shi, and Ting Bai*. 2025. Self-Evolving LLMs via Continual Instruction Tun-
ing. In.ACM, COEX, SEOUL, KOREA, 9 pages. https://doi.org/XXXXXXX.
XXXXXXX
1 Introduction
In the era of large-scale industrial AI deployment, self-evolution—the
ability of large language models (LLMs) to autonomously refine
knowledge integration, adapt to dynamic task patterns, and retain
prior competencies without external intervention [7] has become
indispensable. Industrial scenarios demand LLMs to rapidly respond
to evolving operational demands across diverse real-world tasks:
for instance, Tencent’s content compliance ecosystem, central to
ensuring regulatory adherence and user safety, handles over two
arXiv:2509.18133v3  [cs.LG]  29 Sep 2025Conference’25, July 2025, Washington, DC, USA Kang, et al.
hundred thousand daily text reviews from fields like social plat-
forms, news media, and e-commerce. Each domain presents distinct
linguistic patterns, making self-evolution in continuous learning
critical—only through autonomous adaptation can LLMs sustain
performance across shifting tasks without constant human over-
sight. This operational complexity underscores the need for self-
evolving continuous learning (CL) frameworks [39, 44, 49], which
empower LLMs to seamlessly integrate new knowledge via iterative
tuning and adapt dynamically to evolving data distributions. How-
ever, self-evolution in continuous learning of LLMs is inherently
challenged by the catastrophic forgetting problem. This phenome-
non arises from the optimization dynamics of deep learning, where
parameter updates during new task training unintentionally disrupt
or overwrite the neural representations acquired from prior tasks.
As a result, the model suffers significant performance degradation
on previously mastered tasks, as the iterative fine-tuning for new
tasks undermines the stable knowledge encoding necessary for
maintaining competence in previous tasks—a critical barrier to self-
evolution, which requires autonomous retention of old knowledge
while integrating new insights.
Existing solutions in continuous learning of LLMs fail to fully
enable self-evolution, facing critical trade-offs between knowledge
retention and new task adaptation. For example, replay-based meth-
ods [11, 33, 35] preserve prior knowledge by reusing historical data
or generating pseudo-samples, but these approaches suffer from
data contamination: synthetic data often introduces noise that dis-
torts task-specific representations. Besides, the computational over-
head of storing and processing replay data renders them impractical
for large-scale industrial use. Regularization techniques [1, 16, 21]
constrain parameter updates to protect important weights, yet they
overly restrict the model’s ability to specialize in new tasks with
distinct requirements, limiting adaptability to operational demands
in industry scenarios. On the other hand, dynamic architecture ap-
proaches, like parameter isolation techniques [20, 26, 34], allocate
dedicated parameters to each task, effectively preventing inter-task
interference and retaining old task performance. However, such an
isolationist design limits cross-task knowledge transfer, failing to
leverage shared semantic patterns (e.g., common semantic features
across tasks with related content information). In real-world AI
deployments, this oversight limits the model’s ability to generalize
across domains and accumulate fine-tuning gains, leading to sub-
optimal scalability when handling diverse sequential tasks, which
thus constitutes a major impediment to self-evolution in LLMs that
rely on autonomous knowledge integration across tasks.
In our work, we introduceMoE-CL, a novel adversarialMixture
of LoRA Experts(MoE) architecture designed for self-evolving
Continuous Learning(CL) in large language models. Our core ob-
jective is to enable LLMs to autonomously transfer knowledge from
previously learned tasks to new ones during sequential training
while minimizing the impact of new task updates on old task per-
formance, a challenge that captures the essence of self-evolution of
continuous learning in large-scale industrial deployments of LLMs.
MoE-CL addresses catastrophic forgetting and enables useful knowl-
edge transfer through an adversarial LoRA expert architecture. By
allocating a dedicated LoRA expert to each task, MoE-CL ensures
that training a new task does not overwrite prior parameters, inher-
ently supporting autonomous knowledge retention. Concurrently,
the shared LoRA expert serves as a self-optimizing cross-task bridge,
learning generalized representations that capture common seman-
tic patterns across tasks. Specifically, the collaborative mechanism
between experts is enhanced via a Generative Adversarial Network
(GAN): a task-aware discriminator suppresses task-irrelevant noise
in the shared expert, ensuring only task-aligned knowledge is trans-
ferred, which enables the model to autonomously refine knowledge
integration without external guidance. During inference, MoE-CL
adaptively combines outputs from the shared LoRA expert and the
specific LoRA expert for the fine-tuning of the new task. MoE-CL
freezes the parameters of other tasks and only updates the param-
eters in the task-specific LoRA and the shared LoRA expert. By
doing so, it minimizes the computational overhead, which makes
it well-suited for self-evolving large-scale industrial systems. The
contributions of our paper are summarized as follows:
•We propose a novel adversarial mixture of LoRA experts ar-
chitecture (MoE-CL) for self-evolving continual instruction
tuning of LLMs. MoE-CL achieves self-evolution by main-
taining parameter independence through dedicated LoRA
experts and integrating common knowledge via a shared
LoRA expert, thus addressing the catastrophic forgetting
problem.
•We design a task-aware discriminator in a generative adver-
sarial network, which enhances the self-evolving capability
of MoE-CL. It enables the model to autonomously transfer
task-relevant knowledge while suppressing task-irrelevant
noise, thereby improving knowledge generalization in con-
tinual instruction tuning.
•Extensive experiments on the public MTL5 benchmark and
industrial Tencent3 benchmark demonstrate that MoE-CL
outperforms state-of-the-art baselines, with its self-evolving
ability validated by consistent performance across diverse
tasks. In particular, an offline A/B test on content compliance
review in the Tencent Video Platform shows a 15.3% improve-
ment in stripping rate, confirming its practical feasibility for
large-scale industrial deployments requiring dynamic self-
adaptation.
2 Related Work
Our MoE-CL architecture, a novel adversarial Mixture of LoRA
Experts framework, connects with four related work areas, i.e.,
Self-Evolution of LLMs, Continual Learning, Continual Instruction
Tuning, and Adversarial Learning with MoE. Our work addresses
catastrophic forgetting and enables effective knowledge transfer in
LLMs’ continual instruction tuning for self-evolution.
2.1 Self-Evolution of LLMs
Self-evolution of LLMs is defined as the ability of models to au-
tonomously adapt to dynamic tasks, integrate cross-task knowledge,
and sustain performance without heavy external intervention [7].
Existing approaches for enabling such capability generally fall into
three categories: Autonomous Learning Mechanisms, which en-
able self-improvement via self-generated supervision [45], internal
feedback [28], or rewarding signals [ 38]; Dynamic Architecture
Adaptation, which focuses on structural optimization, includingSelf-Evolving LLMs via Continual Instruction Tuning Conference’25, July 2025, Washington, DC, USA
modular design search [ 6], workflow generation [ 14], and fine-
tuing architecture design [22]; and Knowledge Integration Frame-
works, which consolidate cross-task knowledge through memory
management [46], tool evolution [25], or domain-specific toolset
creation [47]. Current research thus lacks a solution that balances
autonomous knowledge retention and adaptive transfer, which are
core challenges to self-evolution in continuous instruction tuning.
Our work MoE-CL, a Mixture of LoRA Experts architecture, fea-
tures dedicated experts for autonomous knowledge preservation, a
shared expert for cross-task integration, and a GAN-based discrim-
inator for noise suppression, achieving LLM self-evolution through
adaptive continual instruction tuning to adapt to sequential task
dynamics.
2.2 Continual Learning
Continual learning (CL), or termed as lifelong learning in large
language models, plays a critical role in overcoming the limitations
of traditional static-dataset training. The aim of continual learning
is to incrementally incorporate new knowledge, adapt to diverse
tasks across evolving domains, and retain previously acquired capa-
bilities throughout the learning process. By enabling incremental
learning across shifting domains and diverse tasks, continual learn-
ing ensures that LLMs not only adapt to emerging information
but also maintain foundational competencies, directly addressing
the critical challenge of "catastrophic forgetting" inherent in static
training and allowing them to remain relevant and effective in
dynamic real-world scenarios where knowledge and tasks evolve
continuously. The approaches in CL can be generally categorized
into three types [49] based on their knowledge integration mecha-
nisms: continual pre-training, continual fine-tuning, and external
knowledge integration (including retrieval-based and tool-based
methods). Continual pre-training enhances LLMs’ knowledge more
efficiently than full pre-training by incrementally incorporating
new data streams, enabling cross-domain generalization without
significant computational overhead [12, 19]. For example, ELLE [31]
uses function-preserved model expansion and pre-trained domain
prompts to continuously incorporate streaming data and enhance
performance. Continual fine-tuning adapts LLMs to specific tasks
while preserving prior expertise through techniques, such as con-
trastive ensemble distillation for text classification [18], incremental
knowledge transfer for named entity recognition [ 30], and con-
strained optimization to balance new task priorities [48]. External
knowledge integration in CL bridges gaps in LLMs’ internal rep-
resentations via retrieval-based methods like optimized document
retrieval [17] or tool-based approaches such as Chameleon [ 24],
which integrates web search and Python functions. These tech-
niques enhance real-time reasoning capabilities and extend model
utility beyond pre-trained knowledge of LLMs.
2.3 Continual Instruction Tuning
Continual instruction tuning [13, 32, 42] in LLMs refers to dynami-
cally adapting LLMs through sequential task-specific instruction
tuning, enabling them to incrementally incorporate new knowledge
from evolving instructions and adapt to diverse tasks while retain-
ing performance on previously learned tasks. This paradigm aims to
address the limitations of static training by facilitating iterative up-
dates aligned with new instructional inputs, thereby enhancing the
model’s flexibility in dynamic real-world scenarios. A critical chal-
lenge in continual instruction tuning is the catastrophic forgetting
problem, where parameter updates during new task training inad-
vertently disrupt or overwrite the neural representations acquired
from prior tasks. This leads to significant performance degradation
on previously learned tasks, as the model fails to preserve the stable
knowledge encoding necessary for previous tasks.
Existing approaches, such as replay-based, regularization, or
architecture-based techniques, struggle to balance knowledge re-
tention and new task adaptation. For example, the replay-based
method LAMOL [36] that uses generative replay to preserve past
knowledge through pseudo-samples. It suffers from data noise and
computational overhead. Regularization-based techniques such as
ARPER [29] stabilize parameter updates via adaptive regulariza-
tion. This type of method overly restricts model specialization,
Architecture-based methods like TPEM [8] dynamically adjust net-
work structures to retain task relevance. The isolated parameters
limit cross-task knowledge transfer. The SOTA continual instruc-
tion tuning method is MoCL [ 40], which equips each task with
a dedicated PEFT module and calculates the similarity between
the input and the task vector as weights to fuse the outputs of the
training task. However, it faces inherent limitations in balancing
task specificity with cross-domain generalization.
Different from the above instruction tuning methods, we pro-
pose MoE-CL, which integrates dedicated task-specific experts (to
preserve task-specific knowledge) and shared experts (to enable
controlled knowledge transfer), mitigating catastrophic forgetting
through adversarial training, alleviating task-irrelevant noise and
improving efficient cross-task generalization.
2.4 Adversarial Learning with MoE
Learning with Generative Adversarial Networks (GAN) is a pow-
erful technique that trains two models simultaneously: a genera-
tor that produces realistic outputs and a discriminator that distin-
guishes between real and generated data [3, 9, 10]. This adversarial
process enhances the generator’s ability to create outputs that
are indistinguishable from real data, making it highly effective for
tasks that require robustness and invariance. In recommendation
systems, adversarial learning has demonstrated effectiveness in
addressing biases [43] and enhancing generalization by separating
shared and task-specific features [2]. This ability to explicitly dis-
tinguish between generalizable knowledge and task-specific details
aligns with the core challenges of continual learning in large lan-
guage models, where there is a critical need to balance knowledge
retention and adaptation to new tasks. Building on the similar foun-
dation of disentangling generalizable and task-specific knowledge,
the Mixture-of-Experts (MoE) architecture has shown promise in
analogous contexts. MoE enables models to dynamically combine
specialized and shared knowledge, a strategy that has proven suc-
cessful in traditional multi-task learning [27, 37] and general LLM
fine-tuning [5, 23]. However, despite these advancements, the po-
tential of MoE in addressing the catastrophic forgetting problem in
LLMs’ continual learning has remained underexplored.