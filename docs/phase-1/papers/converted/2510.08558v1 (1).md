# 2510.08558v1 (1).pdf

Agent Learning via Early Experience
Kai Zhang1,3,◦,⋆,Xiangchao Chen 3,⋆,Bo Liu 2,⋆,Tianci Xue 3,⋆,Zeyi Liao 3,⋆,Zhihan Liu 1,⋆,Xiyao Wang 1,⋆,
Yuting Ning3,⋆,Zhaorun Chen 1,⋆,Xiaohan Fu 1,Jian Xie 3,Yuxuan Sun 3,Boyu Gou 3,Qi Qi 1,Zihang Meng 1,
Jianwei Yang1,Ning Zhang 1,Xian Li 2,Ashish Shah 1,Dat Huynh 1,Hengduo Li 1,Zi Yang 1,Sara Cao 1,
Lawrence Jang1,Shuyan Zhou 1,⋆,Jiacheng Zhu 1,⋆,Huan Sun 3,⋆,Jason Weston 2,⋆,Yu Su 3,†,Yifan Wu 1,†
1Meta Superintelligence Labs,2FAIR at Meta,3The Ohio State University
⋆Core Contributors, ◦Work done at Meta,†Joint Last Author
A long-term goal of language agents is to learn and improve through their own experience, ultimately
outperforming humans in complex, real-world tasks. However, training agents from experience data
with reinforcement learning remains difficult in many environments, which either lack verifiable rewards
(e.g., websites) or require inefficient long-horizon rollouts (e.g., multi-turn tool use). As a result,
most current agents rely on supervised fine-tuning on expert data, which is challenging to scale and
generalizes poorly. This limitation stems from the nature of expert demonstrations: they capture only
a narrow range of scenarios, and expose the agent to limited environment diversity. We address this
limitation with a middle-ground paradigm we callearly experience: interaction data generated by
the agent’s own actions, where the resulting future states serve as supervision without reward signals.
Within this paradigm we study two strategies of using such data: (1) Implicit world modeling, which
uses collected states to ground the policy in environment dynamics; and (2) Self-reflection, where
the agent learns from its suboptimal actions to improve reasoning and decision-making. We evaluate
across eight diverse environments and multiple model families. Our approaches consistently improve
effectiveness and out-of-domain generalization, highlighting the value of early experience. Moreover,
in environments with verifiable rewards, our results provide promising signals that early experience
offers a strong foundation for subsequent reinforcement learning, positioning it as a practical bridge
between imitation learning and fully experience-driven agents.
Date:October 10, 2025
Correspondence:Kai Zhang (zhang.13253@osu.edu)
What if I do... 
Ah I see ...
Scalable Data
Era of Experience (RL)
Scalable Data
No Reward No Reward Requires Reward
Era of Human Data (IL)
You should do
…
So many steps… 
only reward at 
last!
Scalable Data
Early Experience (Ours)
What if I do this 
step instead?
Scalable DataScalable Data
Reward-Free Reward-Free Requires Reward
You should do
these steps.
Try many steps… 
only a reward at 
last!
Scalable Data
Era of ExperienceEra of Human Data
(Imitation Learning) (Reinforcement Learning)
Early Experience
(Ours)
Figure 1Progression of training paradigms for language agents.Left: TheEra of Human Datarelies on expert
demonstrations, where supervision comes from human-/expert-curated actions; it is reward-free (i.e., does not require
the environment to provide verifiable reward) but not data-scalable.Right: The envisionedEra of Experiencebuilds
upon environments with verifiable rewards, using them as the primary supervision for reinforcement learning; however,
many environments either lack such rewards (Xue et al., 2025) or require inefficient long-horizon rollouts (Xie et al.,
2024a).Center: OurEarly Experienceparadigm enables agents to propose actions and collect the resulting future
states, using them as a scalable and reward-free source of supervision.
1
arXiv:2510.08558v1  [cs.AI]  9 Oct 20251 Introduction
Autonomous agents (Russell and Norvig, 1995; Franklin and Graesser, 1997) have long been a central goal of
artificial intelligence, aiming to perceive, act, and learn in complex environments to accomplish goals without
human intervention. This vision is becoming increasingly realistic with the emergence of language agents (Su
et al., 2024; Sumers et al., 2024), which are built on top of large language models (LLMs; OpenAI (2024)).
Powered by knowledge obtained from large-scale pretraining and the flexibility of the language interface,
language agents are now being applied across a wide range of environments. They can navigate websites and
mobile applications (Zheng et al., 2024a; Deng et al., 2023; Zhou et al., 2024; Trivedi et al., 2024), control
diverse tools (Xie et al., 2024a; Gu et al., 2024), and assist in scientific research (Chen et al., 2025; Lou et al.,
2025), showing strong potential as a foundation for the next generation of intelligent systems.
To build such language agents, one promising solution is reinforcement learning (RL), where agents are trained
by optimizing for expected cumulative reward returned by the environment. This paradigm has enabled
traditional agents such as AlphaGo (Silver et al., 2016) to achieve superhuman performance in domains with
well-defined environments and reward structures, such as Atari games (Bellemare et al., 2013) and the game
of Go, echoing the vision of an emergingera of experience(Silver and Sutton, 2025) for language agents.
However, applying RL to real-world language agents remains highly challenging now. Many environments
of interest lack verifiable or dense reward signals, especially in open-ended settings such as websites where
platforms do not expose ground truth feedback. For example, a form may appear to be submitted successfully,
but the agent receives no indication of whether each piece of information was filled out correctly. In addition,
tasks in multi-turn tool-use environments often involve long interaction sequences (Xie et al., 2024a; Jin et al.,
2025) with delayed or ambiguous outcomes, making credit assignment and training inefficient and unstable.
As a workaround, most current language agents are instead trained on expert-curated data with supervised
fine-tuning (SFT; Deng et al. (2023); Pahuja et al. (2025); Prabhakar et al. (2025)). This paradigm bypasses
the need for reward signals by learning from human demonstrations, where agents map states to actions
using static datasets. While SFT is straightforward and efficient to train, it has inherent limitations. The
agent under this paradigm does not interact with the environment during training; it does not observe the
outcomes of its own actions. This restricts its ability to learn from failure, refine its decision-making, or
generalize to unseen situations (Chu et al., 2025). Furthermore, this approach assumes the data are expert
or near-optimal, yet scaling high-quality human demonstrations is expensive and difficult to sustain. More
critically, it locks the agent into a passive role, bound by the imagination and coverage of its training data
rather than actively learning from its own experience. Given these limitations and that reliable reward signals
are often unavailable aforementioned,how can we train agents to grow from their own experience,without any
external reward signals?
Motivated by these limitations, we introduce theearly experienceparadigm, a middle ground between
imitation learning and reinforcement learning, as shown in Figure 1. In this setting, agents learn not only
from human-curated data but also from future states driven by their own proposed actions in the environment.
These future states are the agent’s own experience, and can be transformed into supervision signals that
enable it to grow directly from the consequences of its actions without relying on external reward signals. We
explore two strategies to transform these future states as supervision: (1)Implicit World Modeling: using the
collected future states to help the agent build internal representations of environment dynamics, allowing it to
better understand the environment by predicting the future states. (2)Self-Reflection: guiding the agent
to compare its behavior with expert demonstrations, identify suboptimal decisions, and extract lessons to
improve future decision-making. Both strategies share the same principle: in the absence of external rewards,
the agent’s own actions and the resulting future states can still constitute experience that serves as a direct
source of supervision. By turning future states generated from its own actions into learning signals, the
language agent can continually improve without relying on additional human data or external rewards.
We comprehensively evaluate early experience across eight diverse environments, spanning embodied navigation,
web navigation, multi-turn tool-use, long-horizon planning, and multi-domain API tasks, using multiple base
architectures. Across all settings, both methods consistently outperform purely imitation learning baselines,
with average absolute gains of+9.6in success rate and+9.4in out-of-domain generalization. Moreover,
in environments where verifiable rewards are available, initializing RL with checkpoints trained with early
2experience methods leads to substantially stronger performance compared to standard imitation-learning
warm starts, improving final success rates by up to+6.4. This shows that the performance gain from early
experience stage can carry over to the final model’s performance after RL. Beyond these empirical gains, our
analysis shows that early experience enables capabilities unattainable through imitation learning alone. It
scales effectively, achieving comparable or superior performance with only half or even less of the expert data.
The paradigm applies seamlessly to larger models, preserving its effectiveness across scales. These results
show that early experience is not merely an alternative to imitation learning, but a practical and scalable
bridge to reinforcement learning, delivering both immediate gains in effectiveness and long-term benefits for
era of experiencetraining regimes.
Our contributions are summarized as follows:(1)We advocate and formalize theearly experienceparadigm as
a practical and scalable bridge between imitation learning and reinforcement learning for building autonomous
language agents. It empowers agents to convert their own experience into learning signals without relying
on external rewards and can be seamlessly integrated into existing training pipelines.(2)We propose and
systematically study two training strategies under this paradigm: implicit world modeling, which enhances
decision-making by modeling environment dynamics directly from collected experience, and self-reflection,
which distills fine-grained lessons from the agent’s own actions.(3)We conduct a comprehensive evaluation
across eight diverse environments and multiple model families. Our methods consistently improve task
effectiveness, out-of-domain generalization, and downstream reinforcement learning performance, achieving
state-of-the-art results on several benchmarks and offering actionable insights through detailed analysis.
2 Related Work
2.1 Training Paradigms for Language Agents
Supervised Fine Tuning (SFT).Most language agents (Yao et al., 2022; Deng et al., 2023; Hong et al., 2024;
Furuta et al., 2024; Pahuja et al., 2025) are trained with SFT, also known as imitation learning or behavior
cloning in the RL literature, on expert trajectories, especially in complex settings such as the web (Zhou
et al., 2024) or operating systems (Xie et al., 2024b). These trajectories may be human-annotated (Yao et al.,
2022; Deng et al., 2023) or synthesized by stronger language models that follow carefully human-designed
workflows (Murty et al., 2024; Pahuja et al., 2025). Although synthetic demonstrations increase coverage,
they offer only incremental gains because the underlying supervision signal is still static. SFT thus provides
dense, reward-free supervision signals but remains limited by the cost of high-quality demonstrations (Qi
et al., 2025) and leaves agents brittle when they confront novel states (Chu et al., 2025; Deng et al., 2023).
Reinforcement Learning (RL).RL trains agents through trial and error, optimizing for long-term rewards (Sutton
et al., 1998). Although it has achieved impressive results in control, board games, and Atari (Mnih et al.,
2013; Silver et al., 2016; Hafner et al., 2020; Schrittwieser et al., 2020), RL remains difficult to apply effectively
in language agent settings (Wang et al., 2025; Qi et al., 2025; Wei et al., 2025a; Feng et al., 2025; Zhou et al.,
2025b; Jin et al., 2025; Zhou et al., 2025a). Current studies are still exploratory: many rely on approximate
rewards produced by larger teacher models (Qi et al., 2025; Zhou et al., 2025b), or on carefully curated reward
functions (Qian et al., 2025) and hand-tuned training recipes (Jin et al., 2025) to maintain stability. The
supporting infrastructure is also underdeveloped; most real-world language agent environments lack reliable
simulators, standard reset mechanisms, and scalable evaluation platforms (Wang et al., 2025; Feng et al.,
2025), making large-scale RL training for language agents costly and brittle. Together, these limitations
suggest that scalable RL for language agents is not yet mature, motivating a paradigm that bridges current
imitation-based training and future fully experience-driven learning (RL).
2.2 Supervision from Exploration
Traditional exploration–exploitation strategies in RL collect trajectories that are later refined through reward
feedback. Methods like Hindsight Experience Replay (Andrychowicz et al., 2017) densify sparse rewards
by retrofitting achieved outcomes as goals, but still require verifiable reward functions unavailable in many
language agent environments. Our setting uses exploration differently: interaction traces become direct
supervision signals, eliminating the need for rewards or manual relabeling entirely.
3