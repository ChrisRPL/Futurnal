# 2504.11544v1.pdf

NodeRAG:
Structuring Graph-based RAG with Heterogeneous Nodes
Tianyang Xu1, Haojie Zheng2, Chengze Li1, Haoxiang Chen1
Yixin Liu3, Ruoxi Chen, Lichao Sun3
1 Columbia University, 2 University of Pennsylvania, 3 Lehigh University
tx2240@columbia.edu, haojiez@seas.upenn.edu
Abstract
Retrieval-augmented generation (RAG) em-
powers large language models to access ex-
ternal and private corpus, enabling factually
consistent responses in specific domains. By
exploiting the inherent structure of the cor-
pus, graph-based RAG methods further enrich
this process by building a knowledge graph
index and leveraging the structural nature of
graphs. However, current graph-based RAG ap-
proaches seldom prioritize the design of graph
structures. Inadequately designed graph not
only impede the seamless integration of di-
verse graph algorithms but also result in work-
flow inconsistencies and degraded performance.
To further unleash the potential of graph for
RAG, we propose NodeRAG, a graph-centric
framework introducing heterogeneous graph
structures that enable the seamless and holis-
tic integration of graph-based methodologies
into the RAG workflow. By aligning closely
with the capabilities of LLMs, this frame-
work ensures a fully cohesive and efficient
end-to-end process. Through extensive experi-
ments, we demonstrate that NodeRAG exhibits
performance advantages over previous meth-
ods, including GraphRAG and LightRAG, not
only in indexing time, query time, and stor-
age efficiency but also in delivering superior
question-answering performance on multi-hop
benchmarks and open-ended head-to-head eval-
uations with minimal retrieval tokens. Our
GitHub repository could be seen at this link.
1 Introduction
Retrieval-augmented generation (RAG) has
emerged as a solution to the challenges posed
by the rapid evolution of real-world knowledge
domains (Fan et al., 2024), coupling large lan-
guage models (LLMs) with an external retrieval
mechanism to ensure the generation of factually
consistent and contextually relevant information
(Tonmoy et al., 2024; Shrestha et al., 2024; Liu
et al., 2024). Despite recent progress, current
RAG methods face notable shortcomings in
handling multi-hop reasoning (Luo et al., 2023;
Wang et al., 2024b) and summary-level queries
(Han et al., 2024a; Wen et al., 2023) due to
their insufficient utilization of data structures
and lack of high-level understanding of the text
corpus. Graph-based RAG methods (Tian et al.,
2024; Park et al., 2023) have been proposed
to enhance retrieval and question-answering
performance, specifically addressing the two main
challenges faced by traditional RAG approaches.
Leveraging LLMs to decompose raw data into
graph structures (Jiménez Gutiérrez et al., 2024;
He et al., 2024) for utilizing structural information,
as well as employing LLMs for summary-based
enhancements (Edge et al., 2024; Guo et al., 2024)
to derive insights beyond the original text, has
gradually become mainstream approaches.
However, previous Graph-based RAG works
(Trajanoska et al., 2023; Jiménez Gutiérrez et al.,
2024) have rarely considered the critical role of
graph structures, i.e., what forms of graph better
support RAG. Among existing approaches, knowl-
edge graphs (Sanmartin, 2024; Wang et al., 2024b)
extract triples, with the graph containing only struc-
tural information, yet retrieval context remains con-
fined to text chunks, which often lack semantic co-
herence and include unrelated information. While
current methods attempt to incorporate more infor-
mation into the graph and extract deeper insights,
they suffer from inefficiencies and inconsistencies
due to inadequately designed structures. For in-
stance, as illustrated in Figure 1, GraphRAG (Edge
et al., 2024), adopt a tightly coupled entity-event
homogeneous structure, hindering the integration
of original context and summary information into
the graph. This results in inconsistencies in re-
trieval methods (separating local and global re-
trieval) and leads to coarse-grained retrieval, where
retrieving an entity indiscriminately includes all
1
arXiv:2504.11544v1  [cs.AI]  15 Apr 2025Community
NaïveRAGHippoRAGGraphRAGHomogenous GraphNodeRAGKnowledge Triples GraphHeterogenous GraphTextLight RAGREntity Information Original Text ChunksTextN Summarization Information
Node 
SRelationshipR……RRRRRHomogenous Graph
Edge 
Community Summary
Text Chunk 1 RNNS
Node 
NNNNNNNNSSSRText
Edge 
Community
Text Chunk 2 
Text Chunk 3 
Text Chunk n 
Text Chunk n 
Text Chunk 1 
Node 
NS
Node 
Node 
Edge 
RNSSSNRNSSNSSNSSS
Figure 1: Comparsions between NodeRAG and other RAG systems. NaïveRAG retrieving fragmented text chunks, leads to
redundant information. HippoRAG introduces knowledge graphs but lacks high-level summarization. GraphRAG retrieves
community summaries but may still produce coarse-grained information. LightRAG incorporates one-hop neighbors but retrieves
redundant nodes. In contrast, NodeRAG utilizes multiple node types, including high-level elements, semantic units, and
relationships, enabling more precise, hierarchical retrieval while reducing irrelevant information.
associated events, adding irrelevant information.
To address these limitations, we propose
NodeRAG, which is built around a well-designed
Heterogeneous Graph, comprehensively consider-
ing the entire process of graph indexing and search-
ing, enabling fine-grained retrieval. The hetero-
graph adheres to the principle of unfolding and
flattening, decomposing different types of infor-
mation to construct a heterogeneous fully nodal-
ized graph where nodes serve distinct functions
and roles. This means that entities, relationships,
original text chunks, independently decomposed
events from text chunks, and summaries extracted
by LLMs are all represented as nodes within the
graph. The heterograph not only encapsulates infor-
mation from the original corpus but also extends be-
yond it, incorporating enriched insights such as key
node attributes, and high-level discoveries. Each
node in heterograph consists of unstructured con-
tent, while preserving structural connections be-
tween nodes, striking a balance between structural
integrity and flexibility. As illustrated in Figure 1,
for a multi-hop question, NodeRAG can retrieve a
semantically coherent, independent event (seman-
tic unit) and high-level discoveries (high-level ele-
ments) related to key entities such as Harry, Neville,
and the three-headed dog using graph algorithms,
providing explainable and fine-grained retrievals as
well as high-level understanding.
The key contributions of our work can be sum-
marized in three main aspects.
(1) Better Graph Structure for RAG The graph
structure serves as the foundation for graph-based
RAG where significance has been overlooked. Our
work emphasizes its importance and introduces a
graph structure that better supports RAG.
(2) Fine-grained and Explainable Retrieval The
heterograph enables fine-grained and functionally
distinct nodes, allowing graph algorithms to effec-
tively and reasonably identify key multi-hop nodes.
This leads to more relevant retrieval with minimal
retrieval context, enhancing both precision and in-
teroperability.
(3) Unified-Level Information Retrieval Decom-
posed information from documents and extracted
insights from LLMs are not treated as separate
layer but are instead unified as nodes within the
heterograph. This integration allows for a cohesive
framework capable of handling information needs
across different levels.
In addition, extensive experiments demonstrate
that NodeRAG not only outperforms previous
graph-based RAG methods on multi-hop tasks but
also exhibits superior performance in open-ended
head-to-head evaluations. With minimal retrieval
tokens, it achieves highly precise retrieval while
also demonstrating system-level efficiency advan-
tages, including improvements in indexing time,
query time, and storage efficiency, as shown in
appendix A.
22 NodeRAG
The NodeRAG pipeline is built on a foundational
graph structure defined as the heterograph, which
will be introduced in Section 2.1. The workflow
is divided into two primary stages, graph indexing
and graph searching. Graph indexing comprises
three components, graph decomposition, graph aug-
mentation, and graph enrichment, which are dis-
cussed in Sections 2.2, 2.3, and 2.4, respectively.
This stage integrates various types of nodes and
edges into the heterograph by leveraging LLMs
and graph algorithms. The subsequent stage, graph
searching, is detailed in Section 2.5 and combines
the structural advantages of the heterograph with
graph algorithms to efficiently retrieve relevant in-
formation. Moreover, the fundamental concepts
and implementation details of the graph algorithms
used in the pipeline are provided in Appendix C,
while the prompting instructions for LLMs can be
found in Appendix E for reference.
2.1 Heterograph
The concept of the heterograph embodies the prin-
ciple of comprehensive unfolding and flattening of
information into a fully nodalized structure. This
structure achieves its granularity through the in-
tegration of seven hetero node types: entity ( N),
relationship (R), semantic unit (S), attribute (A),
high-level elements (H), high-level overview (O),
and text (T). Each node type is tailored to represent
specific roles and characteristics of the information,
enabling a fine-grained and functional decompo-
sition of data. Mathematically, the heterograph is
defined as:
G = (V, E, Ψ),
where G is the heterograph, V represents the set of
nodes, E is the set of edges, and Ψ : V →Types is
a mapping function that assigns each nodev ∈ Vto
a specific type. The set of node types, corresponds
to the seven predefined types:
Types = {N, R, S, A, H, O, T}.
For any node v, Ψ(v) defines its type, with each
node type performing a distinct and well-defined
function, as detailed in subsequent sections and
appendix C. For each e ∈ E, the default weight
of e is set to 1, representing a basic connection
between two nodes. Furthermore, we define Vtypes
as the subset of nodes corresponding to a subset set
types ⊆ Types, formally expressed as:
Vtypes = {v ∈ V |Ψ(v) ∈ types}.
For instance, V{N,R,S} represents the subset con-
taining only entity, relationship, and semantic unit
nodes.
V{T,S,A,H } contain rich informational content
and are classified as retrievable nodes. In contrast,
V{N,O}, which represent names or titles, act solely
as critical linkage and entry points within the graph
but are not directly retrievable. For example, VH
provides detailed context for high-level concepts,
while VO represents the corresponding title and
keywords but does not contribute directly to the
retrieved content. Additionally, VR, is a nodalized
edge, acting as connector nodes and secondary re-
trievable nodes, contributing to the retrieval context
but not serving as graph entry points.
2.2 Graph Decomposition
First, we define a null heterograph G0. The initial
step involves employing a LLM to decompose text
chunks from the source corpus into three primary
node types: semantic units ( S), entities (N), and
relationships (R). These nodes are then intercon-
nected to construct the initial heterograph. This
process can be formalized as:
G1 = G0∪{v ∈ V, ed, er ∈ E |Ψ(v) ∈ {S, N, R}},
Where e represents the connecting edges between
semantic units and entity nodes, as well as between
relationship nodes and their corresponding source
and target entities. For instance, if “Hinton was
awarded the Nobel Prize for inventing backprop-
agation” serves as v ∈ VS derived from a text
chunk, then Hinton, Nobel Prize, and backpropaga-
tion represent v ∈ VN nodes, with ed denoting their
connections to v ∈ VS. An example of v ∈ VR
would be “Hinton received Nobel Prize”, where
er represents the edge connecting the source node
Hinton to the target node Nobel Prize.
Semantic unit (S) The semantic unit acts as a
local summary, representing an independent event
unit in a paraphrased form. It serves as the core
node for graph augmentation and improving search
quality. Since the division of text chunks is not
based on semantics, unrelated or unassociated con-
tent may coexist within a single chunk. This con-
text noise increases entropy, leading to degraded
quality when using text chunks for graph augmen-
tation or searching due to their coarse granularity
and irrelevant information.
Entity (N) and Relationship (R) Entities (N)
are nodes that exclusively represent entity names,
3