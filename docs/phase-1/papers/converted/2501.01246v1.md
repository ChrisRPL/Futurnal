# 2501.01246v1.pdf

Large Language Model-Enhanced Symbolic Reasoning for Knowledge Base
Completion
Qiyuan He1 Jianfei Yu2 Wenya Wang1
1College of Computing and Data Science, Nanyang Technological University
2School of Computer Science and Engineering, Nanjing University of Science and Technology
{qiyuan001,wenya}@ntu.edu.sg jfyu@njust.edu.cn
Abstract
Integrating large language models (LLMs) with
rule-based reasoning offers a powerful solu-
tion for improving the flexibility and reliability
of Knowledge Base Completion (KBC). Tradi-
tional rule-based KBC methods offer verifiable
reasoning yet lack flexibility, while LLMs pro-
vide strong semantic understanding yet suffer
from hallucinations. With the aim of combin-
ing LLMs’ understanding capability with the
logical and rigor of rule-based approaches, we
propose a novel framework consisting of a Sub-
graph Extractor, an LLM Proposer, and a Rule
Reasoner. The Subgraph Extractor first sam-
ples subgraphs from the KB. Then, the LLM
uses these subgraphs to propose diverse and
meaningful rules that are helpful for inferring
missing facts. To effectively avoid hallucina-
tion in LLMs’ generations, these proposed rules
are further refined by a Rule Reasoner to pin-
point the most significant rules in the KB for
Knowledge Base Completion. Our approach
offers several key benefits: the utilization of
LLMs to enhance the richness and diversity
of the proposed rules and the integration with
rule-based reasoning to improve reliability. Our
method also demonstrates strong performance
across diverse KB datasets, highlighting the ro-
bustness and generalizability of the proposed
framework.1
1 Introduction
Knowledge bases (KBs) are repositories of struc-
tured information that serve foundational roles in a
wide range of machine learning applications, such
as question-answering, recommendation systems,
and semantic search (Wang et al., 2017). Despite
their compact and large volume of information stor-
age, KBs are often incomplete, leading to signifi-
cant gaps in knowledge representation. To tackle
1This work will be submitted to the IEEE for possible
publication. Copyright may be transferred without notice,
after which this version may no longer be accessible.
LLM-enhanced Symbolic Reasoning
Propose
Extract
Reason
country of origincountry
country of citizenship
Rule Learning
Embedding
Rule Mining
known entitiesnew entities
occupationX film producer
occupationX film director
Generalizability? 
Semantic 
Understanding? 
Flexibility? 
Figure 1: LeSR: LLM-enhanced Symbolic Reasoning
for KB Completion, aiming for flexibility, semantic
understanding and generalizability.
this challenge, the task of Knowledge Base Com-
pletion (KBC) has attracted considerable attention
in the research community, aiming to automatically
infer missing entities within KBs (Socher et al.,
2013).
Existing KBC methods are mostly embedding-
based and logic rule-based. Earlier research fo-
cuses on embedding-based methods that learn to
encode the semantics of entities and relations as
vectors, enabling efficient inference through vec-
tor operations (Bordes et al., 2013; Yang et al.,
2015; Trouillon et al., 2016; Balazevic et al., 2019).
These methods have gained popularity due to their
effectiveness in capturing latent patterns within
large-scale KBs. On the other hand, logic rule-
based methods (Khot et al., 2011; Rocktäschel and
Riedel, 2017; Yang et al., 2017; Qu et al., 2021)
offer an interpretable approach to KBC, contrast-
ing with the often opaque nature of embedding-
based methods (Bordes et al., 2013; Yang et al.,
2015; Trouillon et al., 2016; Balazevic et al., 2019).
They leverage logic rules such as parent(A, B) ∧
parent(B, C) ⇒ grandparent(A, C), which
reads as if A is B’s parent and B is C’s parent,
then A is C’s grandparent, to infer missing facts
based on existing KB.
Nevertheless, most existing KBC methods still
suffer from several limitations. Embedding-based
methods, while efficient, often lack interpretabil-
ity and fail to handle new entities unseen during
1
arXiv:2501.01246v1  [cs.CL]  2 Jan 2025training. Although rule-based methods excel in
providing transparent and verifiable reasoning, ei-
ther through pattern mining (Galárraga et al., 2013;
Wang and Li, 2015; Meilicke et al., 2019) or neural
modeling (Rocktäschel and Riedel, 2017; Xiong
et al., 2017; Minervini et al., 2020), they struggle
to identify quality and diverse rules. The lack of
flexibility and diversity limits the effectiveness of
these rule-based approaches due to the increasing
complexity and scale of modern KBs (Zhou et al.,
2023).
To tackle the aforementioned limiations, we con-
sider to leverage large language models in the rule
mining process and integrate the power of LLMs
into the symbolic reasoning process of KB com-
pletion. With vast linguistic knolwedge captured
from large-scale pre-training, LLMs have been
used for KBC by framing it as a sequence gen-
eration problem (Yao et al., 2024) where LLMs
are used to directly infer missing entities or rela-
tionships (Yao et al., 2024) given a query such as
“What is the capital of France?”. However, treating
KBC as a generation task with LLM backbones
has raised concerns regarding transparency and ac-
curacy. Namely, these approaches heavily rely on
the inherent abilities of LLMs, lacking clarity in
the internal reasoning processes. Moreover, LLMs
are prone to hallucinations and errors and tend to
perform poorly without extensive fine-tuning, espe-
cially on domain-specific knowledge (Veseli et al.,
2023; Zhang et al., 2024; He et al., 2024).
We propose a novel framework, LeSR (LLM-
enhanced Symbolic Reasoning), which synergizes
the comprehensive understanding capabilities of
LLMs and the rigorousness of rule-based systems.
LeSR consists of a Subgraph Extractor, an LLM
Proposer and a Rule Reasoner, designed to enhance
the relevancy and diversity of logic rules and maxi-
mize the effectiveness and reliability of the knowl-
edge inference process. For each relation in the KB,
the Subgraph Extractor is responsible for identify-
ing meaningful subgraphs surrounding the relation,
which will be further fed into the LLM Proposer
to generate diverse and relevant logic rules. The
comprehensive power of LLMs contributes to iden-
tifying a wide range of entity-agnostic logic rules,
uncovering common patterns behind the extracted
subgraphs, but meanwhile, it also brings in un-
expected noise, which is harmful to knowledge
completion. We thus introduce a Rule Reasoner
to refine LLM proposals by learning to score each
rule, improving reliability and reducing the halluci-
nation inherent in the LLM Proposer.
The contributions of our work are threefold: (1)
We introduce a novel paradigm leveraging LLMs
to propose logic rules that are relation-specific and
sensitive to subgraph structures. The proposed
rules demonstrate sufficient diversity and coverage.
(2) We propose a novel framework that effectively
integrates LLMs with logic rule reasoning. Our
framework combines the power of language un-
derstanding and rigorous reasoning, leading to a
transparent and more reliable inference process, ef-
fectively mitigating the errors produced by LLMs
alone. (3) We conduct extensive experiments over
five knowledge base benchmarks, covering diverse
domains and complexities. Our method achieves
comparable results across all datasets and produces
adequate interpretable rules, demonstrating its ef-
fectiveness as a robust and generalizable solution
for KBC.
2 Related Work
Knowledge Base Completion (KBC), sometimes
known as link prediction, is the task of filling in
missing information in knowledge bases (KBs)
based on existing data. Existing KBC methods can
be broadly and loosely categorized as embedding-
based or rule-based strategies.
Embedding-based KBC Most embedding-based
methods represent entities and relations by vectors
with their semantics preserved in the embedding
space (Sun et al., 2019; Zhang et al., 2022; Bordes
et al., 2013; Trouillon et al., 2016; Dettmers et al.,
2018; Balazevic et al., 2019). They are black-box
in nature and thus lack interpretability. In addi-
tion, they heavily rely on data of good quality to
excel (Nickel et al., 2011). Furthermore, these
embedding-based methods implicitly follow the
closed world assumption (Qu et al., 2021; Paul-
heim, 2017) that all facts not present in the knowl-
edge base dataset are false. However, under real-
world scenarios, knowledge bases tend to be in-
herently incomplete and follow the open-world as-
sumption, i.e. the absence of a fact implies uncer-
tainty instead of falsehood. Because closed-world
assumption does not account for the possibility of
the unknown, KBC methods based on these meth-
ods suffer from incomplete or evolving KBs.
Some works use graph neural networks (GNN)
to solve KBC. In general, GNN-based approaches
learn embeddings for both entities (nodes) and re-
lationships (edges) in the KB and use message-
2Q: (Happy, performer, ?)
A: Pharrell Williams
Q: (Dr. John Watson, performer, ?)
A: Jude Law
KB Completion
performer
Subgraph Extractor LLM Proposer
 Rule Reasoner
extract
 context
 solve
generate
 refine select
performer
Relevant Subgraphs
Shine a Light
The Rolling Stone
performer Ronnie Wood
member of
cast member
performer
IF (A, present in work, B) AND (B, based on, C) AND (C, performer, D) THEN (A, performer, D);
IF (A, present in work, B) AND (B, cast member, C) THEN (A, performer, C); ......
Proposed Logical Rules
Figure 2: An overview of LeSR: LLM-enhanced Symbolic Reasoning. The Subgraph Extractor samples relevant
subgraphs from the KB, then the LLM uses these subgraphs to propose logical rules, which will be further refined
by the Rule Reasoner, learning the significance of the proposed rules and performing KB completion.
passing to aggregate information from neighbour-
ing nodes and edges (Schlichtkrull et al., 2018).
However, GNNs may quickly find and optimize
for existing link existence information in the train-
ing data, potentially leading to overfitting and poor
generalizability (Zhang and Chen, 2018).
Rule-based KBC Rule-based methods assume
relationships between entities, and relations can
be explicitly expressed as logical rules or patterns
(Nickel et al., 2016). Early representative meth-
ods include rule mining (Meilicke et al., 2019),
markov logic networks (Khot et al., 2011), rela-
tional networks (Natarajan et al., 2010), neuro sym-
bolic models (Yang and Song, 2020; Sadeghian
et al., 2019; Qu et al., 2021) and neural theorem
provers (Minervini et al., 2020; Rocktäschel and
Riedel, 2017). While offering verifiable reasoning,
they rely on extensive searching, mining, and appli-
cation of rules across KBs that are computationally
intensive (Zeng et al., 2023). In addition, the rules
may be tailored to specific relationships or patterns
in the KB, limiting their ability to generalize to new
or unseen scenarios (Wu et al., 2023).
Another paradigm adopts reinforcement learning
to learn rules (Das et al., 2018; Lin et al., 2018).
These methods model the KB reasoning process
as a sequential decision-making problem and treat
KBC as a Markov Decision Process, where an RL
agent explores paths in the KB to validate facts.
However, training effective path-finding agents
is particularly difficult due to the sparsity of the
reward signal, making them underperform com-
pared to other alternatives (Qu et al., 2021). More-
over, RL agents often struggle to navigate the large
search space to identify optimal paths, leading to re-
liability issues in KB reasoning (Zhou et al., 2023).
LLMs for KBC There also exist some works uti-
lizing LLMs for Knowledge Base Completion. The
general idea is to treat KBC as a language gener-
ation task and use LLMs to generate answers to
the query. However, KBC is generally considered
knowledge-intensive tasks that require a significant
amount of external knowledge as a supplement,
and existing works focused either on prompt design
or fine-tuning LLMs for performance gain (Veseli
et al., 2023; Zhang et al., 2024; Yao et al., 2024;
He et al., 2024). Notably, commonsense KBs (Sap
et al., 2019; Speer et al., 2017) differ from domain-
specific KBs as they are inherently very sparse and
incomplete. Some research uses LLMs for com-
monsense KB completion and construction with
a focus on capturing the implicit knowledge not
reflected in the commonsense KBs (Hwang et al.,
2021; Bosselut et al., 2019; Lin et al., 2019).
3 Method
We propose a novel framework,LeSR, consisting
of a Subgraph Extractor, an LLM Proposer, and
a Rule Reasoner, as shown in Fig. 2. The Sub-
graph Extractor samples a set of relevant subgraphs
3