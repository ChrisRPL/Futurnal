# 2505.13508v2.pdf

arXiv:2505.13508v2  [cs.CL]  3 Jun 2025
Time-R1: Towards Comprehensive Temporal
Reasoning in LLMs
Zijia Liu, Peixuan Han, Haofei Yu, Haoru Li, Jiaxuan You
Siebel School of Computing and Data Science, University of Illinois at Urbana-Champaign
{zliu331,jiaxuan}@illinois.edu
Abstract
Large Language Models (LLMs) demonstrate impressive capabilities but lack ro-
bust temporal intelligence, struggling to integrate reasoning about the past with
predictions and plausible generations of the future. Meanwhile, existing methods
typically target isolated temporal skills, such as question answering about past
events or basic forecasting, and exhibit poor generalization, particularly when
dealing with events beyond their knowledge cutoff or requiring creative foresight.
To address these limitations, we introduce Time-R1, the first framework to endow
a moderate-sized (3B-parameter) LLM with comprehensive temporal abilities:
understanding, prediction, and creative generation. Our approach features a novel
three-stage development path; the first two constitute a reinforcement learning (RL)
curriculum driven by a meticulously designed dynamic rule-based reward system.
This framework progressively builds (1) foundational temporal understanding and
logical event-time mappings from historical data, (2) future event prediction skills
for events beyond its knowledge cutoff, and finally (3) enables remarkable general-
ization to creative future scenario generation without any fine-tuning. Strikingly,
experiments demonstrate that Time-R1 outperforms models over 200 times larger,
including the state-of-the-art 671B DeepSeek-R1, on highly challenging future
event prediction and creative scenario generation benchmarks. This work provides
strong evidence that thoughtfully engineered, progressive RL fine-tuning allows
smaller, efficient models to achieve superior temporal performance, offering a
practical and scalable path towards truly time-aware AI. To foster further research,
we also release Time-Bench, a large-scale multi-task temporal reasoning dataset
derived from 10 years of news data, and our series of Time-R1 checkpoints.1
1 Introduction
Large Language Models (LLMs) have achieved remarkable success across a spectrum of language
understanding, generation, and even some complex reasoning tasks[ 1–3]. However, a persistent
shortcoming in even the most advanced LLMs is their temporal reasoning ability[ 4, 5]. This en-
compasses several key capacities[6–8]: accurately interpreting temporal relationships within their
existing knowledge base (such as inferring event times, time differences, event order, and completing
temporal entities), predicting the timing of future events based on learned patterns, and creatively
generating plausible future events anchored in time. Studies have shown that most LLMs indeed
struggle to update or contextualize knowledge under time constraints [9]; even frontier models have
been observed to perform worse than some smaller models in tasks that require integrating new
temporal information [10]. This suggests a systemic weakness in how current LLMs grasp time.
This weakness stems from multiple factors: architectural limitations [11], such as the lack of explicit
1Our code, the Time-Bench dataset, and Time-R1 model checkpoints are available at the project repository:
https://github.com/ulab-uiuc/Time-R1 and via our Hugging Face Collection:https://huggingface.
co/collections/ulab-ai/time-r1-682626aea47cb2b876285a16 .
Preprint. Under review.module representation of time; the static nature of their training corpora [ 12], which inevitably
become outdated; and the non-chronological training process [ 13], where temporal information
across different periods is processed concurrently rather than sequentially, hindering the development
of robust logical mappings between events and their corresponding times.
Based on the following information provided, predictthe most likely future date of the event:Japan: Yen depreciation, weak economic growth.<think> The article concerns Japan's inflation, which is likely referring to the year 2024. Typically, comprehensive economic reports for a full calendar year are published a few months into the subsequent year. Therefore, it is reasonable to expect that this article, would be published within the first few months following 2024. </think><answer> 2025-02 </answer> 
Given the future date of 2024-08, predict and generatea plausible news headline about Business that might be published on that date.
“Stock Market Sets New RecordHighs as Economy Remains Strong”
Real headline: “StocksHit a RecordAmid Strong Earnings and Easing Inflation Concerns”
Figure 1: Generated outputs from Time-R1
showcasing its capabilities. (Left) Future Event
Time Prediction (Stage 2). (Right) Creative Sce-
nario Generation (Stage 3), with output compared
to a real-world headline.
While existing research aims to enhance tem-
poral reasoning—for instance, Zhao et al.[13]
aligned LLM knowledge to target times, Kim
et al. [9] improved temporal consistency, and
Yuan et al. [5] focused on future event predic-
tion, with other works exploring representation
methods [14, 15]—these efforts often target iso-
lated skills. They typically fall short of endow-
ing LLMs with unified, comprehensive tempo-
ral intelligence that spans past understanding,
future prediction, and creative, time-anchored
generation, especially for events beyond their
knowledge cutoffs [13, 5].
In this paper, we aim to bridge this gap by equip-
ping a single 3B-parameter model with compre-
hensive temporal reasoning capabilities through multi-stage Reinforcement Learning (RL), which
has become a powerful framework for improving LLM reasoning. Recent frontior models such as
OpenAI-o1 [16] and DeepSeek-R1 [17] utilize RL methods like PPO [18] and GRPO [19], proving
effectiveness to learn complex reasoning capabilities, such as mathematical problem solving and
multi-step logical deduction. We build upon Qwen2.5-3B-Instruct, a moderate-sized LLM, and
demonstrate that through specialized training it can surpass models over 200× larger (for instance,
DeepSeek-R1, a 671B-parameter model) on highly challenging temporal prediction and genera-
tion tasks. We propose a three-stage framework with RL and dynamic rewards to progressively
establish the model’s unified temporal capabilities, spanning temporal logic, future prediction, and
time-anchored scenario generation: (1) Stage 1 - Comprehension: RL fine-tune the model using
pre-cutoff data from a cold start on four fundamental temporal tasks – timestamp inference, time-
difference estimation, events ordering, and masked time entity completion – to develop powerful
logical mappings between events and their corresponding times. (2) Stage 2 - Prediction: Further
train the model to predict events occurring after knowledge cutoff, thereby teaching it to utilize
general reasoning ability built in Stage 1 to extrapolate trends and anticipate future outcomes. (3)
Stage 3 - Generation: Directly have the model generate logical future scenario without fine-tuning,
leveraging the capabilities obtained from the first two stages.
Through this staged curriculum, the LLM thus progresses from comprehending known temporal
facts to skillfully navigating the complexities of the future. This advanced training culminates in
robust capabilities for both predicting future event timelines and creatively generating plausible
scenarios for unseen future contexts—addressing significant limitations in how current AI handles
such challenging forward-looking tasks. Illustrative examples of these advanced future-oriented skills,
such as Time-R1’s proficiency in forecasting event dates and generating contextually appropriate
news headlines for future dates (as depicted in Figure 1), highlight the practical efficacy of our
approach.
In summary, the key contributions of our work are as follows: (1) Unified Temporal Reasoning
in One Model: We introduce the first LLM that exhibits a holistic temporal reasoning ability
encompassing logic, prediction, and generation. (2) Small Model, Big Performance: We show that
a relatively small 3B model, when fine-tuned with our meticulously designed multi-stage dynamic-
reward RL strategy, can match or even exceed the performance of models with hundreds of billions
of parameters (e.g., the 671B-parameter R1 model) on temporal prediction and generation tasks.
(3) Fast Adaptability and Cost Efficiency: Our approach demonstrates that temporal knowledge
can be continuously refreshed in a cost-effective manner. A 3B model can be quickly fine-tuned on
new data as time progresses, which is infeasible for a hundreds of billion model that would require
enormous computational resources (on the order of millions of dollars for fine-tuning). (4) Resources
for the Community: To encourage further research in temporal-aware AI, we releaseTime-Bench, a
dataset of over 200,000 examples with explicit temporal annotations covering diverse tasks including
timestamp inference, time-gap estimation, event ordering, and temporal entity completion. We also
2release Time-R1, a series of high-performing and continuously updatable temporal reasoning model
checkpoints, offering a strong foundation for future time-aware LLM development and iterative
refinement.
2 Related Work
Temporal Reasoning in LLMs. While adept at many complex tasks [ 17, 20], LLMs struggle
significantly with temporal reasoning—understanding time and event interrelations—a faculty crucial
for comprehensive world understanding and interaction [ 4, 21, 6]. Recent studies increasingly
target these deficiencies, often focusing on specific temporal facets. For example, some efforts aim
to improve temporal accuracy by aligning LLM knowledge with a target time for time-sensitive
questions [13]. Meantime, some investigate methods for better integrating temporal information
into model representations [ 14], while others explore leveraging external knowledge sources or
structured representations like temporal graphs to augment LLM capabilities [15]. However, LLMs
exhibit particularly poor generalization when reasoning about the future, especially for events beyond
their knowledge cutoff or tasks requiring creative foresight. Consequently, robust methods for
direct, challenging future event prediction or creative scenario generation remain scarce in the
literature. While some initiatives explore future event prediction and forecasting (e.g., Yuan et al.
[5] employed instruction tuning to predict event occurrences from past contexts), comprehensive
approaches addressing the full spectrum of complex and creative future-oriented reasoning are largely
underdeveloped.
Reinforcement Learning in LLMs. Reinforcement learning (RL) has recently attracted attention due
to its scalability and enhanced generalization capabilities. Building on policy optimization algorithms
like PPO [18], reinforcement learning from human feedback (RLHF) — the first application of RL to
large language models — has become a standard paradigm for aligning LLMs with desired behaviors
[22, 23]. Recent advances aim to simplify or improve this process: Direct Preference Optimization
(DPO) [24] and Simple Preference Optimization (SimPO) [25] replace the conventional RL loop with
more direct optimization of preference-based rewards, eliminating the need for a separate reward
model or reference policy. Other methods are tailored specifically for LLMs; for instance, Group
Regularized Policy Optimization (GRPO) [19] introduces a group-based reward formulation in place
of a single critic, achieving more stable training and better generalization. Likewise, Ahmadian
et al.[26] revisit classic policy gradient techniques [27] to propose RLOO (REINFORCE-Leave-
One-Out), an online RL algorithm that refines LLM policies with reduced variance and cost. These
RL-driven approaches have demonstrated notable gains in LLM reasoning capabilities. In particular,
GRPO and related strategies have yielded state-of-the-art performance on complex reasoning tasks
including mathematical problem solving [19, 28], search engine interaction and knowledge retrieval
[29, 30], code generation tasks [31] and others [32–34]. Despite these successes, the application of
reinforcement learning to temporally-grounded reasoning remains underexplored. This gap suggests
an opportunity to leverage RL methods to develop unified, time-sensitive reasoning abilities in future
LLMs.
3 Method
This section details the Time-R1 methodology for enhancing LLM temporal capabilities via Rein-
forcement Learning (RL) fine-tuning. We introduce a novel three-stage training framework (Sec-
tion 3.2) guided by a dynamic, rule-based reward system (Section 3.3). We first outline the underlying
RL optimization setup using Group Relative Policy Optimization (GRPO) (Section 3.1) before
detailing these core framework and reward components.
3.1 Reinforcement Learning Fine-tuning for Temporal Reasoning
Our approach employs reinforcement learning (RL) to fine-tune a Large Language Model (LLM)
for complex temporal reasoning tasks. The core process involves interaction between the LLM
policy and a rule-based environment. Given a prompt x detailing a specific temporal task, the LLM,
parameterized by θ, generates an output sequence y autoregressively according to its current policy
πθ(y | x) = Q|y|
t=1 πθ(yt | x, y<t).
3