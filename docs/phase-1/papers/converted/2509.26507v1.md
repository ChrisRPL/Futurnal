# 2509.26507v1.pdf

THEDRAGONHATCHLING: THEMISSINGLINK
BETWEEN THETRANSFORMER ANDMODELS OF THEBRAIN
Adrian Kosowski∗† Przemysław Uzna´nski† Jan Chorowski† Zuzanna Stamirowska†
Michał Bartoszkiewicz†
Pathway, Palo Alto, USA
research@pathway.com
ABSTRACT
The relationship between computing systems and the brain has served as motivation for pioneering
theoreticians since John von Neumann and Alan Turing. Uniform, scale-free biological networks,
such as the brain, have powerful properties, including generalizing over time, which is the main
barrier for Machine Learning on the path to Universal Reasoning Models.
We introduce ‘Dragon Hatchling’ (BDH), a new Large Language Model architecture based on a
scale-free biologically inspired network ofnlocally-interacting neuron particles. BDH couples
strong theoretical foundations and inherent interpretability without sacrificing Transformer-like per-
formance.
BDH is a practical, performant state-of-the-art attention-based state space sequence learning archi-
tecture. In addition to being a graph model, BDH admits a GPU-friendly formulation. It exhibits
Transformer-like scaling laws: we find empirically that BDH rivals GPT2-architecture Transformer
performance on language and translation tasks, at the same number of parameters (10M to 1B), for
the same training data.
BDH provides theoretical foundations for understanding model behavior in the limit of large size and
reasoning time. Our results, formalized as a chain of reductions of expressiveness in the framework
of computational Complexity Theory and Distributed Computing, and combined with findings on
the BDH model, show a macro-to-micro correspondence of function between the general attention
mechanisms in state-of-the-art Language Models, and attention mechanisms observed in the brain.
These attention mechanisms formally converge as closed-form local graph dynamics at neurons and
synapses: “the equations of reasoning”.
BDH can be represented as a brain model. It containsnneurons, organized as an excitatory circuit
and an inhibitory circuit with integrate-and-fire thresholding of input signals at neurons. The work-
ing memory of BDH during inference entirely relies on synaptic plasticity with Hebbian learning
using spiking neurons, at potentiation scales of minutes for the brain (up to hundreds of tokens). We
confirm empirically that specific, individual synapses strengthen connection whenever BDH hears
or reasons about a specific concept while processing language inputs. The neuron interaction net-
work of BDH is a graph of high modularity with heavy-tailed degree distribution. The BDH model
is biologically plausible, explaining one possible mechanism which human neurons could use to
achieve speech.
BDH is designed for interpretability. Activation vectors of BDH are sparse and positive. We demon-
strate monosemanticity in BDH on language tasks, including representation of concept abstractions,
which happens even for small models, below 100M-parameter scale. Interpretability of state, which
goes beyond interpretability of neurons and model parameters, is an inherent feature of the BDH
architecture.
We believe BDH opens the door to a new theory of “Thermodynamic Limit” behavior for language
and reasoning models, with the ultimate goal of Probably Approximately Correct (PAC)-like bounds
for generalization of reasoning over time.
▷Technical blog entry:https://pathway.com/research/bdh.
▷Code listings:https://github.com/pathwaycom/bdh.
∗Author contributions are listed at the end of the paper.
†Corresponding author.
arXiv:2509.26507v1  [cs.NE]  30 Sep 2025Contents
1 Introduction 3
1.1 Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3
1.2 Intuition of results: combiningmodus ponensreasoning with Hebbian learning . . . . . . . . . . . . 5
1.3 Contribution of this work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7
1.4 Notation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9
2 BDH: a language model architecture given by local distributed graph dynamics 10
2.1 Formalism for local graph-based language models . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10
2.2 Definition of BDH as a local edge-reweighting process (equations of reasoning) . . . . . . . . . . . . 12
2.3 Interpretation of attention as a micro-inductive bias of reasoning . . . . . . . . . . . . . . . . . . . . 14
2.4 Interpretation of BDH as an oscillator network toy-model . . . . . . . . . . . . . . . . . . . . . . . . 14
2.5 Expressing BDH using brain models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16
3 BDH-GPU: a tensor-friendly version of the BDH architecture 17
3.1 Notation for BDH-GPU . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17
3.2 Definition of BDH-GPU as a state-space system . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18
3.3 Interpretation of BDH-GPU as a local interacting particle system . . . . . . . . . . . . . . . . . . . . 20
3.4 Expressing BDH-GPU using BDH: preserving parameter and state size . . . . . . . . . . . . . . . . 21
4 Implementation and scaling laws 23
4.1 Implementation characteristics of BDH-GPU . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
4.2 Comparison of BDH-GPU to GPT2-like Transformers . . . . . . . . . . . . . . . . . . . . . . . . . 24
4.3 Comparison of BDH-GPU to other sequence processing architectures . . . . . . . . . . . . . . . . . 24
5 Analysis: emergence of modularity and scale-free structure 26
5.1 Background: modularity and scale-free property of systems . . . . . . . . . . . . . . . . . . . . . . . 26
5.2 BDH-GPU feed-forward network with the ‘ReLU-lowrank’ block . . . . . . . . . . . . . . . . . . . 27
5.3 ReLU-lowrank as a signal propagation dynamics . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28
5.4 Modularity in BDH-GPU signal propagation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29
5.5 Empirical findings: parameter distribution in ReLU-lowrank matrix products . . . . . . . . . . . . . 31
6 Analysis: linear attention, sparse positive activation, and monosemanticity 34
6.1 Macro-expressiveness of attention in BDH-GPU . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34
6.2 Micro-interpretation of attention in BDH-GPU . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37
6.3 Empirical findings: monosemantic synapses . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37
6.4 Empirical findings: sparse neuron activations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38
7 Playing with the Hatchling 41
7.1 Model merging: concatenating two models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41
7.2 Training without backpropagation through time . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42
28 Conclusions 43
8.1 Takeaways for model engineering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43
8.2 Implications for brain science . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43
8.3 Societal impact . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44
A Connection between generalization of reasoning and computational expressiveness 53
B Further description of experiments 53
C Omitted formal claims and proofs 55
D Desirable properties of a local graph dynamics for language models 58
E BDH-GPU PyTorch code listing 61
1 Introduction
Long reasoning and long context inference pose a severe challenge of generalization across scales of time. From
vibe coding to market research, users of Language Models and agentic systems are increasingly relying on defining
tasks through informal prompts, which the language model is expected to follow over long sequences of actions or
decisions, like a reasonable human actor would. Implicitly, most users expect machines to follow the generalization
patterns of human reasoning, i.e., to generalize reasoning in the same way as humans do. The complexity of tasks
attempted in this way has gone from the equivalent of hours of human work for a single prompt, to weeks (Emberson
et al., 2025). However, experimental evidence suggests that the Transformer and other state-of-the-art architectures
do not systematically generalize chain-of-thought (CoT) reasoning to scenarios longer than the ones seen during
training (Shojaee et al., 2025).
Chain-of-Thought reasoning models can be considered through the lens of computational complexity theory. For a
Language Model to generalize human reasoning on a given class of tasks, we expect this model to be able to emulate
the corresponding reasoning function of the human brain efficiently.3 While the Transformer with Chain-of-Thought
is Turing-complete and can efficiently emulate certain restricted classes of formal languages (Merrill and Sabharwal,
2024), this does not in itself provide a satisfactory answer as to how it emulates human reasoning. The human brain
is an extremely complex graph-based distributed computing system withn≈8·10 10 neurons, andm >10 14 neuron
connections (synapses), of which a certain percentage is actively used. The direct simulation of such a distributed
system by a Language Model through generic Turing-machine reductions would require billions of CoT tokens of the
Language Model to represent a single step of reasoning in the brain. So, do Transformer-like models actually relate to
brain function?
Such a relationship should follow more closely from a tighter, more direct simulation. Finding such a connection
between Language Models and human brain function has, so far, proved elusive. Indeed, when comparing a tensor-
based Language Model based on feed-forward network blocks and attention, to a uniform, scale-free graph-based
distributed system, such as the brain, the two may, at first glance, appear very dissimilar.
This apparent dissimilarity of structure between Language Models and brain structure has been one of the main causes
of concern in attempts to reconcile Computation and the Brain (Olshausen, 2018), as well as a cause of concern
regarding the difficulty to foresee the behavior of autonomous AI systems.
In this paper, we show the link between the Transformer and Brain models.
1.1 Motivation
The development of Artificial Intelligence and the understanding of Neural Science have gone hand in hand since
the 1940’s, both being efforts to understand the “mystery of intelligence”. The relationship between computing sys-
tems and the brain served as motivation for the pioneering theoreticians such as John von Neumann (1958), Alan
Turing (1950), Goeff Hinton (2005), Warren McCulloch and Walter Pitts (1943), and Horace Barlow (1972).
3We provide a more formal explanation of this point in Appendix A.
3