# 2509.21730v1.pdf

Preprint.
PROPERSIM: DEVELOPINGPROACTIVE AND
PERSONALIZEDAI ASSISTANTS THROUGH
USER-ASSISTANTSIMULATION
Jiho Kim1, Junseong Choi1, Woosog Chay1, Daeun Kyung1, Yeonsu Kwon1,
Yohan Jo2, Edward Choi1
1KAIST 2SNU
{jiho.kim, edwardchoi}@kaist.ac.kr
ABSTRACT
As large language models (LLMs) become increasingly integrated into daily
life, there is growing demand for AI assistants that are not only reactive but
alsoproactiveandpersonalized. While recent advances have pushed forward
proactivity and personalization individually, their combination remains underex-
plored. To bridge this gap, we introduceProPerSim, a new task and simu-
lation framework for developing assistants capable of making timely, personal-
ized recommendations in realistic home scenarios. In our simulation environ-
ment, a user agent with a rich persona interacts with the assistant, providing
ratings on how well each suggestion aligns with its preferences and context.
The assistant‚Äôs goal is to use these ratings to learn and adapt to achieve higher
scores over time. Built onProPerSim, we proposeProPerAssistant,
a retrieval-augmented, preference-aligned assistant that continually learns and
adapts through user feedback. Experiments across 32 diverse personas show that
ProPerAssistantadapts its strategy and steadily improves user satisfaction,
highlighting the promise of uniting proactivity and personalization.
1 INTRODUCTION
Large Language Models (LLMs) have become a familiar part of everyday life. Beyond simply
answering questions, they now assist with a wide range of tasks such as writing (Chakrabarty et al.,
2023; Lee et al., 2022; 2024), programming (Mozannar et al., 2024; Xiao et al., 2024; Akhoroz &
Yildirim, 2025), and managing schedules (Google, 2024), making them increasingly indispensable.
As the scope of their assistance continues to grow, there is rising demand for LLMs to evolve from
passive chatbots into personal assistants that can take initiative before a user makes a request (i.e.,
proactivity) and adapt to individual users (i.e.,personalization) (Li et al., 2024b; Lu et al., 2024).
In response to this trend, researchers have begun developing AI assistants designed to embody these
capabilities. In terms of proactivity, recent studies have explored assistants that offer timely sugges-
tions in everyday situations (Lu et al., 2024) or programming environments (Chen et al., 2024). For
personalization, researchers have focused on customizing interactions by using tailored prompts and
modeling users‚Äô past behavior (Dai et al., 2023; Yang et al., 2023; Baek et al., 2024; Lyu et al., 2024;
Zhang et al., 2024a; Zhou et al., 2024). These efforts have improved user experience by addressing
different aspects of assistant behavior. However, since they have progressed independently, impor-
tant limitations remain. Without personalization, proactive suggestions may arrive when the user
does not want them and may present content misaligned with the user‚Äôs needs; without proactivity,
even personalized support still requires users to initiate interaction, as shown in Figure 1. Thus, to
build truly helpful AI assistants, it is crucial to integrate both proactivity and personalization.
To address this gap, we introduceProPerSim, a new simulation-based task and benchmark de-
signed to develop proactive and personalized AI assistants. InProPerSim, a user agent inhabits a
simulated home environment and interacts with an AI assistant that offers context-aware recommen-
dations. The assistant‚Äôs objective is to maximize the user agent‚Äôs satisfaction over time by making
timely and personalized suggestions.
1
arXiv:2509.21730v1  [cs.CL]  26 Sep 2025Preprint.
 &YUSBWFSTJPO)JHI
 "HSFFBCMFOFTT-PX
 0QFOOFTT)JHI
 $POTDJFOUJPVTOFTT-PX
 /FVSPUJDJTN)JHI
JIMMY
üë®
Personality
Lifestyle
 7FHFUBSJBO
Only Proactivity
Only Personalization
ü§ñ
ü§ñ 	4JMFODF
üôÖ
ü§¶
Ours (Proactivity + Personalization)
ü§ñ
üôÜ
)PXBCPVUEJOOFSBU1SJNF#BSSFM *UsTBUPQSBUFETUFBLIPVTFLOPXOGPSJUTUPNBIBXL
4FSJPVTMZ :PVsSFSFDPNNFOEJOHBUPNBIBXLTUFBLUPBWFHFUBSJBO 5IBUNBLFTOPTFOTF
ü§ñ
:PVOFWFSUBLFUIFJOJUJBUJWF$PVMEZPVSFDPNNFOETPNFUIJOHGPSEJOOFS 
*UsTBMNPTUEJOOFSUJNF4JODFZPVsSFWFHFUBSJBOIPXBCPVU(SFFO)PVTF 5IFJS1PUBUP#PXM
BOE8JME.VTISPPN5SVGGMF3JTPUUPBSFGBWPSJUFT
4PVOETHSFBUnBOEQFSGFDUUJNJOH*MPWFQPUBUPEJTIFT
)PXBCPVUBWFHFUBSJBONFBMBU(SFFO)PVTF 5IFJSTJHOBUVSFEJTIFTJODMVEFUIF1PUBUP#PXM
Figure 1:Only Proactivityshows initiative but ignores preferences (steakhouse to a vegetarian);Only
Personalizationfits preferences but lacks initiative. Ours (Proactivity+Personalization) proactively
recommends a vegetarian dinner at the right moment.
The user agent is modeled to realistically mimic human behavior, defined by a rich persona that in-
cludes attributes such as background, lifestyle, and the Big Five personality traits (McCrae & John,
1992). This persona guides the agent‚Äôs behavior as it engages in everyday activities through an
LLM-based simulation. Throughout the simulation, the assistant continuously monitors the agent‚Äôs
behavior to determine optimal moments for intervention, deciding at each timestep whether a rec-
ommendation is appropriate and, if so, tailoring it to the agent‚Äôs current context and preferences.
These decisions are evaluated by the user agent based on both content and timing, reflecting how
well the recommendation aligns with its goals, personality, and situation. The evaluation relies on
criteria informed by large-scale survey data, ensuring realistic and nuanced assessments of assistant
behavior. Feedback from the user agent serves as a training signal to iteratively refine the assistant‚Äôs
recommendation strategy, enabling continual improvement in personalization and proactivity. We
generated a total of 32 distinct personas, and human evaluators confirmed both the realism of the
user agents‚Äô daily activities and the quality of recommendation evaluations based on these personas.
Building onProPerSim, we presentProPerAssistant, a proactive and personalized assistant
that leverages retrieval-augmented generation (RAG) (Lewis et al., 2020) and preference alignment
to adapt its behavior to individual user agents. Trained to personalize its behavior for each persona,
the assistant begins with an average performance score of 2.2 out of 4, then improves over time
and eventually stabilizes at 3.3, enabling it to deliver timely and appropriate recommendations. We
further provide an in-depth analysis of how the assistant‚Äôs strategy evolves across different personas
and aligns with various evaluation criteria.
2 RELATEDWORKS
Proactive AgentsProactivity refers to an assistant‚Äôs ability to initiate interactions or offer helpful
suggestions before receiving a user query. This capability has been explored in various domains,
including making conversations more engaging (Fitzpatrick et al., 2017; Liu et al., 2024), enhancing
helpfulness of responses (Ren et al., 2021; Bi et al., 2021; Li et al., 2024c), providing timely sup-
port in educational settings (Winkler & Roos, 2019), and assisting with programming tasks (Chen
et al., 2024). More recently, Proactive Agent (Lu et al., 2024) has been introduced, trained on a
dataset of 6,790 training events and 233 test events spanning coding, writing, and daily life scenar-
ios. The agent demonstrated strong performance, as evaluated by a reward model that estimated user
satisfaction. Despite these advances, the role of personalization in proactive interactions remains un-
derexplored. Users may have different preferences regarding when they want the assistant to initiate
a conversation and what type of proactive content they find useful. However, current research rarely
takes these individual differences into account, leaving a significant gap in the development of truly
user-centered proactive systems.
2Preprint.
Personalized AgentsPersonalization aims to tailor models to individual users by incorporating
their preferences, tastes, and interaction history (Tseng et al., 2024). Personalized assistants have
been applied across various domains, from simple dialogue generation (Ashby et al., 2023) to ed-
ucation (Arefeen et al., 2024; Hao et al., 2024), healthcare (Abbasian et al., 2023; Zhang et al.,
2024b), and recommendation systems (Chen et al., 2022; Li et al., 2023). Recent studies have ex-
plored personalization through (1) prompt-based approaches that use user demonstrations or struc-
tured prompts to elicit personalized responses (Dai et al., 2023; Lyu et al., 2024), (2) retrieval-based
methods that reformulate prompts using user history and preferences (Yang et al., 2023; Zhou et al.,
2024), and (3) fine-tuning techniques such as RLHF (Ouyang et al., 2022) adapted to user-specific
feedback (Jang et al., 2023; Li et al., 2024a). While these approaches enhance user satisfaction by
enabling more tailored interactions, they generally do not consider proactivity, such as initiating
conversations or recommending actions based on the user‚Äôs current state.
Human Behavior Simulation in Generative AgentsLeveraging the high contextual understand-
ing and reasoning capabilities of LLMs, Generative Agents have emerged as a means to simulate
human-like behavior (Park et al., 2023). In this study, a social simulation was conducted in a virtual
town calledSmallville, where 25 agents lived and interacted with one another. These generative
agents successfully mimicked human social behaviors by planning daily routines, observing their
environment, forming interpersonal relationships, engaging in self-reflection, and using this reflec-
tion to inform future actions. To evaluate how well these agents understood and embodied their
roles, the authors conducted interviews that assessed their self-knowledge, memory, and other cog-
nitive functions. The generative agents demonstrated performance comparable to that of human
role-players, suggesting that LLM-based agents can effectively simulate human behavior at a high
level within a simulated environment.
3 TASKFORMULATION
To build a proactive and personalized AI assistant, it is crucial to construct preference data that
captures a user‚Äôs unique persona across diverse situational contexts. However, collecting large-
scale human behavioral data poses significant challenges due to the wide variability in individual
preferences and concerns over user privacy (Li et al., 2024b). To address these challenges, we
propose a simulation-based task, inspired by recent research showing that agents with personas can
effectively mimic human behavior (Park et al., 2023).
In our task, a user‚Äôs day is modeled as a sequence of actions, each with a specific time interval:
{(Ai,Range i)}N
i=1 =U(E, P, S)(1)
Here,A i denotes thei-th user action, and Range i = [tstart
i , tend
i )is the time span during which
the action occurs (e.g., Washing face and brushing teeth, [08:00:00‚Äì08:15:00]). The user policy
Ugenerates this sequence based on the environmentE, the user‚Äôs personaP, and internal stateS
(capturing factors like the user‚Äôs memory, plans, and emotions). The total number of actions in a
day isN.
At discrete time stepst‚àà {T,2T,3T, . . .}, the assistant generates a recommendationRt through its
policyA Œ∏, which takes as input the current user actionA t and the assistant‚Äôs internal stateS (a)
t for
recommendation:
Rt =A Œ∏(At, S(a)
t )(2)
Here,A t is the user‚Äôs action being performed at timet‚ààRange i, andS (a)
t is the assistant‚Äôs internal
state specifically designed for recommendation purposes. It captures the assistant‚Äôs accumulated un-
derstanding of the user over time, such as observed behavior patterns, past interactions, and inferred
preferences, enabling personalized and contextually relevant suggestions. Notably,R t may also be
a ‚ÄúNo Recommendation‚Äù response. The ability to withhold suggestions when they are unnecessary
is a key trait of a well-designed proactive agent.
To evaluate the quality of the assistant‚Äôs recommendations, we define an evaluation functionEthat
outputs Scoret, based on the user‚Äôs personaP, personalized rubricr, user actionA t, recommenda-
tionR t, and the user‚Äôs evaluative stateS (u)
t :
Scoret =E(P, r, At, Rt, S(u)
t )(3)
3