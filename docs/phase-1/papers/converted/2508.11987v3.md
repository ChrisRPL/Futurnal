# 2508.11987v3.pdf

FutureX: An Advanced Live Benchmark for LLM Agents
in Future Prediction
1ByteDance Seed,2Fudan University,3Stanford University,4Princeton University
Full author list in Contributions
Abstract
Future prediction is a complex task for LLM agents, requiring a high level of analytical thinking,
information gathering, contextual understanding, and decision-making under uncertainty. Agents
must not only gather and interpret vast amounts of dynamic information but also integrate diverse
data sources, weigh uncertainties, and adapt predictions based on emerging trends, just as human
experts do in fields like politics, economics, and finance. Despite its importance, no large-scale
benchmark exists for evaluating agents on future prediction, largely due to challenges in handling
real-time updates and retrieving timely, accurate answers. To address this, we introduceFutureX,
a dynamic and live evaluation benchmark specifically designed for LLM agents performing future
prediction tasks. FutureX is the largest and most diverse live benchmark for future prediction,
supporting real-time daily updates and eliminating data contamination through an automated
pipeline for question gathering and answer collection. We evaluate 25 LLM/agent models, including
those with reasoning, search capabilities, and integration of external tools such as the open-source
Deep Research Agent and closed-source Deep Research models. This comprehensive evaluation
assesses agents’ adaptive reasoning and performance in dynamic environments. Additionally, we
provide in-depth analyses of agents’ failure modes and performance pitfalls in future-oriented
tasks, including the vulnerability to fake web pages and the temporal validity. Our goal is to
establish a dynamic, contamination-free evaluation standard that drives the development of LLM
agents capable of performing at the level of professional human analysts in complex reasoning and
predictive thinking.
Project Page:https://futurex-ai.github.io/
Figure 1Overall scores on FutureX between July 20th and August 3rd.
Correspondence toliujiashuo.77@bytedance.comandhuang.wenhao@bytedance.com
1
arXiv:2508.11987v3  [cs.AI]  5 Sep 20251 Introduction
The rapid evolution of Large Language Models (LLMs) has catalyzed a fundamental shift in the landscape
of artificial intelligence, moving from the generation of coherent text to the creation of autonomous agents
capable of complex, goal-oriented behavior [1, 2, 24, 26, 29, 40]. This transition from passive text generators to
active problem-solvers necessitates a corresponding evolution in evaluation methodologies. While foundational
benchmarks like MMLU [10] and SuperGLUE [31] are instrumental in assessing the static knowledge of LLMs,
they are insufficient for measuring what a model can do when deployed as part of an interactive, goal-seeking
system. An agent’s performance is defined not just by its underlying model, but by its ability to plan, use
external tools, and adapt to a dynamic environment.
In response, a new generation of agent-centric benchmarks has emerged, primarily focused on evaluating
search, tool usage, and coding skills in controlled or simulated settings. For example, AgentBench [17]
provides a comprehensive evaluation of an agent’s reasoning and decision-making across eight simulated
environments, such as operating systems and databases. Similarly, WebArena [47] and mind2web [6] offer
high-fidelity simulations of real-world websites, assessing an agent’s ability to complete complex, long-horizon
tasks. GAIA [18], introduced to evaluate agents as general-purpose assistants, presents real-world questions
that are conceptually simple for humans but require a sophisticated blend of reasoning, multi-modality, web
browsing, and tool proficiency. Its questions are designed to have clear, factual answers, simplifying evaluation
while testing a broad range of skills across three difficulty levels. BrowseComp [33] challenges agents to locate
hard-to-find, entangled information online using “inverted” questions that are difficult to solve but easy to
verify. This benchmark specifically tests persistence and creative search strategies, going beyond simple fact
retrieval. In the specialized domain of software engineering, SWE-bench [13] evaluates agents on their ability
to resolve real-world issues from open-source GitHub repositories. By having agents generate a code patch
and verify it against the project’s test suite, it provides a realistic, execution-based assessment of coding
proficiency.
While these benchmarks offer valuable insights into agent capabilities, they largely address static, well-defined
problems whose solutions are already known. Further, they fail to address a critical gap: the ability to
synthesize dynamic, real-world information, process it, and perform complex analysis and reasoning—the
very skills possessed by human experts across different domains. Future prediction, in fact, directly addresses
these two drawbacks. This task directly tests an agent’s ability to integrate dynamic, real-world information,
process it in context, and generate complex analysis and reasoning about problems whose answers are not
yet known to the world. Such tasks naturally involve a dynamic element, and their primary significance lies
in preparing agents to anticipate and navigate genuinely novel scenarios, mirroring the foresight applied by
human experts across diverse domains.
However, building benchmarks on future prediction faces significant methodological and technical challenges.
First, creating questions and answers for such a benchmark is inherently difficult because forecasting requires
information that is not readily available or easily constructed. Unlike static factual questions, future events
are uncertain, making it difficult to generate reliable, verifiable answers. Additionally, the dynamic nature
of real-world data means that the benchmark would require continuous updates to ensure relevance and
accuracy—something that would demand a constantly evolving question pool [15]. This continuous update
process is both technically demanding and resource-intensive. Another significant challenge is the timeliness
of testing, as it cannot be performed after the resolution date, and there are inherent flaws when relying on
historical data for evaluation [22]. Past data inherently contains information about future events, which leads
to logical leakage—where knowledge of the outcome can influence the evaluation, undermining its accuracy.
Additionally, relying on historical data to assess future prediction introduces retrieval contamination: search
results for past events are inevitably biased by knowledge of events that happened afterward, making it
impossible to accurately evaluate an agent’s forecasting ability. Therefore, probably the only methodologically
sound way to evaluate future prediction capability is to do so prospectively, in a live, forward-looking pipeline.
In response to this need, we introduceFutureX, a dynamic and live evaluation benchmark specifically designed
for LLM agents performing future prediction tasks. FutureX is built upon a semi-automated pipeline that
continuously collects future-oriented questions from 195 diverse websites, curated from a pool of 2,008 sites
covering areas such as politics, economics, technology, sports, healthcare, and more. This curation process
2involves both LLM-based agents and human experts, a necessary combination to ensure quality. Each event is
associated with a start date (several days prior to the resolution date) and a resolution date. The pipeline
automatically collects and stores agents’ predictions on the start date. After the resolution date passes,
the system dynamically crawls the web to retrieve the ground-truth outcome and scores the agent’s prior
predictions. FutureX provides four key advantages that directly address the limitations of existing benchmarks:
• Large-Scale and Broad Domain Coverage: Using a semi-automated pipeline for question collection and
filtering, we currently select 195 websites from a pool of 2,008 as our sources. These selected websites
cover a wide range of topics—including politics, economics, finance, sports, and entertainment—making
it, to our knowledge, thelargest and most diverse live benchmarkfor future prediction.
• Real-Time Updates: FutureX continuously collects future-oriented questions from 128 websites, with
daily updates to ensure real-time relevance. By dynamically crawling answers, the benchmark maintains
both timeliness and diversity in the questions, presenting a unique challenge for LLM agents to process
and adapt to constantly evolving information.
• No Data Contamination: By focusing exclusively on future events, FutureX inherently eliminates any
risk of data contamination, preventing any exploitation of historical information to manipulate the
results.
• Comprehensive & Automated Assessment of LLM Agents: Building on FutureX, we have developed a
fully automated evaluation pipeline that updates future questions daily, runs various LLM agents for
each event on its start date, collects event outcomes after the resolution date, and evaluates agents’
performance. The models under evaluation include base LLMs, LLMs with reasoning and search
capabilities, open-source Deep Research Agents, and closed-source Deep Research Agents, for a total of
25 models.
In addition to the overall performance leaderboard in Figure 1, we conduct an in-depth analysis of LLM
agents’ performance, addressing the following questions:
•How do LLM agents perform on questions of varying difficulty levels and across different domains?
(performance analysis, see Section 4.2 and Section 4.3)
• What factors (such as the type of LLM model, agent framework, and question domain) have the most
statistically significant impact on performance?
(performance analysis, see Section 4.4)
•How do LLM agents perform when making predictions after the resolution date?
(performance analysis, see Section 4.5.1)
•How planning and search capabilities affect the performance?
(performance analysis, see Section 4.5.2 and Section 4.5.3)
Furthermore, we also provide some “out-of-benchmark” analysis that emerged during the development of
FutureX, including:
•Can LLM agents beat professional Wall Street financial analysts in finance prediction?
(capability analysis, see Section 5.1)
•Are Deep Research agents vulnerable to fake news?
(safety analysis, see Section 5.2)
• Can the most advanced LLMs with reasoning and web-searching capabilities gather timely information
effectively?
(efficiency analysis, see Section 5.3)
To pave the way of the “Second Half of AI1”, we firmly believe FutureX has great potential to unlock new
research directions for developing LLM agents capable of performing at the level of professional human analysts
in real-world, high-stakes domains.
1https://ysymyth.github.io/The-Second-Half/
3