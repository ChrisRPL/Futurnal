# 2509.25189v1.pdf

Under Review
INFOAGENT: ADVANCINGAUTONOMOUSINFORMA-
TION-SEEKINGAGENTS
Gongrui Zhang1*† Jialiang Zhu1*† Ruiqi Yang2*† Kai Qiu3*‡ Miaosen Zhang1†
Zhirong Wu3 Qi Dai3 Bei Liu3 Chong Luo3 Zhengyuan Yang3 Linjie Li3 Lijuan Wang3
Weizhu Chen3 Yuan Zhang3 Xin Li3 Zhaoyi Liu3 Xin Geng1 Baining Guo3
1Southeast University 2Brown University 3Microsoft
*Equal Core Contributors †This work was done during the internship at MSRA ‡Project Leader
ABSTRACT
Building Large Language Model agents that expand their capabilities by inter-
acting with external tools represents a new frontier in AI research and applica-
tions. In this paper, we introduce InfoAgent, a deep research agent powered by
an innovative data synthesis pipeline and orchestrated web search tools. To con-
struct challenging, hard-to-find queries, we build entity trees and apply sub-tree
sampling with entity fuzzification to systematically increase question difficulty.
Unlike prior work that relies heavily on commercial search tools, we develop a
dedicated self-hosted search infrastructure, enhancing transparency of agent en-
vironments and facilitating further advancement of agent capacity. We evalu-
ate the effectiveness of our data pipeline by measuring the average number of
tool calls required to correctly answer a question, and also show that our agent
yields better performance when equipped with our tools. Our InfoAgent is post-
trained from Qwen3-14B using a two-stage recipe: cold-start supervised finetun-
ing to instill long-horizon search behaviors, followed by reinforcement learning
which significantly improves reasoning-driven tool use. With our methods, In-
foAgent achieves 15.3% accuracy on BrowseComp, 29.2% on BrowseComp-ZH,
and 40.4% on Xbench-DS, outperforming prior open-source deep research agents
such as WebSailor-72B and DeepDive-32B.
1 INTRODUCTION
The Internet has revolutionized the way people acquire knowledge, yet the tools that mediate access
to online information have evolved unevenly (Zhang et al., 2025). Recently, researchers have en-
hanced Large Language Models (LLMs) with agentic capabilities via Reinforcement Learning (RL),
which allows them to autonomously plan, search, and learn in an ongoing loop (OpenAI, 2025b).
Deep Research Agents (DRAs) are distinguished by their ability to plan, reason, execute multi-step
information-seeking actions, such as retrieving documents from the Internet via given tools, and
complete complex research tasks. Recognizing their potential, major AI providers have raced to de-
liver commercial implementations (OpenAI, 2025a; Perplexity, 2025; xAI, 2025a; Google, 2025).
This phenomenon shows that deep research is becoming a defining feature of next-generation infor-
mation platforms.
The implementation of DRA faces two challenges: effective strategy for data synthesis and the
establishment of an efficient interactive environment. Existing open-source DRAs often perform
shallow searches, mainly because they are trained on relatively simple data (Jin et al., 2025; Li
et al., 2025c). Training dataset must encompass a broad range of data, which is of various uncertain
types, so that the agent is forced to link disparate pieces of information and infer new knowledge
when retrieving documents. Meanwhile, some agents are trained in simulated environments, which
are underpowered when confronted with challenging real-world problems (Jin et al., 2025). We
suggest that RL infrastructure for DRA must provide tools accessible to real-world information,
which must be able to handle high-concurrency search and browsing calls, and returns consistent
results during RL training.
1
arXiv:2509.25189v1  [cs.CL]  29 Sep 2025Under Review
Question
“Which teammate accompanied 
a Swiss dancer who won the 
Juste Debout World 
Champion title
 with some vibrating 
dance actions?”
Step 1: Search
Step 2: Browse
Continue 
searching
……
Step N-1: Search
Step N: Answer
<think> Let’s first determine what kind of                            
                                            street dance style it is </think>
                            <tool> search “vibration dancing actions” </tool>
Dance stylesis a 
general term that is 
interchangeable with 
the terms “dance 
genres” or “types of 
dance.” Here we 
feature all  ……
20+ Types of 
Dance: …
The slow pace of Waltz 
gives couples freedom 
to move and connect 
closer than ever with 
each other.
This is where the 
romance of Waltz
……
Vibration of 
Dance Floor
I have seen people 
adding some 
percentages of live and 
superimposed dead 
loads to the mass of the 
floor. The model is then 
……
…
<think> After checking the search results, I  believe doc-2    
      contain the information I need, I should take a closer 
            look at it. </think>   <tool> browse doc-2 </tool>
66 dancing styles from A to Z
The dance was popularized by a Fresno and Long Beach-based dance 
group called theElectric Boogaloos, which mixed popping 
techniques with boogaloo. Closely related dance styles and 
techniques are often integrated into popping to create a more varied 
performance. These dance styles include therobot, vibration and 
tutting. However, popping is distinct frombreaking and locking, with 
which it is often confused. A popping dancer is commonly referred to 
as a "popper".
As stated earlier,poppinghas become the latest umbrella term for a 
group of closely related styles and techniques rooted in the boogaloo 
tradition that have often been combined, ……
66 dancing 
styles …
<think>  Now I need to find the Swiss dancer that won
the popping champion of Juste Debout </think>
<tool> search “Swiss Juste Debout popping 
champion ” </tool>
We met Swiss popping 
dancer Poppin'C to talk 
about his background, 
……
10 things you 
need to know 
about Poppin C
Poppin Final
JD-2019
Video of Poppin C & 
NESS vs Creesto & 
Prince. From jaw-
dropping battles to 
awe-inspiring 
performances, Juste 
Debout is the ……
…
<think> In the search results, we can find 
out that the Swiss dancer that won Juste Debout
popping champion is Popping C from doc-1. 
In the doc-3, we found the information that the 
battle in the Juste Debout Final happened between 
Popping C & Ness vs. Creesto & Prince. Therefore, we 
can conclude that the teammate of Popping C is Ness,
and I don’t need to search anymore. </think>
<answer> Ness </answer>
✓
BrowseComp
BrowseComp-ZH
XBench
MiroThinker-14B
WebDancer-32B WebSailor-7B
DeepDive-9B
InfoAgent-14B (ours)
2.5
6.7
6.3
9.0
15.3
14.1
14.2
15.1
11.1
29.2
38.7
34.3
38.0
30.0
40.4
Inf
Agent
Figure 1: An illustration of how InfoAgent leverage search and browse tools to solve information-
seeking problems (left). InfoAgent achieves advanced results on several deep research benchmarks
(right).
In this paper, we build InfoAgent, a DRA designed for long-horizon information seeking and deep
reasoning. Our work addresses the two bottlenecks identified above, data synthesis and interactive
environment. On the data side, we devise an end-to-end synthesis pipeline that generates challenging
problems that must be solved via deep research. Considering the requirements for broader logical
structures and more varied uncertain types in training data, we start from raw Wikipedia entity set
and build entity trees. Then, we apply sub-tree sampling with entity fuzzification to systematically
enhance the difficulty of each question. These designs force an agent to perform long-horizon re-
trieval and conjunctive reasoning rather than rely on single-hop lookup, thereby lifting the upper
bound of its ability. Regarding the environment, many existing agents lean heavily on commercial
search APIs. In this case, the retrieval process is hidden behind proprietary services, and the ef-
ficiency is also constrained by external rate limits and tool availability. This dependence not only
makes the behavior of the agent uncontrollable, but also makes training and evaluation hard to
reproduce. We therefore forgo commercial services and construct our own search and browsing in-
frastructure. This gives us fine-grained control over outputs and provides a transparent experimental
environment.
We evaluate our data and environment by post-training the Qwen3-14B (Yang et al., 2025) with
a two-stage recipe. In the first stage, we perform supervised finetuning (SFT) as a cold start, in
order to instill long-horizon search behavior into the model. In the second stage, we apply RL
to refine its ability of reasoning-driven tool use. As shown in Figure 1, InfoAgent attains 15.3%
accuracy on BrowseComp (Wei et al., 2025), outperforming previous open-source DRA such as
WebSailor-72B (Li et al., 2025b) and DeepDive-32B (Lu et al., 2025). Moreover, even though all
of our synthetic training data are in English, InfoAgent exhibits strong cross-lingual generalization:
it achieves competitive accuracy on the Chinese benchmarks, including an accuracy of 29.2% on
BrowseComp-ZH (Zhou et al., 2025b) and 40.4% Xbench-DeepResearch (Chen et al., 2025a). Our
model not only achieves first place on multiple benchmarks (e.g., BrowseComp, BrowseComp-ZH,
WebWalkerQA) among models with fewer than 15B parameters, but also surpasses some 32B and
72B models.
2 RELATEDWORKS
Reasoning Model.The emergence of ChatGPT (OpenAI, 2022) has brought the general reasoning
capabilities of LLMs to widespread public attention. In the period that followed, several improved
foundational model series such as Grok, Gemini, Claude, and GPT were proposed. These models
have validated the scaling laws (Kaplan et al., 2020) and consistently set new records on reasoning
benchmarks. As the marginal effects of pre-training diminish, works such as o1 (Jaech et al., 2024)
and Deepseek-R1 (Guo et al., 2025) have found that post-training with reinforcement learning to
2Under Review
increase the reasoning length during test-time further enhances the model’s ability to tackle excep-
tionally difficult problems. Today, advanced reasoning models like Gemini-2.5-pro (Comanici et al.,
2025), o3 (OpenAI, 2025c), Claude-4 (Antropic, 2025), and Grok-4 (xAI, 2025b) have achieved
gold-level performance in top-tier human mathematics (AoPS, 2025) and coding (Jimenez et al.,
2023; Miserendino et al., 2025) competitions. Such reasoning capabilities should theoretically drive
evolutionary changes in societal productivity. We believe that the gap lies in the model’s reasoning
abilities and its interaction level with reality. Models need to use tools to interact with the real world
to maximize the practical impact of their powerful reasoning capabilities.
Retrieval-augmented Generation (RAG).RAG refers to models that combine a pre-trained lan-
guage generator with an explicit retrieval component (Lewis et al., 2020; Borgeaud et al., 2022; Guu
et al., 2020). Typically, RAG systems segment data and employ some form of vector retrieval for in-
formation access (Borgeaud et al., 2022; Chen et al., 2022; Gong et al., 2020; Ishiwatari et al., 2017).
RAG often integrates the retriever as a component within the system, allowing for end-to-end train-
ing alongside the generator (Guu et al., 2020; Izacard et al., 2023; Lewis et al., 2020). Compared to
information-seeking agents, most RAG systems demonstrate high efficiency because RAG treats the
retrieved passages as latent variables (Li et al., 2021; Wang et al., 2023; Févry et al., 2020) rather
than directly inputting the text into the generator. In summary, RAG retrieval is efficient, static, and
localized, whereas the agent studied in this paper is iterative, offering greater flexibility. This allows
it to handle data from multiple sources from a wider range of tools.
Deep Research Agents and Benchmarks.Recent researches bring web-browsing into agen-
tic model, such as DeepDive (Lu et al., 2025), WebSailor (Li et al., 2025b), WebSailor-V2 (Li
et al., 2025a), and ASearcher (Gao et al., 2025). They leverage data-synthesis methods like InfoS-
eek (Xia et al., 2025), which enhance long-horizon browsing via knowledge-graph question synthe-
sis, scalable multi-turn RL, and large-scale data generation. Search-augmented frameworks includ-
ing Search-o1 (Li et al., 2025c), Search-R1 (Jin et al., 2025), DeepResearcher (Zheng et al., 2025),
and the multi-agent WebThinker (Li et al., 2025d) integratethink–search–writeloops in real-web
environments. Memory-based continual agents such as AgentFly (Zhou et al., 2025a) achieve ad-
vanced performance without tuning base models. At the foundation-model level, GLM-4.5 (Zeng
et al., 2025a) and Kimi-Researcher (MoonshotAI, 2025) adopt multi-stage post-training with RL to
strengthen tool use and reasoning. Benchmarks for DRAs include BrowseComp (Wei et al., 2025),
HLE (Phan et al., 2025), BrowseComp-ZH (Zhou et al., 2025b) xBench (Chen et al., 2025a), and
GAIA (Mialon et al., 2023). They provide more complex assessment that require integration of more
sources of information compared to traditional RAG and knowledge-based QA benchmarks.
3 INFOAGENTAPPROACH
We adopt the ReAct framework (Yao et al., 2023) to construct InfoAgent, which iteratively combines
reasoning with tool calls to arrive at a solution. For a given problem, the agent engages in an
action cycle: at each step, it incorporates new observations from the tool, generates reasoning traces,
and continually calls tools with the corresponding arguments. Both the reasoning process and tool
outputs are appended to the context of the LLM, enabling continuous research. For our InfoAgent,
we provide two web-based tools, search and browse, to process information-seeking requests. The
search tool retrieves a ranked list of web URLs along with content snippets, while the browse tool
enables deeper investigation of the content associated with a given URL.
Below, we introduce our method to synthesize data and the implementation of tools, which enable
effective and efficient training for InfoAgent.
3.1 DATASYNTHESISPIPELINE
We introduce a two-stage pipeline to automatically synthesize complex, multi-entity search ques-
tions, which can be solved only if the model excels in long-horizon search and reasoning. This
pipeline converts raw Wikipedia entities into structured QA pairs via (1)Tree Constructionand (2)
QA Generationshown in Figure 2.
3