# 2510.03279v1.pdf

Under review as a conference paper at ICLR 2026
MEMMAMBA: RETHINKINGMEMORYPATTERNS IN
STATESPACEMODEL
Youjin Wang†
School of Statistics
Renmin University of China
Beijing, China
Yangjingyi Chen ‡
Shanghai University of Finance and Economics
Shanghai, China
Jiahao Yan ‡
Gao Ling Institute of Artificial Intelligence
Renmin University of China
Beijing, China
Jiaxuan Lu ∗
Shanghai Artificial Intelligence Laboratory
Shanghai, China
Xiao Sun ∗
Shanghai Artificial Intelligence Laboratory
Shanghai, China
ABSTRACT
With the explosive growth of data, long-sequence modeling has become increas-
ingly important in tasks such as natural language processing and bioinformatics.
However, existing methods face inherent trade-offs between efficiency and mem-
ory. Recurrent neural networks suffer from gradient vanishing and explosion,
making them hard to scale. Transformers can model global dependencies but are
constrained by quadratic complexity. Recently, selective state-space models such
as Mamba have demonstrated high efficiency withO(n)time andO(1)recur-
rent inference, yet their long-range memory decays exponentially. In this work,
we conduct mathematical derivations and information-theoretic analysis to sys-
tematically uncover the memory decay mechanism of Mamba, answering a fun-
damental question: what is the nature of Mamba’s long-range memory and how
does it retain information? To quantify key information loss, we further introduce
horizontal–vertical memory fidelity metrics that capture degradation both within
and across layers. Inspired by how humans distill and retain salient information
when reading long documents, we propose MemMamba, a novel architectural
framework that integrates state summarization mechanism together with cross-
layer and cross-token attention, which alleviates long-range forgetting while pre-
serving linear complexity. MemMamba achieves significant improvements over
existing Mamba variants and Transformers on long-sequence benchmarks such as
PG19-PPL and Passkey Retrieval, while delivering a 48% speedup in inference
efficiency. Both theoretical analysis and empirical results demonstrate that Mem-
Mamba achieves a breakthrough in the complexity–memory trade-off, offering a
new paradigm for ultra-long sequence modeling.
1 INTRODUCTION
Long-sequence data typically refers to continuous sequences spanning thousands to millions of time
steps or tokens, which pervade modern machine learning applications, from modeling book-length
documents in NLP, to analyzing DNA sequences in bioinformatics, to processing complex multi-
modal medical records. A central challenge in sequence modeling is how to capture ultra-long-range
†Independent first author: Youjin Wang (wangyoujin@ruc.edu.cn)
‡These authors contributed equally as the second authors.
∗Corresponding authors: Jiaxuan Lu (lujiaxuan@pjlab.org.cn); Xiao Sun.
1
arXiv:2510.03279v1  [cs.LG]  28 Sep 2025Under review as a conference paper at ICLR 2026
dependencies while maintaining efficiency. Traditional architectures exhibit significant limitations
when dealing with such data. Recurrent neural networks (RNNs) and their variants (LSTM, GRU)
are inherently sequential and suffer from vanishing or exploding gradients, making them unstable
for long dependencies (Pascanu et al., 2013) (Hochreiter & Schmidhuber, 1997). Transformers in-
troduced a paradigm shift with self-attention and global context modeling (Vaswani et al., 2017), but
their quadratic complexity in sequence length renders them inefficient for truly long contexts (Brown
et al., 2020). The trade-off between expressiveness and scalability has created an architectural im-
passe.
Recent advances in selective state-space models (SSMs), notably the Mamba architecture (Gu &
Dao, 2023), offer a compelling alternative. By decoupling sequence length from computation,
Mamba achieves linear-time complexityO(n)and constant-time recurrent inferenceO(1), posi-
tioning itself as a promising foundation for long-sequence modeling. However, despite this compu-
tational leap, its memory fidelity degrades rapidly at scale. As sequence length grows, Mamba and
its successors (e.g., Mamba-2) exhibit sharp declines in tasks demanding strong memory retention,
such as 5-shot MMLU or long-range key-value retrieval (Waleffe et al., 2024). This leads to a fun-
damental question: How does Mamba’s memory pattern evolve with distance and depth, and what
underlies its degradation?
This paper introduces a new lens for understanding and advancing long-sequence models. We
present the first systematic analysis of Mamba’s memory mechanism. Through mathematical deriva-
tion and information-theoretic analysis, we characterize its memory decay behavior and introduce
thehorizontal–vertical memory fidelityframework, which quantifies critical information loss from
two perspectives: token-level semantic transmission and cross-layer information coupling. Our anal-
ysis reveals that, although Mamba’s state update ensures computational stability, the contribution of
early information decays exponentially during both intra-layer recursion and inter-layer propagation,
fundamentally constraining its long-range memory capacity.
Building on these insights, we proposeMemMamba, a novel architecture that reimagines state-
space modeling as a structured memory system. Inspired by how humans take notes while reading
long texts, MemMamba integrates lightweight state summarization with cross-layer and cross-token
attention to dynamically preserve and reuse salient information, all while maintaining linear com-
putational complexity. This “note-taking” mechanism alleviates long-range forgetting, breaking the
classical trade-off in SSMs. Empirically, MemMamba achieves breakthrough improvements across
multiple long-sequence benchmarks. On the PG19 language modeling task, it maintains stable per-
plexity (17.35) even at 60k tokens, where Mamba and DeciMamba (Ben-Kish et al., 2025) of similar
parameter scale collapse completely. On the Passkey Retrieval task, MemMamba preserves 90% re-
trieval accuracy at 400k tokens. On the cross-document retrieval task under noisy conditions, it
significantly outperforms both Transformers and existing state-space variants.
The main contributions of this work are summarized as follows:
•Memory-theoretic insight.We formalize Mamba’s information bottlenecks through
the horizontal–vertical memory fidelity framework, offering a new perspective on long-
sequence degradation.
•Architectural innovation.MemMamba introduces state summarization and cross-layer /
cross-token attention to simulate note-taking and bridge across-time and across-layer mem-
ory decay, without compromising efficiency.
•Empirical breakthroughs.On language modeling, sparse retrieval, and cross-document
reasoning tasks, MemMamba consistently outperforms Mamba variants and Transformer
baselines, achieving 48% inference speedup, setting a new bar for memory retention in
efficient sequence models.
2 RELATEDWORK
2.1 STATESPACEMODELS
State space models (SSMs) have become strong candidates for long-sequence modeling due to their
linear-time complexity and recursive inference. Since S4 (Gu et al., 2021), SSMs have made con-
2Under review as a conference paper at ICLR 2026
tinuous progress in language, speech, and time-series modeling. Mamba (Gu & Dao, 2023) stands
out by leveraging a selective SSM mechanism that enhances expressiveness, achieving performance
comparable to or surpassing Transformers in language modeling, genomics, and reasoning tasks.
Building on Mamba’s success, follow-up works focus on three main directions: 1) Architectural
optimization: BiMamba (Liang et al., 2024) improves long-range dependency modeling with bidi-
rectional state updates, and Vision Mamba (Zhu et al., 2024) adapts Mamba for vision tasks. 2)
Computational efficiency: FastMamba (Wang et al., 2025) improves training and inference speed
via parallelization and caching, enabling scalability to longer sequences. 3) Application extension:
Mamba has been applied to molecular dynamics (Hu et al., 2025), speech recognition (Zhang et al.,
2025), EEG signal understanding (Liu et al., 2025), image recognition (Lin et al., 2025; Lu et al.,
2025), event analysis (Lin et al., 2024), and long-text understanding, showcasing its cross-modal
generalization capabilities.
Nevertheless, most of these efforts primarily target architectural design or efficiency improvements,
which further highlights the central challenge of long-sequence modeling:how to continuously
enhance a model’s ability to capture long dependencies while preserving computational efficiency?
2.2 LONG-SEQUENCEMODELING
Long-sequence modeling is a critical issue in AI and cognitive science. Early models like LSTMs
and GRUs introduced gating for long-term dependencies, while NTMs and DNCs added external
memory. Memory Networks proposed slot-based storage, and Hopfield networks improved associa-
tive memory. Neuroscience-inspired models, such as spiking neural networks and HTM, have also
emerged.
The Transformer has become the standard for modeling long-range dependencies. The Compres-
sive Transformer improves efficiency with compressed memory, though at the cost of information
loss (Rae et al., 2019). Megalodon supports million-token contexts but excels in extreme-length
tasks (Ma et al., 2024). Sparse-attention models like Longformer (Beltagy et al., 2020) and Big-
Bird (Zaheer et al., 2020) reduce complexity, but struggle with ultra-long sequences.
These limitations have led to SSM-based approaches like Mamba. DeciMamba extends context
length 25× through dynamic pooling, boosting performance by 220% on TriviaQA (Ben-Kish et al.,
2025), but risks losing fine-grained information. mmMamba integrates multimodal distillation for
a 20.6× speedup (Li et al., 2024), but requires costly distillation data. DocMamba reduces memory
overhead by 88.3% for document processing (Hu et al., 2024), though gains are task-specific. Long-
Mamba improves long-context extrapolation, but faces stability issues (Ye et al., 2025). Mamba-2
refines architecture and stability, outperforming the original Mamba, but still lags behind Transform-
ers in tasks requiring strong copying or in-context learning (Gu & Dao, 2024).
These advances highlight Mamba’s potential in long-sequence tasks. However, most prior work
focuses on structural or context extension. The question of how models remember and forget critical
information remains largely unexplored. Our work focuses on memory patterns in Mamba to address
long-range forgetting and expand the design space for memory-augmented SSMs.
3 INVESTIGATION OFMEMORYPATTERNS
3.1 MEMORYDECAY INMAMBA
The Mamba model builds on the mechanism of selective state space models (SSMs), achieving ef-
ficient sequence modeling through dynamic state compression, with explicit state update equations
forming its computational core. While Mamba has significant advantages in computational effi-
ciency, it tends to suffer from memory decay when modeling long-range dependencies, and it is also
limited in capturing fine-grained local information.
The state update of Mamba is defined as:
ht =A·h t−1 +B·x t, yt =C·h t,(1)
3