"""Telemetry data analysis and reporting.

Provides utilities for analyzing telemetry logs generated by the orchestrator,
including failure analysis, throughput calculations, and connector-specific metrics.
"""

from __future__ import annotations

import json
from collections import defaultdict
from dataclasses import dataclass
from datetime import datetime, timedelta
from pathlib import Path
from typing import Any, Dict, List, Optional


@dataclass
class FailureStatistics:
    """Statistics about job failures."""

    total_failures: int
    failures_by_reason: Dict[str, int]
    failures_by_connector: Dict[str, int]
    recent_failures_24h: int
    failure_rate: float  # Percentage of jobs that failed


@dataclass
class ThroughputMetrics:
    """Throughput metrics over a time window."""

    files_processed: int
    bytes_processed: int
    duration_seconds: float
    throughput_bytes_per_second: float
    throughput_files_per_second: float
    throughput_mbps: float


@dataclass
class ConnectorMetrics:
    """Metrics for a specific connector type."""

    connector_type: str
    total_jobs: int
    succeeded_jobs: int
    failed_jobs: int
    total_files: int
    total_bytes: int
    total_duration: float
    avg_throughput_bps: float
    success_rate: float


class TelemetryAnalyzer:
    """Analyzes telemetry log files to generate operator reports."""

    def __init__(self, telemetry_dir: Path) -> None:
        """Initialize telemetry analyzer.

        Args:
            telemetry_dir: Directory containing telemetry files
        """
        self._telemetry_dir = telemetry_dir
        self._telemetry_file = telemetry_dir / "telemetry.log"

    def analyze_failures(
        self,
        since: Optional[datetime] = None,
    ) -> FailureStatistics:
        """Analyze failure statistics from telemetry logs.

        Args:
            since: Only include failures since this timestamp

        Returns:
            FailureStatistics with failure breakdown
        """
        if not self._telemetry_file.exists():
            return FailureStatistics(
                total_failures=0,
                failures_by_reason={},
                failures_by_connector={},
                recent_failures_24h=0,
                failure_rate=0.0,
            )

        cutoff_time = since or datetime.min
        cutoff_24h = datetime.utcnow() - timedelta(hours=24)

        total_jobs = 0
        total_failures = 0
        recent_failures = 0
        failures_by_reason: Dict[str, int] = defaultdict(int)
        failures_by_connector: Dict[str, int] = defaultdict(int)

        with self._telemetry_file.open("r") as f:
            for line in f:
                try:
                    entry = json.loads(line.strip())
                    timestamp_str = entry.get("timestamp")
                    if not timestamp_str:
                        continue

                    timestamp = datetime.fromisoformat(
                        timestamp_str.replace("Z", "+00:00")
                    )

                    # Count total jobs
                    if entry.get("status") in ("succeeded", "failed"):
                        total_jobs += 1

                    # Filter by time
                    if timestamp < cutoff_time:
                        continue

                    # Count failures
                    if entry.get("status") == "failed":
                        total_failures += 1

                        # Recent failures
                        if timestamp >= cutoff_24h:
                            recent_failures += 1

                        # By reason (from metadata)
                        metadata = entry.get("metadata", {})
                        failure_reason = metadata.get("failure_reason", "unknown")
                        failures_by_reason[failure_reason] += 1

                        # By connector
                        connector_type = metadata.get("connector_type", "unknown")
                        failures_by_connector[connector_type] += 1

                except (json.JSONDecodeError, ValueError, KeyError):
                    continue

        failure_rate = (total_failures / total_jobs * 100.0) if total_jobs > 0 else 0.0

        return FailureStatistics(
            total_failures=total_failures,
            failures_by_reason=dict(failures_by_reason),
            failures_by_connector=dict(failures_by_connector),
            recent_failures_24h=recent_failures,
            failure_rate=failure_rate,
        )

    def calculate_throughput(
        self,
        since: Optional[datetime] = None,
        until: Optional[datetime] = None,
    ) -> ThroughputMetrics:
        """Calculate throughput metrics over a time window.

        Args:
            since: Start of time window (default: beginning of logs)
            until: End of time window (default: now)

        Returns:
            ThroughputMetrics for the specified time window
        """
        if not self._telemetry_file.exists():
            return ThroughputMetrics(
                files_processed=0,
                bytes_processed=0,
                duration_seconds=0.0,
                throughput_bytes_per_second=0.0,
                throughput_files_per_second=0.0,
                throughput_mbps=0.0,
            )

        cutoff_start = since or datetime.min
        cutoff_end = until or datetime.utcnow()

        total_files = 0
        total_bytes = 0
        total_duration = 0.0

        with self._telemetry_file.open("r") as f:
            for line in f:
                try:
                    entry = json.loads(line.strip())
                    timestamp_str = entry.get("timestamp")
                    if not timestamp_str:
                        continue

                    timestamp = datetime.fromisoformat(
                        timestamp_str.replace("Z", "+00:00")
                    )

                    # Filter by time window
                    if timestamp < cutoff_start or timestamp > cutoff_end:
                        continue

                    # Only count succeeded jobs
                    if entry.get("status") != "succeeded":
                        continue

                    total_files += entry.get("files_processed", 0)
                    total_bytes += entry.get("bytes_processed", 0)
                    total_duration += entry.get("duration", 0.0)

                except (json.JSONDecodeError, ValueError, KeyError):
                    continue

        # Calculate rates
        throughput_bps = (
            (total_bytes / total_duration) if total_duration > 0 else 0.0
        )
        throughput_fps = (
            (total_files / total_duration) if total_duration > 0 else 0.0
        )
        throughput_mbps = throughput_bps / (1024 * 1024)

        return ThroughputMetrics(
            files_processed=total_files,
            bytes_processed=total_bytes,
            duration_seconds=total_duration,
            throughput_bytes_per_second=throughput_bps,
            throughput_files_per_second=throughput_fps,
            throughput_mbps=throughput_mbps,
        )

    def metrics_by_connector(
        self,
        since: Optional[datetime] = None,
    ) -> List[ConnectorMetrics]:
        """Calculate metrics broken down by connector type.

        Args:
            since: Only include metrics since this timestamp

        Returns:
            List of ConnectorMetrics for each connector type
        """
        if not self._telemetry_file.exists():
            return []

        cutoff_time = since or datetime.min

        # Accumulate metrics per connector
        connector_data: Dict[str, Dict[str, Any]] = defaultdict(
            lambda: {
                "total_jobs": 0,
                "succeeded": 0,
                "failed": 0,
                "files": 0,
                "bytes": 0,
                "duration": 0.0,
            }
        )

        with self._telemetry_file.open("r") as f:
            for line in f:
                try:
                    entry = json.loads(line.strip())
                    timestamp_str = entry.get("timestamp")
                    if not timestamp_str:
                        continue

                    timestamp = datetime.fromisoformat(
                        timestamp_str.replace("Z", "+00:00")
                    )

                    # Filter by time
                    if timestamp < cutoff_time:
                        continue

                    # Get connector type
                    metadata = entry.get("metadata", {})
                    connector_type = metadata.get("connector_type", "unknown")

                    status = entry.get("status")
                    if status not in ("succeeded", "failed"):
                        continue

                    # Accumulate metrics
                    data = connector_data[connector_type]
                    data["total_jobs"] += 1

                    if status == "succeeded":
                        data["succeeded"] += 1
                        data["files"] += entry.get("files_processed", 0)
                        data["bytes"] += entry.get("bytes_processed", 0)
                        data["duration"] += entry.get("duration", 0.0)
                    elif status == "failed":
                        data["failed"] += 1

                except (json.JSONDecodeError, ValueError, KeyError):
                    continue

        # Build ConnectorMetrics list
        metrics = []
        for connector_type, data in connector_data.items():
            avg_throughput = (
                (data["bytes"] / data["duration"]) if data["duration"] > 0 else 0.0
            )
            success_rate = (
                (data["succeeded"] / data["total_jobs"] * 100.0)
                if data["total_jobs"] > 0
                else 0.0
            )

            metrics.append(
                ConnectorMetrics(
                    connector_type=connector_type,
                    total_jobs=data["total_jobs"],
                    succeeded_jobs=data["succeeded"],
                    failed_jobs=data["failed"],
                    total_files=data["files"],
                    total_bytes=data["bytes"],
                    total_duration=data["duration"],
                    avg_throughput_bps=avg_throughput,
                    success_rate=success_rate,
                )
            )

        # Sort by total jobs descending
        metrics.sort(key=lambda m: m.total_jobs, reverse=True)
        return metrics

    def clean_old_telemetry(
        self,
        older_than_days: int,
        dry_run: bool = False,
    ) -> int:
        """Remove telemetry entries older than specified days.

        Args:
            older_than_days: Remove entries older than this many days
            dry_run: If True, don't actually delete, just count

        Returns:
            Number of lines that would be/were removed
        """
        if not self._telemetry_file.exists():
            return 0

        cutoff_date = datetime.utcnow() - timedelta(days=older_than_days)
        lines_to_keep = []
        lines_removed = 0

        with self._telemetry_file.open("r") as f:
            for line in f:
                try:
                    entry = json.loads(line.strip())
                    timestamp_str = entry.get("timestamp")
                    if not timestamp_str:
                        # Keep lines without timestamp (shouldn't happen)
                        lines_to_keep.append(line)
                        continue

                    timestamp = datetime.fromisoformat(
                        timestamp_str.replace("Z", "+00:00")
                    )

                    if timestamp >= cutoff_date:
                        lines_to_keep.append(line)
                    else:
                        lines_removed += 1

                except (json.JSONDecodeError, ValueError, KeyError):
                    # Keep malformed lines to avoid data loss
                    lines_to_keep.append(line)
                    continue

        # Write back if not dry run
        if not dry_run and lines_removed > 0:
            # Create backup first
            backup_file = self._telemetry_file.with_suffix(".log.backup")
            if backup_file.exists():
                backup_file.unlink()
            self._telemetry_file.rename(backup_file)

            # Write cleaned data
            with self._telemetry_file.open("w") as f:
                f.writelines(lines_to_keep)

        return lines_removed
